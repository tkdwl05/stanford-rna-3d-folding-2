{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3815736",
   "metadata": {},
   "source": [
    "# improve_data_label_v11\n",
    "\n",
    "v8~v10 ë¹„êµë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì¢Œí‘œ ë¶•ê´´(collapse)**ë¥¼ ì¤„ì´ê³ , K=5 í›„ë³´ì˜ **ë‹¤ì–‘ì„±**ê³¼ **ìŠ¤ì¼€ì¼/êµ­ì†Œ ê¸°í•˜**ë¥¼ ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ìˆ˜ì •í•œ v11ì´ë‹¤.\n",
    "\n",
    "í•µì‹¬ ë³€ê²½ì :\n",
    "- CoordHeadë¥¼ `DeltaCoordHead`ë¡œ êµì²´: per-residue **delta**ë¥¼ ì˜ˆì¸¡í•˜ê³  **cumsum**ìœ¼ë¡œ ì¢Œí‘œë¥¼ êµ¬ì„±(ì—°ì†ì„±/ìŠ¤í”„ë ˆë“œ ìœ ë„)\n",
    "- softmin ì˜¨ë„ ìƒí–¥ + **head diversity loss** ì¶”ê°€(í•œ í—¤ë“œë¡œ ì ë¦¼/ì£½ëŠ” í—¤ë“œ ë°©ì§€)\n",
    "- non-neighborì— ëŒ€í•œ **repulsion hinge** ì¶”ê°€(ê²¹ì¹¨ ë°©ì§€)\n",
    "- confidence headëŠ” ìœ ì§€(í›„ë³´ ë­í‚¹ ë³´ì¡°)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3304af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 0) Imports, Device, Config  [v11]\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd51ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 30\n",
    "\n",
    "    # model\n",
    "    n_tokens: int = 5           # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    num_preds: int = 5\n",
    "\n",
    "    # train\n",
    "    batch: int = 16\n",
    "    epochs: int = 80\n",
    "    warmup_epochs: int = 5\n",
    "    lr: float = 5e-5            # v8ë³´ë‹¤ ë„ˆë¬´ ë‚®ìœ¼ë©´ collapseì—ì„œ ëª» ë¹ ì ¸ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ìˆì–´ ì•½ê°„ ìƒí–¥\n",
    "    wd: float = 1e-2\n",
    "    grad_clip: float = 1.0\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # loss\n",
    "    dist_w: float = 0.08\n",
    "    softmin_temp: float = 0.80          # v9/v10(0.3)ì—ì„œ head ì ë¦¼ì´ ì‹¬í•´ì ¸ì„œ ìƒí–¥\n",
    "    pair_num_pairs: int = 2048\n",
    "\n",
    "    # regularizers\n",
    "    var_w: float = 0.01\n",
    "    local_w: float = 0.10\n",
    "    repulse_w: float = 0.05             # non-neighbor ê²¹ì¹¨ ë°©ì§€\n",
    "    repulse_margin: float = 2.5         # Ã… ë‹¨ìœ„ ê°€ì •(ë°ì´í„°ì— ë§ì¶° ì¡°ì ˆ)\n",
    "    diversity_w: float = 0.05           # head ë‹¤ì–‘ì„±\n",
    "    diversity_margin: float = 2.0\n",
    "\n",
    "    # candidate confidence (ranking)\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05\n",
    "\n",
    "    # amp\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # early stop\n",
    "    patience: int = 10\n",
    "    ckpt_path: str = \"best_structured_v11.pt\"\n",
    "    ckpt_stage1_path: str = \"best_stage1_v11.pt\"\n",
    "    dist_w_ramp_epochs: int = 8\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4595817",
   "metadata": {},
   "source": [
    "## 1) Dataset / Data Loading\n",
    "\n",
    "v10ê³¼ ë™ì¼í•œ ë°ì´í„° ë¡œë”©/ë¼ë²¨ êµ¬ì„± ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤. ì•„ë˜ ì…€ì€ v10 ë…¸íŠ¸ë¶ì—ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¨ ë¶€ë¶„ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6ee1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_1216\\799196925.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n",
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n",
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07521775",
   "metadata": {},
   "source": [
    "## 2) Loss helpers (Kabsch, pairwise, local, variance) + v11 ì¶”ê°€(Repulsion, Diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392f6ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (4750, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n",
      "train batches: 268 hold batches: 30\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = np.asarray(self.tokens_list[idx], dtype=np.int64)\n",
    "        coords = np.asarray(self.coords_list[idx], dtype=np.float32)\n",
    "        mask   = np.asarray(self.mask_list[idx], dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        valid = mask.astype(bool)\n",
    "        if self.center_only and valid.sum() > 0:\n",
    "            center = coords[valid].mean(axis=0, keepdims=True)\n",
    "            coords = coords - center\n",
    "\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64);  tokens_p[:L] = tokens\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32); coords_p[:L] = coords\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32); mask_p[:L] = mask\n",
    "\n",
    "        return torch.tensor(tokens_p), torch.tensor(coords_p), torch.tensor(mask_p)\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_ds, batch_size=cfg.batch, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 7) Losses + Composer  [v10]\n",
    "#   - per-head losses (B,K) so we can:\n",
    "#       (1) aggregate with softmin for stability\n",
    "#       (2) train a confidence head to rank candidates (aux loss)\n",
    "#   - Kabsch runs in FP32 even when AMP is enabled\n",
    "#   - pairwise distance loss uses sampled residue pairs (fast)\n",
    "#   - anti-collapse + local bond regularizers\n",
    "# ==========================================\n",
    "\n",
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    return torch.softmax(-losses / max(float(temperature), 1e-8), dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = softmin_weights(losses, temperature)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    out = []\n",
    "    denom = m.sum(dim=(1,2)).clamp_min(1.0)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        l1 = diff.sum(dim=(1,2)) / denom\n",
    "        out.append(l1)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        out.append(rmsd)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds, target, mask, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return losses_bk\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)  # (B,1,1)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]  # (B,T,3)\n",
    "        mean = (pk * m).sum(dim=1, keepdim=True) / denom\n",
    "        var = ((pk - mean) ** 2 * m).sum(dim=(1,2)) / denom.squeeze(1).squeeze(1).clamp_min(1.0)  # (B,)\n",
    "        out.append(var)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m_adj = (mask[:, 1:] * mask[:, :-1]).bool()  # (B,T-1)\n",
    "    tgt = (target[:, 1:] - target[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    out = []\n",
    "    denom = m_adj.sum(dim=1).clamp_min(1)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        l = (diff * m_adj).sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned - target).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd9d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# v11 ì¶”ê°€: non-neighbor repulsion + head diversity\n",
    "# ==========================================\n",
    "def repulsion_losses_sampled(preds: torch.Tensor, mask: torch.Tensor,\n",
    "                             num_pairs: int = 2048, margin: float = 2.5) -> torch.Tensor:\n",
    "    \"\"\"ê²¹ì¹¨ ë°©ì§€ìš© hinge loss. ì¸ì ‘(i,i+1)ì€ ì œì™¸í•˜ê³  ëœë¤ pairì— ëŒ€í•´\n",
    "    d < margin ì´ë©´ (margin-d)^2 ë¥¼ ë¶€ê³¼.\n",
    "    returns (B,K)\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses = preds.new_zeros((B, K))\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 3:\n",
    "            continue\n",
    "\n",
    "        # sample pairs\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        # exclude neighbors\n",
    "        nn_mask = (torch.abs(i - j) > 1)\n",
    "        if nn_mask.sum() < 8:\n",
    "            continue\n",
    "        i = i[nn_mask]\n",
    "        j = j[nn_mask]\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            hinge = (margin - d).clamp_min(0.0)\n",
    "            losses[b, k] = (hinge * hinge).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "def head_diversity_losses(preds: torch.Tensor, mask: torch.Tensor, margin: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"í—¤ë“œ ê°„ ìœ ì‚¬í•˜ë©´ íŒ¨ë„í‹°. (B,K)ë¡œ ë°˜í™˜í•´ì„œ ê¸°ì¡´ softmin í”„ë ˆì„ì— ë§ì¶˜ë‹¤.\n",
    "    ê° í—¤ë“œì˜ masked centered coordsë¥¼ ë§Œë“¤ê³ , í—¤ë“œìŒ RMSDê°€ marginë³´ë‹¤ ì‘ìœ¼ë©´ hinge.\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "    # centered\n",
    "    centered = preds - (preds * m.unsqueeze(1)).sum(dim=2, keepdim=True) / denom.unsqueeze(1)\n",
    "\n",
    "    # pairwise head RMSD (no rotation; diversity ëª©ì ì´ë¼ ë‹¨ìˆœ RMSD)\n",
    "    out = preds.new_zeros((B, K))\n",
    "    if K < 2:\n",
    "        return out\n",
    "\n",
    "    for a in range(K):\n",
    "        pen = 0.0\n",
    "        cnt = 0\n",
    "        for b in range(K):\n",
    "            if a == b: \n",
    "                continue\n",
    "            diff = (centered[:, a] - centered[:, b])**2  # (B,T,3)\n",
    "            rmsd = torch.sqrt((diff * m).sum(dim=(1,2)) / (mask.sum(dim=1).clamp_min(1.0)*3.0) + 1e-8)  # (B,)\n",
    "            hinge = (margin - rmsd).clamp_min(0.0)\n",
    "            pen = pen + hinge*hinge\n",
    "            cnt += 1\n",
    "        out[:, a] = pen / max(cnt, 1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a857de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3) LossComposer  [v11]\n",
    "#   - warmup: masked L1 (+optional confidence)\n",
    "#   - main: Kabsch RMSD + pairwise + local + variance + repulsion + diversity (+confidence)\n",
    "# ==========================================\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: structured losses (all are (B,K))\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)\n",
    "        pair_bk = pairwise_distance_losses_sampled(preds, target, mask, num_pairs=int(self.cfg.pair_num_pairs))\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)\n",
    "        var_bk  = coord_variance_losses(preds, mask)\n",
    "\n",
    "        rep_bk  = repulsion_losses_sampled(preds, mask, num_pairs=int(self.cfg.pair_num_pairs),\n",
    "                                           margin=float(self.cfg.repulse_margin))\n",
    "        div_bk  = head_diversity_losses(preds, mask, margin=float(self.cfg.diversity_margin))\n",
    "\n",
    "        # ramp pairwise/repulsion after warmup\n",
    "        t = max(0, int(epoch - self.cfg.warmup_epochs))\n",
    "        ramp = min(1.0, t / max(1, int(self.cfg.dist_w_ramp_epochs)))\n",
    "        dist_w_eff = float(self.cfg.dist_w) * ramp\n",
    "        rep_w_eff  = float(self.cfg.repulse_w) * ramp\n",
    "\n",
    "        total_bk = (\n",
    "            rmsd_bk\n",
    "            + dist_w_eff * pair_bk\n",
    "            + float(self.cfg.local_w) * loc_bk\n",
    "            - float(self.cfg.var_w) * var_bk\n",
    "            + rep_w_eff * rep_bk\n",
    "            + float(self.cfg.diversity_w) * div_bk\n",
    "        )\n",
    "\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"STRUCTURED(+CONF)\" if aux != 0.0 else \"STRUCTURED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69844dfb",
   "metadata": {},
   "source": [
    "## 4) Model  [v11]\n",
    "\n",
    "`CoordHead` ëŒ€ì‹  `DeltaCoordHead`ë¥¼ ì‚¬ìš©í•œë‹¤. ê° residueì˜ 3D deltaë¥¼ ì˜ˆì¸¡í•˜ê³  ëˆ„ì í•©(cumsum)ìœ¼ë¡œ ì¢Œí‘œë¥¼ ë§Œë“ ë‹¤. ì´ ë°©ì‹ì€ (1) ì¢Œí‘œê°€ í•œ ì ì— ë¶•ê´´ë˜ëŠ” í•´ë¥¼ ë” ì–´ë µê²Œ ë§Œë“¤ê³ , (2) ì¸ì ‘ ê±°ë¦¬(local bond) ì†ì‹¤ê³¼ ìƒì„±ì´ ì¢‹ì•„ ì´ˆê¸° í•™ìŠµì´ ì•ˆì •ì ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e9f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4) Model (Backbone + DeltaCoordHead + ConfidenceHead)  [v11]\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos).unsqueeze(0)\n",
    "\n",
    "class RNABackbone(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg.n_tokens, cfg.d_model, padding_idx=0)\n",
    "        self.pos = PositionalEncodingLearned(cfg.d_model, max_len=2048)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=cfg.layers)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        pad_mask = (tokens == 0)\n",
    "        x = self.emb(tokens)\n",
    "        x = self.pos(x)\n",
    "        x = self.enc(x, src_key_padding_mask=pad_mask)\n",
    "        return x, pad_mask\n",
    "\n",
    "class DeltaCoordHead(nn.Module):\n",
    "    \"\"\"per-residue deltaë¥¼ ì˜ˆì¸¡í•˜ê³  cumsumìœ¼ë¡œ coords êµ¬ì„±\"\"\"\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.proj = nn.Linear(cfg.d_model, 3 * cfg.num_preds)\n",
    "        # delta ìŠ¤ì¼€ì¼(í•™ìŠµ ê°€ëŠ¥). ì´ˆê¸°ê°’ì€ 1.0ìœ¼ë¡œ ë‘ê³ , ì•„ë˜ì—ì„œ ë°ì´í„° í†µê³„ë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤.\n",
    "        self.step_scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        B, T, D = h.shape\n",
    "        d = self.proj(h).view(B, T, self.num_preds, 3).permute(0, 2, 1, 3).contiguous()  # (B,K,T,3)\n",
    "        # ê³¼ë„í•œ ë°œì‚°ì„ ë§‰ê³ , ìŠ¤ì¼€ì¼ì€ step_scaleë¡œ ì¡°ì ˆ\n",
    "        d = torch.tanh(d) * self.step_scale\n",
    "\n",
    "        coords = torch.cumsum(d, dim=2)  # along T\n",
    "\n",
    "        # masked centering (translation ì œê±°)\n",
    "        m = (~pad_mask).float().unsqueeze(1).unsqueeze(-1)  # (B,1,T,1)\n",
    "        denom = m.sum(dim=2, keepdim=True).clamp_min(1.0)\n",
    "        mean = (coords * m).sum(dim=2, keepdim=True) / denom\n",
    "        coords = coords - mean\n",
    "        return coords\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.d_model, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        m = (~pad_mask).float().unsqueeze(-1)  # (B,T,1)\n",
    "        denom = m.sum(dim=1).clamp_min(1.0)\n",
    "        pooled = (h * m).sum(dim=1) / denom\n",
    "        return self.mlp(pooled)\n",
    "\n",
    "class RNAModel(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = RNABackbone(cfg)\n",
    "        self.coord_head = DeltaCoordHead(cfg)\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        h, pad_mask = self.backbone(tokens)\n",
    "        coords = self.coord_head(h, pad_mask)\n",
    "        conf_logits = self.conf_head(h, pad_mask) if self.cfg.use_confidence else None\n",
    "        return coords, conf_logits\n",
    "\n",
    "model = RNAModel(cfg).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e5e99",
   "metadata": {},
   "source": [
    "## 5) Trainer / Train loop\n",
    "\n",
    "v10 Trainerë¥¼ ê·¸ëŒ€ë¡œ ì“°ë˜, `model()` ì¶œë ¥ê³¼ LossComposer ì¸ìë§Œ ë§ì¶˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 [train]:   0%|          | 0/268 [00:00<?, ?it/s]c:\\Users\\tkdwl\\anaconda3\\envs\\rna_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1/80 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:10<00:00, 26.74it/s, loss=49.4, stage=MASKED_L1(+CONF), lr=1e-5]   \n",
      "Epoch 1/80 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 20.22it/s, loss=50.3, stage=MASKED_L1(+CONF), lr=1e-5, rmsd=18.7, tm=0.0508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] stage=MASKED_L1(+CONF) train_loss=49.446262 val_loss=50.269764 val_rmsd=18.6840 val_tm=0.0508\n",
      "ğŸ’¾ stage1 best loss updated: 50.269764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.64it/s, loss=48.7, stage=MASKED_L1(+CONF), lr=2e-5]   \n",
      "Epoch 2/80 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 22.65it/s, loss=50.9, stage=MASKED_L1(+CONF), lr=2e-5, rmsd=18.6, tm=0.0512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] stage=MASKED_L1(+CONF) train_loss=48.685627 val_loss=50.899717 val_rmsd=18.6137 val_tm=0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.59it/s, loss=49.1, stage=MASKED_L1(+CONF), lr=3e-5]   \n",
      "Epoch 3/80 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 23.72it/s, loss=51.3, stage=MASKED_L1(+CONF), lr=3e-5, rmsd=18.9, tm=0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] stage=MASKED_L1(+CONF) train_loss=49.073270 val_loss=51.301439 val_rmsd=18.9187 val_tm=0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.07it/s, loss=49, stage=MASKED_L1(+CONF), lr=4e-5]     \n",
      "Epoch 4/80 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 23.71it/s, loss=51.4, stage=MASKED_L1(+CONF), lr=4e-5, rmsd=18.7, tm=0.0512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] stage=MASKED_L1(+CONF) train_loss=49.047347 val_loss=51.370393 val_rmsd=18.6866 val_tm=0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.69it/s, loss=49.1, stage=MASKED_L1(+CONF), lr=5e-5]   \n",
      "Epoch 5/80 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 22.87it/s, loss=50.5, stage=MASKED_L1(+CONF), lr=5e-5, rmsd=18.8, tm=0.0503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] stage=MASKED_L1(+CONF) train_loss=49.065555 val_loss=50.451784 val_rmsd=18.7668 val_tm=0.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:45<00:00,  5.86it/s, loss=-37.4, stage=STRUCTURED(+CONF), lr=5e-5]\n",
      "Epoch 6/80 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:02<00:00, 11.20it/s, loss=-42.3, stage=STRUCTURED(+CONF), lr=5e-5, rmsd=19, tm=0.0481]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06] stage=STRUCTURED(+CONF) train_loss=-37.385646 val_loss=-42.317355 val_rmsd=18.9726 val_tm=0.0481\n",
      "ğŸ” Stage2 started: reset best_tm/stale\n",
      "âœ… best TM updated: 0.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80 [train]:  10%|â–ˆ         | 27/268 [00:04<00:43,  5.53it/s, loss=16.4, stage=STRUCTURED(+CONF), lr=5e-5]"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg: CFG, model: nn.Module, loss_fn: LossComposer, device):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        self.use_amp = bool(cfg.use_amp and str(device).startswith(\"cuda\"))\n",
    "        self.scaler = torch.amp.GradScaler('cuda', enabled=self.use_amp)\n",
    "\n",
    "    def build_scheduler(self, steps_per_epoch: int):\n",
    "        total_steps = self.cfg.epochs * steps_per_epoch\n",
    "        warmup_steps = self.cfg.warmup_epochs * steps_per_epoch\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return (step + 1) / max(1, warmup_steps)\n",
    "            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, lr_lambda)\n",
    "\n",
    "    def run_epoch(self, loader, epoch: int, train: bool):\n",
    "        self.model.train(train)\n",
    "\n",
    "        total_loss, steps = 0.0, 0\n",
    "        total_rmsd, rmsd_steps = 0.0, 0\n",
    "        total_tm, tm_steps = 0.0, 0\n",
    "        stage_name = None\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.cfg.epochs} [{'train' if train else 'eval'}]\")\n",
    "\n",
    "        for tokens, target, mask in pbar:\n",
    "            tokens = tokens.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            mask = mask.to(self.device)\n",
    "\n",
    "            fb = filter_batch(tokens, target, mask, self.cfg.min_valid)\n",
    "            if fb is None:\n",
    "                continue\n",
    "            tokens, target, mask = fb\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
    "                    coords, conf_logits = self.model(tokens)  # coords (B,K,T,3), conf (B,K) or None\n",
    "                    loss, stage_name = self.loss_fn(coords, target, mask, epoch, conf_logits=conf_logits)\n",
    "\n",
    "                if not train:\n",
    "                    rmsd = kabsch_rmsd_metric_min(coords, target, mask)\n",
    "                    tm = tm_score_metric_maxK(coords, target, mask)\n",
    "                    total_rmsd += float(rmsd.item()); rmsd_steps += 1\n",
    "                    total_tm += float(tm.item()); tm_steps += 1\n",
    "\n",
    "                if train:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                    self.scaler.step(self.opt)\n",
    "                    self.scaler.update()\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "\n",
    "            total_loss += float(loss.item()); steps += 1\n",
    "            lr = self.opt.param_groups[0][\"lr\"]\n",
    "            post = {\"loss\": total_loss / max(1, steps), \"stage\": stage_name, \"lr\": lr}\n",
    "            if (not train) and rmsd_steps > 0:\n",
    "                post[\"rmsd\"] = total_rmsd / rmsd_steps\n",
    "            if (not train) and tm_steps > 0:\n",
    "                post[\"tm\"] = total_tm / tm_steps\n",
    "            pbar.set_postfix(post)\n",
    "\n",
    "        avg_loss = total_loss / max(1, steps)\n",
    "        avg_rmsd = total_rmsd / max(1, rmsd_steps) if rmsd_steps > 0 else float(\"nan\")\n",
    "        avg_tm = total_tm / max(1, tm_steps) if tm_steps > 0 else float(\"nan\")\n",
    "        return avg_loss, stage_name, avg_rmsd, avg_tm\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        self.build_scheduler(len(train_loader))\n",
    "\n",
    "        best_tm = -1e9\n",
    "        stale = 0\n",
    "        stage2_started = False\n",
    "\n",
    "        best_stage1 = float(\"inf\")\n",
    "\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            tr_loss, stage_tr, _, _ = self.run_epoch(train_loader, epoch, train=True)\n",
    "            va_loss, stage_va, va_rmsd, va_tm = self.run_epoch(val_loader, epoch, train=False)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1:02d}] stage={stage_va} train_loss={tr_loss:.6f} val_loss={va_loss:.6f} val_rmsd={va_rmsd:.4f} val_tm={va_tm:.4f}\")\n",
    "\n",
    "            # Stage1 optional checkpoint (by val loss)\n",
    "            if epoch < self.cfg.warmup_epochs:\n",
    "                if va_loss < best_stage1 - 1e-4:\n",
    "                    best_stage1 = va_loss\n",
    "                    torch.save(self.model.state_dict(), self.cfg.ckpt_stage1_path)\n",
    "                    print(f\"ğŸ’¾ stage1 best loss updated: {best_stage1:.6f}\")\n",
    "                continue\n",
    "\n",
    "            # Stage2 starts: reset patience tracking once\n",
    "            if (not stage2_started):\n",
    "                stage2_started = True\n",
    "                best_tm = -1e9\n",
    "                stale = 0\n",
    "                print(\"ğŸ” Stage2 started: reset best_tm/stale\")\n",
    "\n",
    "            # Stage2: maximize TM-score (competition aligned)\n",
    "            if va_tm > best_tm + 1e-4:\n",
    "                best_tm = va_tm\n",
    "                stale = 0\n",
    "                torch.save(self.model.state_dict(), self.cfg.ckpt_path)\n",
    "                print(f\"âœ… best TM updated: {best_tm:.4f}\")\n",
    "            else:\n",
    "                stale += 1\n",
    "                print(f\"â¸ no TM improvement: {stale}/{self.cfg.patience}\")\n",
    "                if stale >= self.cfg.patience:\n",
    "                    print(\"ğŸ›‘ early stopping (Stage2, TM-based).\")\n",
    "                    break\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.cfg.ckpt_path, map_location=self.device))\n",
    "        print(\"Best model loaded:\", self.cfg.ckpt_path, \"best_tm=\", best_tm)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 9.5) Fit (stage-aware: Stage2 RMSD-based checkpoint)\n",
    "# ==========================================\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "trainer = Trainer(cfg, model, loss_fn, device)\n",
    "trainer.fit(train_loader, hold_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c7b74",
   "metadata": {},
   "source": [
    "## 6) Inference / Submission\n",
    "\n",
    "v10ì˜ ì¶”ë¡ /ì œì¶œ ì…€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ëœë‹¤. ë‹¨, ëª¨ë¸ ì¶œë ¥ì´ `(coords, conf_logits)` í˜•íƒœì´ë¯€ë¡œ ì¶”ë¡  ì‹œì—ëŠ” `coords`ë§Œ ì‚¬ìš©í•˜ë©´ ëœë‹¤.\n",
    "\n",
    "ì¶”ê°€ íŒ: `conf_logits`ë¡œ í›„ë³´ë¥¼ ì •ë ¬í•´ì„œ (ê°€ì¥ ì¢‹ì€ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ”) 1~2ê°œë¥¼ ì•ìª½ì— ë°°ì¹˜í•˜ë©´ ì•™ìƒë¸”ì´ë‚˜ í›„ì²˜ë¦¬ì— ìœ ë¦¬í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26792674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) Quick sanity check on holdout batch  [v10]\n",
    "#   - pick best head by TM-score (competition aligned) for sample 0\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    coords_b, conf_b = model(tokens_b)  # coords (B,K,T,3)\n",
    "\n",
    "# choose sample 0\n",
    "preds0 = coords_b[0]   # (K,T,3)\n",
    "tgt0 = tgt_b[0]        # (T,3)\n",
    "mask0 = m_b[0]         # (T,)\n",
    "\n",
    "# compute TM-score per head (after Kabsch), pick best\n",
    "tms = []\n",
    "aligned = []\n",
    "for k in range(preds0.size(0)):\n",
    "    pk = preds0[k:k+1]  # (1,T,3)\n",
    "    tk = tgt0.unsqueeze(0)\n",
    "    mk = mask0.unsqueeze(0)\n",
    "    pk_al = kabsch_align(pk, tk, mk)[0].detach().cpu()\n",
    "    aligned.append(pk_al)\n",
    "\n",
    "    # TM-score for this sample/head\n",
    "    tm = tm_score_single(pk, tk, mk)[0].item()\n",
    "    tms.append(tm)\n",
    "\n",
    "best_k = int(np.argmax(tms))\n",
    "pred_best = aligned[best_k].numpy()\n",
    "tgt_np = tgt0.detach().cpu().numpy()\n",
    "mask_np = mask0.detach().cpu().numpy()\n",
    "\n",
    "print(\"TM per k:\", [round(x,4) for x in tms], \"best_k:\", best_k)\n",
    "if conf_b is not None:\n",
    "    print(\"conf logits:\", conf_b[0].detach().cpu().numpy().round(3))\n",
    "\n",
    "valid = mask_np.astype(bool)\n",
    "x_t, y_t, z_t = tgt_np[valid].T\n",
    "x_p, y_p, z_p = pred_best[valid].T\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].scatter(x_t, y_t, s=10, label='target')\n",
    "axes[0].scatter(x_p, y_p, s=10, label='pred(aligned)')\n",
    "axes[0].set_title('x-y'); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(x_t, z_t, s=10)\n",
    "axes[1].scatter(x_p, z_p, s=10)\n",
    "axes[1].set_title('x-z')\n",
    "\n",
    "axes[2].scatter(y_t, z_t, s=10)\n",
    "axes[2].scatter(y_p, z_p, s=10)\n",
    "axes[2].set_title('y-z')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6549f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) Quick visualization on holdout batch (k=0)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_b = model(tokens_b)  # (B,K,T,3)\n",
    "pred0 = preds_b[0,0].detach().cpu().numpy()\n",
    "tgt0  = tgt_b[0].detach().cpu().numpy()\n",
    "m0    = m_b[0].detach().cpu().numpy().astype(bool)\n",
    "\n",
    "pred0 = pred0[m0]\n",
    "tgt0  = tgt0[m0]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(tgt0[:,0], tgt0[:,1], tgt0[:,2], s=6)\n",
    "ax.scatter(pred0[:,0], pred0[:,1], pred0[:,2], s=6)\n",
    "ax.set_title(\"Holdout sample (target vs pred k=0) - centered\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
