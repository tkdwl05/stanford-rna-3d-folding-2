{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6cdd8b",
   "metadata": {},
   "source": [
    "# improve_data_label_v7.ipynb\n",
    "\n",
    "- v7 changes:\n",
    "  - stage-aware early stopping + checkpointing (reset at stage switch)\n",
    "  - validation metric: Kabsch RMSD (min-over-K) logged and used for best model in stage2\n",
    "  - pairwise dist_w ramp-up after warmup\n",
    "  - updated AMP API (torch.amp.autocast / torch.amp.GradScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06254dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0) Imports, Device, Config\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 30\n",
    "\n",
    "    # model\n",
    "    n_tokens: int = 5           # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    num_preds: int = 5\n",
    "\n",
    "    # train\n",
    "    batch: int = 16\n",
    "    epochs: int = 60\n",
    "    warmup_epochs: int = 5\n",
    "    lr: float = 3e-5\n",
    "    wd: float = 1e-2\n",
    "    grad_clip: float = 1.0\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # loss\n",
    "    dist_w: float = 0.05\n",
    "    softmin_temp: float = 1.0   # softmin temperature for K heads\n",
    "    pair_num_pairs: int = 2048  # sampled pair count for pairwise loss\n",
    "\n",
    "    # amp\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # early stop\n",
    "    patience: int = 7\n",
    "    ckpt_path: str = \"best_structured_v7.pt\"\n",
    "    ckpt_stage1_path: str = \"best_stage1_v7.pt\"\n",
    "    dist_w_ramp_epochs: int = 5  # ramp dist_w over N epochs after warmup\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_all(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25102718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_8024\\648990858.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3980fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120aa1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa5e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (4750, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f292848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = np.asarray(self.tokens_list[idx], dtype=np.int64)\n",
    "        coords = np.asarray(self.coords_list[idx], dtype=np.float32)\n",
    "        mask   = np.asarray(self.mask_list[idx], dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        valid = mask.astype(bool)\n",
    "        if self.center_only and valid.sum() > 0:\n",
    "            center = coords[valid].mean(axis=0, keepdims=True)\n",
    "            coords = coords - center\n",
    "\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64);  tokens_p[:L] = tokens\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32); coords_p[:L] = coords\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32); mask_p[:L] = mask\n",
    "\n",
    "        return torch.tensor(tokens_p), torch.tensor(coords_p), torch.tensor(mask_p)\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccda3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 268 hold batches: 30\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_ds, batch_size=cfg.batch, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0226a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7) Losses + Composer  [v6.2]\n",
    "#   - softmin aggregation across K heads (stable, no dead heads)\n",
    "#   - Kabsch runs in FP32 even when AMP is enabled (fixes SVD half error)\n",
    "#   - pairwise distance loss uses sampled residue pairs (fast)\n",
    "# ==========================================\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = torch.softmax(-losses / max(temperature, 1e-8), dim=1)  # (B,K)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_loss(preds, target, mask, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"preds: (B,K,T,3), target: (B,T,3), mask: (B,T)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    losses = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        denom = m.sum(dim=(1,2)).clamp_min(1.0)\n",
    "        l1 = diff.sum(dim=(1,2)) / denom  # (B,)\n",
    "        losses.append(l1)\n",
    "    losses = torch.stack(losses, dim=1)  # (B,K)\n",
    "    return softmin_aggregate(losses, temperature)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. Runs SVD in FP32 for AMP safety.\n",
    "    P,Q: (B,T,3)  mask: (B,T) with 1 for valid coords\n",
    "    Returns aligned P: (B,T,3)\n",
    "    \"\"\"\n",
    "    # Always compute alignment in FP32 to avoid Half SVD kernels\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "\n",
    "        # FP32 batched SVD (supported)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_loss(preds, target, mask, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"Softmin RMSD after Kabsch alignment. Returns scalar.\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    per_k = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)  # safe under AMP\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        per_k.append(rmsd)\n",
    "    losses = torch.stack(per_k, dim=1)  # (B,K)\n",
    "    return softmin_aggregate(losses, temperature)\n",
    "\n",
    "def pairwise_distance_loss_sampled(preds, target, mask,\n",
    "                                   num_pairs: int = 2048,\n",
    "                                   temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"Sampled pairwise distance MSE. Returns scalar.\n",
    "    Builds per-sample, per-head (B,K) losses then softmin over K.\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    # precompute valid indices per sample on device\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            # leave zeros; will be effectively ignored by softmin since main loss dominates\n",
    "            continue\n",
    "\n",
    "        # sample pairs (with replacement)\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return softmin_aggregate(losses_bk, temperature)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Validation metric: Kabsch RMSD (min over K). Returns scalar (mean over batch).\n",
    "    Uses FP32 alignment for AMP safety.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        per_k = []\n",
    "        for k in range(K):\n",
    "            pk = preds[:, k]\n",
    "            pk_aligned = kabsch_align(pk, target, mask)\n",
    "            diff_sq = (pk_aligned - target) ** 2\n",
    "            sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "            n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "            rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "            per_k.append(rmsd)\n",
    "        losses = torch.stack(per_k, dim=1)  # (B,K)\n",
    "        return losses.min(dim=1).values.mean()\n",
    "\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch):\n",
    "        temp = self.cfg.softmin_temp\n",
    "\n",
    "        # Stage 1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            return masked_l1_loss(preds, target, mask, temperature=temp), \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2\n",
    "        main = kabsch_rmsd_loss(preds, target, mask, temperature=temp)\n",
    "\n",
    "        dist = pairwise_distance_loss_sampled(\n",
    "            preds,\n",
    "            target,\n",
    "            mask,\n",
    "            num_pairs=self.cfg.pair_num_pairs,\n",
    "            temperature=temp\n",
    "        )\n",
    "\n",
    "        # âœ… ì—¬ê¸°ì„œ epoch ì‚¬ìš© ê°€ëŠ¥\n",
    "        t = max(0, epoch - self.cfg.warmup_epochs)\n",
    "        ramp = min(1.0, t / max(1, self.cfg.dist_w_ramp_epochs))\n",
    "        dist_w_eff = self.cfg.dist_w * ramp\n",
    "\n",
    "        total = main + dist_w_eff * dist\n",
    "        return total, \"KABSCH+PAIR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94edaa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 5.267983 M\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 8) Model (Backbone + Head)\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,D)\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos).unsqueeze(0)\n",
    "\n",
    "class RNABackbone(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg.n_tokens, cfg.d_model, padding_idx=0)\n",
    "        self.pos = PositionalEncodingLearned(cfg.d_model, max_len=2048)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=cfg.layers)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        pad_mask = (tokens == 0)\n",
    "        x = self.emb(tokens)\n",
    "        x = self.pos(x)\n",
    "        x = self.enc(x, src_key_padding_mask=pad_mask)\n",
    "        return x, pad_mask\n",
    "\n",
    "class CoordHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.proj = nn.Linear(cfg.d_model, 3 * cfg.num_preds)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B,T,D) -> (B,K,T,3)\n",
    "        B, T, D = h.shape\n",
    "        out = self.proj(h).view(B, T, self.num_preds, 3).permute(0,2,1,3)\n",
    "        return out\n",
    "\n",
    "class RNAModel(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.backbone = RNABackbone(cfg)\n",
    "        self.head = CoordHead(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        h, _ = self.backbone(tokens)\n",
    "        coords = self.head(h)\n",
    "        return coords\n",
    "\n",
    "model = RNAModel(cfg).to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d36f246",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 73)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:73\u001b[1;36m\u001b[0m\n\u001b[1;33m    total_loss += float(loss.item())\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 9) Training (v7: stage-aware early stopping + RMSD metric + dist_w ramp + AMP API update)\n",
    "# ==========================================\n",
    "class Trainer:\n",
    "    def __init__(self, cfg: CFG, model: nn.Module, loss_fn: LossComposer, device):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        self.use_amp = bool(cfg.use_amp and (str(device).startswith(\"cuda\")))\n",
    "        self.scaler = torch.amp.GradScaler('cuda', enabled=self.use_amp)\n",
    "\n",
    "    def build_scheduler(self, train_steps_per_epoch: int):\n",
    "        total_steps = self.cfg.epochs * train_steps_per_epoch\n",
    "        warmup_steps = self.cfg.warmup_epochs * train_steps_per_epoch\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return (step + 1) / max(1, warmup_steps)\n",
    "            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, lr_lambda)\n",
    "\n",
    "    def run_epoch(self, loader, epoch: int, train: bool):\n",
    "        self.model.train(train)\n",
    "        total_loss, steps = 0.0, 0\n",
    "        total_rmsd, rmsd_steps = 0.0, 0\n",
    "        stage_name = None\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.cfg.epochs} [{'train' if train else 'eval'}]\")\n",
    "        for tokens, target, mask in pbar:\n",
    "            tokens = tokens.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            mask = mask.to(self.device)\n",
    "\n",
    "            fb = filter_batch(tokens, target, mask, self.cfg.min_valid)\n",
    "            if fb is None:\n",
    "                continue\n",
    "            tokens, target, mask = fb\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
    "                    preds = self.model(tokens)\n",
    "                    loss, stage_name = self.loss_fn(preds, target, mask, epoch)\n",
    "\n",
    "\n",
    "if not train:\n",
    "    # Always report a consistent metric across stages: Kabsch RMSD (min over K)\n",
    "    rmsd = kabsch_rmsd_metric_min(preds, target, mask)\n",
    "    total_rmsd += float(rmsd.item())\n",
    "    rmsd_steps += 1\n",
    "\n",
    "if train:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                    self.scaler.step(self.opt)\n",
    "                    self.scaler.update()\n",
    "\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            steps += 1\n",
    "            lr = self.opt.param_groups[0][\"lr\"]\n",
    "            post = {\"loss\": float(loss.item()), \"stage\": stage_name, \"lr\": lr}\n",
    "            if (not train) and rmsd_steps>0:\n",
    "                post[\"rmsd\"] = total_rmsd / rmsd_steps\n",
    "            pbar.set_postfix(post)\n",
    "\n",
    "        avg_loss = total_loss / max(1, steps)\n",
    "        avg_rmsd = total_rmsd / max(1, rmsd_steps)\n",
    "        return avg_loss, stage_name, avg_rmsd\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        self.build_scheduler(len(train_loader))\n",
    "        best = float(\"inf\")\n",
    "        stale = 0\n",
    "\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            tr, stage = self.run_epoch(train_loader, epoch, train=True)\n",
    "            va, _ = self.run_epoch(val_loader, epoch, train=False)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1:02d}] stage={stage} train={tr:.6f} val={va:.6f}\")\n",
    "\n",
    "            if va < best - 1e-4:\n",
    "                best = va\n",
    "                stale = 0\n",
    "                torch.save(self.model.state_dict(), self.cfg.ckpt_path)\n",
    "                print(f\"âœ… best updated: {best:.6f}\")\n",
    "            else:\n",
    "                stale += 1\n",
    "                print(f\"â¸ no improvement: {stale}/{self.cfg.patience}\")\n",
    "                if stale >= self.cfg.patience and epoch >= self.cfg.warmup_epochs:\n",
    "                    print(\"ðŸ›‘ early stopping.\")\n",
    "                    break\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.cfg.ckpt_path, map_location=self.device))\n",
    "        print(\"Best model loaded:\", self.cfg.ckpt_path, \"best_val=\", best)\n",
    "\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "trainer = Trainer(cfg, model, loss_fn, device)\n",
    "trainer.fit(train_loader, hold_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) Quick sanity check on holdout batch\n",
    "#   - pick best head (min Kabsch RMSD) for sample 0\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_b = model(tokens_b)  # (B,K,T,3)\n",
    "\n",
    "# choose sample 0\n",
    "preds0 = preds_b[0]   # (K,T,3)\n",
    "tgt0 = tgt_b[0]       # (T,3)\n",
    "mask0 = m_b[0]        # (T,)\n",
    "\n",
    "# compute RMSD per head after Kabsch, pick best\n",
    "rmsds = []\n",
    "aligned = []\n",
    "for k in range(preds0.size(0)):\n",
    "    pk = preds0[k:k+1]  # (1,T,3)\n",
    "    tk = tgt0.unsqueeze(0)\n",
    "    mk = mask0.unsqueeze(0)\n",
    "    pk_al = kabsch_align(pk, tk, mk)[0].detach().cpu()\n",
    "    aligned.append(pk_al)\n",
    "    diff_sq = (pk_al - tgt0.detach().cpu())**2\n",
    "    sum_sq = (diff_sq * mask0.detach().cpu().unsqueeze(-1)).sum()\n",
    "    n_valid = (mask0.detach().cpu().sum() * 3).clamp_min(1.0)\n",
    "    rmsd = torch.sqrt(sum_sq / n_valid + 1e-8).item()\n",
    "    rmsds.append(rmsd)\n",
    "\n",
    "best_k = int(np.argmin(rmsds))\n",
    "pred_best = aligned[best_k].numpy()\n",
    "tgt_np = tgt0.detach().cpu().numpy()\n",
    "mask_np = mask0.detach().cpu().numpy().astype(bool)\n",
    "\n",
    "print(\"best_k:\", best_k, \"RMSD:\", rmsds[best_k])\n",
    "\n",
    "# 2D projections (xy, xz, yz)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pairs = [(0,1,\"x-y\"), (0,2,\"x-z\"), (1,2,\"y-z\")]\n",
    "for ax,(i,j,title) in zip(axes, pairs):\n",
    "    ax.scatter(tgt_np[mask_np, i], tgt_np[mask_np, j], s=8, label=\"target\")\n",
    "    ax.scatter(pred_best[mask_np, i], pred_best[mask_np, j], s=8, label=\"pred(aligned)\")\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"equal\")\n",
    "axes[0].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
