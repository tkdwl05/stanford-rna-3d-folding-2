{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5ce781",
   "metadata": {},
   "source": [
    "# improve_data_label_v13\n",
    "\n",
    "v13: EGNN backbone + base-pair 확률(pair feature) 주입 + weak distance auxiliary (0.05)\n",
    "\n",
    "- 목표: topology(장거리 상호작용) 개선\n",
    "- 입력: train_sequences.csv / validation_sequences.csv, train_labels.csv / validation_labels.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v13.2_safe 변경점\n",
    "- Windows/Jupyter에서 `DataLoader(num_workers>0)`가 **멈춘 것처럼 보이는 현상**을 방지하기 위해 기본 `num_workers=0`으로 고정했다\n",
    "- `timeout=60`을 걸어 무한 대기를 피했다\n",
    "- 학습 시작 전에 **첫 배치 precheck**를 수행해 로더가 정상인지 즉시 확인한다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 0) Imports, Device, Config  [v13.2]\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "\n",
    "    # data\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 10\n",
    "    batch: int = 8\n",
    "    num_workers: int = 2\n",
    "\n",
    "    # model\n",
    "    vocab: int = 5  # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 192\n",
    "    d_edge: int = 256\n",
    "    n_layers: int = 8\n",
    "    k_nn: int = 12  # sequence neighborhood edges\n",
    "    num_preds: int = 4  # K heads\n",
    "\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # base-pair feature\n",
    "    bp_tau: float = 40.0         # distance decay for |i-j|\n",
    "    bp_min_sep: int = 4          # do not pair too-close residues\n",
    "    pair_alpha: float = 2.0      # message/coord weight boost: (1 + pair_alpha * p_ij)\n",
    "\n",
    "    # optimization\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 0.02\n",
    "    epochs: int = 20\n",
    "    warmup_epochs: int = 2\n",
    "\n",
    "    # softmin aggregation\n",
    "    softmin_temp: float = 1.0\n",
    "\n",
    "    # losses\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05\n",
    "\n",
    "    dist_w: float = 0.05         # weak distance auxiliary (as requested)\n",
    "    pair_num_pairs: int = 512    # sampled pairs for distance/repulsion (speed-tuned)\n",
    "    aux_every: int = 2          # compute expensive aux losses every N steps\n",
    "    profile_first_batch: bool = True  # set True to print data/to_gpu/forward timings\n",
    "    local_w: float = 0.2\n",
    "    var_w: float = 0.02\n",
    "    repulse_w: float = 0.02\n",
    "    diversity_w: float = 0.01\n",
    "    repulse_margin: float = 2.5\n",
    "    diversity_margin: float = 2.0\n",
    "\n",
    "cfg = CFG()\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73414564",
   "metadata": {},
   "source": [
    "## 1) Dataset / Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2e4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_2492\\799196925.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n",
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n",
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b18db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (5739, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n",
      "train batches: 646 hold batches: 72\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 직접 참조 (복사 최소화)\n",
    "        tokens = self.tokens_list[idx]\n",
    "        coords = self.coords_list[idx]\n",
    "        mask   = self.mask_list[idx]\n",
    "\n",
    "        # numpy 변환 (필요 시 1회만)\n",
    "        if not isinstance(tokens, np.ndarray):\n",
    "            tokens = np.array(tokens, dtype=np.int64)\n",
    "        if not isinstance(coords, np.ndarray):\n",
    "            coords = np.array(coords, dtype=np.float32)\n",
    "        if not isinstance(mask, np.ndarray):\n",
    "            mask = np.array(mask, dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        if self.center_only:\n",
    "            valid = mask.astype(bool)\n",
    "            if valid.any():\n",
    "                coords = coords - coords[valid].mean(axis=0, keepdims=True)\n",
    "\n",
    "        # padding (vectorized, 최소 연산)\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64)\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32)\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32)\n",
    "\n",
    "        tokens_p[:L] = tokens\n",
    "        coords_p[:L] = coords\n",
    "        mask_p[:L]   = mask\n",
    "\n",
    "        # torch.from_numpy (복사 없음 → 매우 빠름)\n",
    "        return (\n",
    "            torch.from_numpy(tokens_p),\n",
    "            torch.from_numpy(coords_p),\n",
    "            torch.from_numpy(mask_p),\n",
    "        )\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader  [v13.2 speed]\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "dl_num_workers = int(getattr(cfg, \"num_workers\", 0))\n",
    "# NOTE (Windows/Jupyter): num_workers>0 can hang silently depending on environment.\n",
    "# Start with 0 (safe). If you want speed, try 2 -> 4 gradually *only if it doesn't hang*.\n",
    "dl_num_workers=0 if dl_num_workers is None else int(dl_num_workers)\n",
    "\n",
    "dl_kwargs = dict(\n",
    "    num_workers=dl_num_workers,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    persistent_workers=(dl_num_workers > 0),\n",
    ")\n",
    "if dl_num_workers > 0:\n",
    "    dl_kwargs[\"prefetch_factor\"] = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch,\n",
    "    shuffle=True,\n",
    "    timeout=60,   # fail fast instead of hanging forever\n",
    "    **dl_kwargs\n",
    ")\n",
    "hold_loader = DataLoader(\n",
    "    hold_ds,\n",
    "    batch_size=cfg.batch,\n",
    "    shuffle=False,\n",
    "    timeout=60,\n",
    "    **dl_kwargs\n",
    ")\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n",
    "\n",
    "# ---- First-batch sanity check (detect loader hangs / preprocessing stalls) ----\n",
    "import time as _time\n",
    "_t0 = _time.time()\n",
    "try:\n",
    "    _it = iter(train_loader)\n",
    "    _b = next(_it)\n",
    "    print(f\"[loader precheck] first batch OK in {_time.time()-_t0:.2f}s\")\n",
    "    # print shapes\n",
    "    if isinstance(_b, (tuple, list)):\n",
    "        print(\"[loader precheck] batch shapes:\", [getattr(x, \"shape\", type(x)) for x in _b])\n",
    "except Exception as e:\n",
    "    print(\"[loader precheck] FAILED:\", repr(e))\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb878f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m)\n\u001b[0;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(it)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PROFILE] first batch time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== First batch performance profile =====\n",
    "import time\n",
    "t0 = time.time()\n",
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "print(\"[PROFILE] first batch time:\", time.time() - t0)\n",
    "print(\"[PROFILE] shapes:\", [x.shape for x in batch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4df894",
   "metadata": {},
   "source": [
    "## 2) Base-pair feature 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2) Base-pair probability encoder (sequence-only heuristic)\n",
    "#   - 입력 tokens(B,T) -> p(B,T,T) in [0,1]\n",
    "#   - canonical: A-U, C-G, G-U wobble\n",
    "#   - 거리 prior: exp(-|i-j|/tau), and |i-j|>=bp_min_sep\n",
    "# ==========================================\n",
    "class BasePairEncoder(nn.Module):\n",
    "    def __init__(self, tau: float = 40.0, min_sep: int = 4):\n",
    "        super().__init__()\n",
    "        self.tau = float(tau)\n",
    "        self.min_sep = int(min_sep)\n",
    "\n",
    "        # 0 PAD, 1 A,2 C,3 G,4 U\n",
    "        # canonical probs\n",
    "        P = torch.zeros((5,5), dtype=torch.float32)\n",
    "        P[1,4] = 1.0; P[4,1] = 1.0  # A-U\n",
    "        P[2,3] = 1.0; P[3,2] = 1.0  # C-G\n",
    "        P[3,4] = 0.6; P[4,3] = 0.6  # G-U wobble (weaker)\n",
    "        self.register_buffer(\"pair_table\", P, persistent=False)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, pad_mask: torch.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        tokens: (B,T) int64\n",
    "        pad_mask: (B,T) True for valid nodes (optional)\n",
    "        returns p: (B,T,T) float32\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        t_i = tokens[:, :, None]  # (B,T,1)\n",
    "        t_j = tokens[:, None, :]  # (B,1,T)\n",
    "\n",
    "        base = self.pair_table[t_i, t_j]  # (B,T,T)\n",
    "\n",
    "        # 거리 prior\n",
    "        idx = torch.arange(T, device=tokens.device)\n",
    "        dist = (idx[None, :, None] - idx[None, None, :]).abs().float()  # (1,T,T)\n",
    "        sep_ok = (dist >= float(self.min_sep)).float()\n",
    "        prior = torch.exp(-dist / max(self.tau, 1e-6)) * sep_ok\n",
    "\n",
    "        p = base * prior  # (B,T,T)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            m = pad_mask.float()\n",
    "            p = p * (m[:, :, None] * m[:, None, :])\n",
    "\n",
    "        # zero diagonal\n",
    "        p = p * (1.0 - torch.eye(T, device=p.device, dtype=p.dtype)[None, :, :])\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4f945",
   "metadata": {},
   "source": [
    "## 3) Loss helpers (Kabsch + sampled distance + local/var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eab1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    return torch.softmax(-losses / max(float(temperature), 1e-8), dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = softmin_weights(losses, temperature)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    out = []\n",
    "    denom = m.sum(dim=(1,2)).clamp_min(1.0)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        l1 = diff.sum(dim=(1,2)) / denom\n",
    "        out.append(l1)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        out.append(rmsd)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds, target, mask, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return losses_bk\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)  # (B,1,1)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]  # (B,T,3)\n",
    "        mean = (pk * m).sum(dim=1, keepdim=True) / denom\n",
    "        var = ((pk - mean) ** 2 * m).sum(dim=(1,2)) / denom.squeeze(1).squeeze(1).clamp_min(1.0)  # (B,)\n",
    "        out.append(var)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m_adj = (mask[:, 1:] * mask[:, :-1]).bool()  # (B,T-1)\n",
    "    tgt = (target[:, 1:] - target[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    out = []\n",
    "    denom = m_adj.sum(dim=1).clamp_min(1)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        l = (diff * m_adj).sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned - target).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2774e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# v11 추가: non-neighbor repulsion + head diversity\n",
    "# ==========================================\n",
    "def repulsion_losses_sampled(preds: torch.Tensor, mask: torch.Tensor,\n",
    "                             num_pairs: int = 2048, margin: float = 2.5) -> torch.Tensor:\n",
    "    \"\"\"겹침 방지용 hinge loss. 인접(i,i+1)은 제외하고 랜덤 pair에 대해\n",
    "    d < margin 이면 (margin-d)^2 를 부과.\n",
    "    returns (B,K)\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses = preds.new_zeros((B, K))\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 3:\n",
    "            continue\n",
    "\n",
    "        # sample pairs\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        # exclude neighbors\n",
    "        nn_mask = (torch.abs(i - j) > 1)\n",
    "        if nn_mask.sum() < 8:\n",
    "            continue\n",
    "        i = i[nn_mask]\n",
    "        j = j[nn_mask]\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            hinge = (margin - d).clamp_min(0.0)\n",
    "            losses[b, k] = (hinge * hinge).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "def head_diversity_losses(preds: torch.Tensor, mask: torch.Tensor, margin: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"헤드 간 유사하면 패널티. (B,K)로 반환해서 기존 softmin 프레임에 맞춘다.\n",
    "    각 헤드의 masked centered coords를 만들고, 헤드쌍 RMSD가 margin보다 작으면 hinge.\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "    # centered\n",
    "    centered = preds - (preds * m.unsqueeze(1)).sum(dim=2, keepdim=True) / denom.unsqueeze(1)\n",
    "\n",
    "    # pairwise head RMSD (no rotation; diversity 목적이라 단순 RMSD)\n",
    "    out = preds.new_zeros((B, K))\n",
    "    if K < 2:\n",
    "        return out\n",
    "\n",
    "    for a in range(K):\n",
    "        pen = 0.0\n",
    "        cnt = 0\n",
    "        for b in range(K):\n",
    "            if a == b: \n",
    "                continue\n",
    "            diff = (centered[:, a] - centered[:, b])**2  # (B,T,3)\n",
    "            rmsd = torch.sqrt((diff * m).sum(dim=(1,2)) / (mask.sum(dim=1).clamp_min(1.0)*3.0) + 1e-8)  # (B,)\n",
    "            hinge = (margin - rmsd).clamp_min(0.0)\n",
    "            pen = pen + hinge*hinge\n",
    "            cnt += 1\n",
    "        out[:, a] = pen / max(cnt, 1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58a6a7",
   "metadata": {},
   "source": [
    "## 4) Model (Pair-aware EGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4) Model (EGNN backbone + Pair-aware message passing + K coord heads + ConfidenceHead)  [v13]\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos)[None, :, :]\n",
    "\n",
    "def build_seq_edges(T: int, k: int, device):\n",
    "    r = max(1, k // 2)\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(T):\n",
    "        for j in range(max(0, i - r), min(T, i + r + 1)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            src.append(i); dst.append(j)\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long, device=device)  # (2,E)\n",
    "    return edge_index\n",
    "\n",
    "class EGNNPairAwareLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EGNN + pair feature p_ij (base-pair 확률)\n",
    "    message: phi(h_i, h_j, d_ij^2, p_ij)\n",
    "    + long-range boost: (1 + pair_alpha * p_ij) 를 coord/node 업데이트에 곱한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_node: int, d_edge: int, dropout: float, pair_alpha: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.pair_alpha = float(pair_alpha)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node*2 + 1 + 1, d_edge),  # +p_ij\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_edge, 1),\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node + d_edge, d_node),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_node, d_node),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(d_node)\n",
    "\n",
    "    def forward(self, h, x, edge_index, node_mask, pair_p):\n",
    "        \"\"\"\n",
    "        h: (B,T,D), x: (B,T,3)\n",
    "        edge_index: (2,E)\n",
    "        node_mask: (B,T) bool\n",
    "        pair_p: (B,T,T) float in [0,1]\n",
    "        \"\"\"\n",
    "        B, T, D = h.shape\n",
    "        src, dst = edge_index[0], edge_index[1]  # (E,)\n",
    "\n",
    "        hi = h[:, src, :]\n",
    "        hj = h[:, dst, :]\n",
    "        xi = x[:, src, :]\n",
    "        xj = x[:, dst, :]\n",
    "\n",
    "        rij = xi - xj\n",
    "        dij2 = (rij * rij).sum(dim=-1, keepdim=True)  # (B,E,1)\n",
    "        pij = pair_p[:, src, dst].unsqueeze(-1)        # (B,E,1)\n",
    "\n",
    "        m_ij = self.edge_mlp(torch.cat([hi, hj, dij2, pij], dim=-1))  # (B,E,d_edge)\n",
    "        boost = (1.0 + self.pair_alpha * pij).clamp(0.0, 10.0)\n",
    "\n",
    "        w = self.coord_mlp(m_ij) * boost  # (B,E,1)\n",
    "        dx = rij * w  # (B,E,3)\n",
    "\n",
    "        agg_dx = x.new_zeros((B, T, 3))\n",
    "        agg_m  = h.new_zeros((B, T, m_ij.size(-1)))\n",
    "\n",
    "        agg_dx.index_add_(1, src, dx.to(agg_dx.dtype))\n",
    "        agg_m.index_add_(1, src, (m_ij * boost).to(agg_m.dtype))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            agg_dx = agg_dx.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            agg_m  = agg_m.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "\n",
    "        x = x + agg_dx\n",
    "        h = self.ln(h + self.node_mlp(torch.cat([h, agg_m], dim=-1)))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            h = h.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            x = x.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "        return h, x\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(cfg.d_model//2, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        if pad_mask is None:\n",
    "            pooled = h.mean(dim=1)\n",
    "        else:\n",
    "            m = pad_mask.float().unsqueeze(-1)\n",
    "            denom = m.sum(dim=1).clamp_min(1.0)\n",
    "            pooled = (h * m).sum(dim=1) / denom\n",
    "        return self.mlp(pooled)  # (B,K)\n",
    "\n",
    "class EGNNv13(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(cfg.vocab, cfg.d_model, padding_idx=0)\n",
    "        self.posenc = PositionalEncodingLearned(cfg.d_model, max_len=cfg.max_len)\n",
    "        self.bp_enc = BasePairEncoder(tau=cfg.bp_tau, min_sep=cfg.bp_min_sep)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EGNNPairAwareLayer(cfg.d_model, cfg.d_edge, cfg.dropout, pair_alpha=cfg.pair_alpha)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        # K coord heads\n",
    "        self.coord_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(cfg.d_model, cfg.d_model),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(cfg.d_model, 3),\n",
    "            ) for _ in range(cfg.num_preds)\n",
    "        ])\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "    def forward(self, tokens, pad_mask):\n",
    "        \"\"\"\n",
    "        tokens: (B,T)\n",
    "        pad_mask: (B,T) bool\n",
    "        returns:\n",
    "          preds: (B,K,T,3)\n",
    "          conf_logits: (B,K)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        h = self.embed(tokens)\n",
    "        h = self.posenc(h)\n",
    "        if pad_mask is not None:\n",
    "            h = h.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        # init coords at 0\n",
    "        x = torch.zeros((B, T, 3), device=tokens.device, dtype=h.dtype)\n",
    "\n",
    "        edge_index = build_seq_edges(T, self.cfg.k_nn, tokens.device)\n",
    "\n",
    "        # pair probabilities (sequence-only heuristic)\n",
    "        pair_p = self.bp_enc(tokens, pad_mask)  # (B,T,T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h, x = layer(h, x, edge_index, pad_mask, pair_p)\n",
    "\n",
    "        preds = []\n",
    "        for head in self.coord_heads:\n",
    "            preds.append(head(h).unsqueeze(1))\n",
    "        preds = torch.cat(preds, dim=1)  # (B,K,T,3)\n",
    "\n",
    "        conf_logits = self.conf_head(h, pad_mask)  # (B,K)\n",
    "        return preds, conf_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9630",
   "metadata": {},
   "source": [
    "## 5) LossComposer (weak distance auxiliary 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3) LossComposer  [v13.2]\n",
    "#   - warmup: masked L1 (+optional confidence)\n",
    "#   - main: Kabsch RMSD + (optional sampled pairwise/repulsion) + local + variance + diversity (+confidence)\n",
    "#   - speed: compute expensive aux losses every cfg.aux_every steps\n",
    "# ==========================================\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, step: int = 0, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: structured losses (all are (B,K))\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)\n",
    "        var_bk  = coord_variance_losses(preds, mask)\n",
    "\n",
    "        # expensive aux losses: compute every N steps\n",
    "        aux_every = int(getattr(self.cfg, \"aux_every\", 1))\n",
    "        do_aux = (aux_every <= 1) or ((step % aux_every) == 0)\n",
    "\n",
    "        if do_aux:\n",
    "            num_pairs = int(self.cfg.pair_num_pairs)\n",
    "            pair_bk = pairwise_distance_losses_sampled(preds, target, mask, num_pairs=num_pairs)\n",
    "            rep_bk  = repulsion_losses_sampled(preds, mask, num_pairs=num_pairs,\n",
    "                                               margin=float(self.cfg.repulse_margin))\n",
    "        else:\n",
    "            B, K = preds.shape[0], preds.shape[1]\n",
    "            pair_bk = preds.new_zeros((B, K))\n",
    "            rep_bk  = preds.new_zeros((B, K))\n",
    "\n",
    "        # diversity is relatively cheap; keep every step\n",
    "        div_bk  = head_diversity_losses(preds, mask, margin=float(self.cfg.diversity_margin))\n",
    "\n",
    "        # weak auxiliary weights (as requested)\n",
    "        dist_w_eff = float(self.cfg.dist_w)\n",
    "        rep_w_eff  = float(self.cfg.repulse_w)\n",
    "\n",
    "        total_bk = (\n",
    "            rmsd_bk\n",
    "            + dist_w_eff * pair_bk\n",
    "            + float(self.cfg.local_w) * loc_bk\n",
    "            - float(self.cfg.var_w) * var_bk\n",
    "            + rep_w_eff * rep_bk\n",
    "            + float(self.cfg.diversity_w) * div_bk\n",
    "        )\n",
    "\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"STRUCTURED(+CONF)\" if aux != 0.0 else \"STRUCTURED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3584cf",
   "metadata": {},
   "source": [
    "## 6) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6) Train loop  [v13.2 speed]\n",
    "#   - non_blocking GPU transfers\n",
    "#   - optional profiling (cfg.profile_first_batch=True)\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "model = EGNNv13(cfg).to(device)\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "def run_epoch(loader, epoch: int, train: bool):\n",
    "    model.train(train)\n",
    "    total_loss, n = 0.0, 0\n",
    "    total_rmsd, n_r = 0.0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs} [{'train' if train else 'eval'}]\", mininterval=0.5)\n",
    "    for step, (tokens, target, mask) in enumerate(pbar):\n",
    "        # (optional) simple timing probe on step 0\n",
    "        do_prof = bool(getattr(cfg, \"profile_first_batch\", False)) and (step == 0)\n",
    "\n",
    "        if do_prof:\n",
    "            t0 = time.time()\n",
    "\n",
    "        tokens = tokens.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        mask   = mask.to(device, non_blocking=True)\n",
    "\n",
    "        if do_prof and device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "\n",
    "        fb = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "        if fb is None:\n",
    "            continue\n",
    "        tokens, target, mask = fb\n",
    "        pad_mask = (tokens != 0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            if do_prof:\n",
    "                t2 = time.time()\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n",
    "                preds, conf_logits = model(tokens, pad_mask)\n",
    "                loss, stage = loss_fn(preds, target, mask, epoch, step=step, conf_logits=conf_logits)\n",
    "\n",
    "            if do_prof and device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "                t3 = time.time()\n",
    "                print(f\"[profile] step0 to_gpu: {t1-t0:.3f}s  forward+loss: {t3-t2:.3f}s\")\n",
    "\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()); n += 1\n",
    "\n",
    "        if not train:\n",
    "            rmsd = kabsch_rmsd_metric_min(preds, target, mask)\n",
    "            total_rmsd += float(rmsd.item()); n_r += 1\n",
    "\n",
    "        pbar.set_postfix(loss=total_loss/max(n,1), stage=stage,\n",
    "                         rmsd=(total_rmsd/max(n_r,1) if n_r>0 else None))\n",
    "    return total_loss/max(n,1), (total_rmsd/max(n_r,1) if n_r>0 else None)\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    tr_loss, _ = run_epoch(train_loader, epoch, train=True)\n",
    "    ev_loss, ev_rmsd = run_epoch(hold_loader, epoch, train=False)\n",
    "    print(f\"[Epoch {epoch+1}] train_loss={tr_loss:.4f}  eval_loss={ev_loss:.4f}  eval_rmsd={ev_rmsd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e3366",
   "metadata": {},
   "source": [
    "## 7) Sanity check 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31348ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7) Quick sanity check plot (xy/xz/yz)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens, target, mask = next(iter(hold_loader))\n",
    "tokens = tokens.to(device); target = target.to(device); mask = mask.to(device)\n",
    "tokens, target, mask = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "pad_mask = (tokens != 0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds, conf_logits = model(tokens, pad_mask)  # (B,K,T,3)\n",
    "    # pick best head by TM-score for sample 0\n",
    "    tm_per_head = []\n",
    "    for k in range(cfg.num_preds):\n",
    "        tm = tm_score_single(preds[0:1, k], target[0:1], mask[0:1])\n",
    "        tm_per_head.append(float(tm.item()))\n",
    "    best_k = int(np.argmax(tm_per_head))\n",
    "    pred0 = preds[0, best_k].detach()\n",
    "    tgt0  = target[0].detach()\n",
    "    m0    = mask[0].detach().bool()\n",
    "\n",
    "    pred0a = kabsch_align(pred0.unsqueeze(0), tgt0.unsqueeze(0), mask[0:1]).squeeze(0)\n",
    "\n",
    "def scat(a, b, title, ax):\n",
    "    ax.scatter(a[:,0].cpu(), a[:,1].cpu(), s=10, label=\"target\")\n",
    "    ax.scatter(b[:,0].cpu(), b[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "A = tgt0[m0]\n",
    "B = pred0a[m0]\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "axes[0].scatter(A[:,0].cpu(), A[:,1].cpu(), s=10, label=\"target\")\n",
    "axes[0].scatter(B[:,0].cpu(), B[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[0].set_title(\"x-y\"); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(A[:,0].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[1].scatter(B[:,0].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[1].set_title(\"x-z\"); axes[1].legend()\n",
    "\n",
    "axes[2].scatter(A[:,1].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[2].scatter(B[:,1].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[2].set_title(\"y-z\"); axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"best head:\", best_k, \"tm per head:\", tm_per_head)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}