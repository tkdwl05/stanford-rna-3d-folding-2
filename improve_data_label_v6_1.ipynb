{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6cdd8b",
   "metadata": {},
   "source": [
    "# improve_data_label_v6_1.ipynb\n",
    "\n",
    "v6ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **êµ¬ì¡°(ëª¨ë“ˆí™”/ì±…ìž„ ë¶„ë¦¬/ì‹¤í—˜ ê´€ë¦¬)** ë¥¼ ê°•í™”í•œ ë²„ì „ì´ë‹¤, ë‚˜.\n",
    "\n",
    "í•µì‹¬ ë³€ê²½:\n",
    "- Datasetì—ì„œ **íƒ€ê¹ƒ ê¸°ë°˜ RMS ìŠ¤ì¼€ì¼ ì •ê·œí™” ì œê±°**(centeringë§Œ)\n",
    "- Config / Data / Model / Loss / Trainerë¡œ êµ¬ì¡°í™”\n",
    "- 2-stage loss(Masked L1 â†’ Kabsch) + Pairwise distance loss ìœ ì§€\n",
    "- early stopping + cosine(warmup í¬í•¨) ìœ ì§€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06254dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0) Imports, Device, Config\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 30\n",
    "\n",
    "    # model\n",
    "    n_tokens: int = 5           # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    num_preds: int = 5\n",
    "\n",
    "    # train\n",
    "    batch: int = 16\n",
    "    epochs: int = 60\n",
    "    warmup_epochs: int = 5\n",
    "    lr: float = 3e-5\n",
    "    wd: float = 1e-2\n",
    "    grad_clip: float = 1.0\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # loss\n",
    "    dist_w: float = 0.05\n",
    "    softmin_temp: float = 1.0   # softmin temperature for K heads\n",
    "    pair_num_pairs: int = 2048  # sampled pair count for pairwise loss\n",
    "\n",
    "    # amp\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # early stop\n",
    "    patience: int = 7\n",
    "    ckpt_path: str = \"best_structured_v6_1.pt\"\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_all(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25102718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_21604\\648990858.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3980fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120aa1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa5e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (4750, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f292848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = np.asarray(self.tokens_list[idx], dtype=np.int64)\n",
    "        coords = np.asarray(self.coords_list[idx], dtype=np.float32)\n",
    "        mask   = np.asarray(self.mask_list[idx], dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        valid = mask.astype(bool)\n",
    "        if self.center_only and valid.sum() > 0:\n",
    "            center = coords[valid].mean(axis=0, keepdims=True)\n",
    "            coords = coords - center\n",
    "\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64);  tokens_p[:L] = tokens\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32); coords_p[:L] = coords\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32); mask_p[:L] = mask\n",
    "\n",
    "        return torch.tensor(tokens_p), torch.tensor(coords_p), torch.tensor(mask_p)\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccda3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 268 hold batches: 30\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_ds, batch_size=cfg.batch, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0226a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7) Losses (Masked L1, Kabsch RMSD, Pairwise Distance MSE) + Composer  [v6.1]\n",
    "#   - min-over-K -> softmin aggregation (stable gradients)\n",
    "#   - pairwise full -> pairwise sampled (faster, less memory)\n",
    "# ==========================================\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = torch.softmax(-losses / max(temperature, 1e-8), dim=1)  # (B,K)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_loss(preds, target, mask, temperature: float = 1.0):\n",
    "    # preds: (B,K,T,3)  target: (B,T,3)  mask: (B,T)\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    losses = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        denom = m.sum(dim=(1,2)).clamp_min(1.0)\n",
    "        l1 = diff.sum(dim=(1,2)) / denom  # (B,)\n",
    "        losses.append(l1)\n",
    "    losses = torch.stack(losses, dim=1)  # (B,K)\n",
    "    return softmin_aggregate(losses, temperature)\n",
    "\n",
    "def kabsch_rmsd_loss(preds, target, mask, temperature=1.0):\n",
    "    B, K, T, _ = preds.shape\n",
    "    per_k = []\n",
    "\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "\n",
    "        # âœ… Kabschë§Œ autocast OFF (FP32)\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            pk32 = pk.float()\n",
    "            tgt32 = target.float()\n",
    "            m32 = mask.float()\n",
    "            pk_aligned = kabsch_align(pk32, tgt32, m32).to(pk.dtype)\n",
    "\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)\n",
    "        per_k.append(rmsd)\n",
    "\n",
    "    losses = torch.stack(per_k, dim=1)  # (B,K)\n",
    "    return softmin_aggregate(losses, temperature=temperature)\n",
    "\n",
    "\n",
    "def kabsch_rmsd_loss(preds, target, mask, temperature: float = 1.0):\n",
    "    B, K, T, _ = preds.shape\n",
    "    losses = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1,2))\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        losses.append(rmsd)\n",
    "    losses = torch.stack(losses, dim=1)  # (B,K)\n",
    "    return softmin_aggregate(losses, temperature)\n",
    "\n",
    "def pairwise_distance_loss_sampled(preds, target, mask, num_pairs: int = 2048, temperature: float = 1.0):\n",
    "    \"\"\"Sampled pairwise distance MSE (aggregated across K with softmin).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    losses_k = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        per_b = []\n",
    "        for b in range(B):\n",
    "            idx = valid_indices[b]\n",
    "            n = idx.numel()\n",
    "            if n < 2:\n",
    "                continue\n",
    "            i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "            j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "            d_pred = (pk[b, i] - pk[b, j]).norm(dim=-1)\n",
    "            d_true = (target[b, i] - target[b, j]).norm(dim=-1)\n",
    "            per_b.append(((d_pred - d_true) ** 2).mean())\n",
    "        if len(per_b) == 0:\n",
    "            losses_k.append(torch.zeros((), device=device_))\n",
    "        else:\n",
    "            losses_k.append(torch.stack(per_b).mean())\n",
    "\n",
    "    losses_k = torch.stack(losses_k)  # (K,)\n",
    "    w = torch.softmax(-losses_k / max(temperature, 1e-8), dim=0)\n",
    "    return (w * losses_k).sum()\n",
    "\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch: int):\n",
    "        temp = self.cfg.softmin_temp\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            return masked_l1_loss(preds, target, mask, temperature=temp), \"MASKED_L1(softmin)\"\n",
    "        main = kabsch_rmsd_loss(preds, target, mask, temperature=temp)\n",
    "        dist = pairwise_distance_loss_sampled(preds, target, mask,\n",
    "                                             num_pairs=self.cfg.pair_num_pairs,\n",
    "                                             temperature=temp)\n",
    "        return main + self.cfg.dist_w * dist, \"KABSCH(softmin)+PAIR(sampled)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94edaa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 5.267983 M\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 8) Model (Backbone + Head)\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,D)\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos).unsqueeze(0)\n",
    "\n",
    "class RNABackbone(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg.n_tokens, cfg.d_model, padding_idx=0)\n",
    "        self.pos = PositionalEncodingLearned(cfg.d_model, max_len=2048)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=cfg.layers)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        pad_mask = (tokens == 0)\n",
    "        x = self.emb(tokens)\n",
    "        x = self.pos(x)\n",
    "        x = self.enc(x, src_key_padding_mask=pad_mask)\n",
    "        return x, pad_mask\n",
    "\n",
    "class CoordHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.proj = nn.Linear(cfg.d_model, 3 * cfg.num_preds)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B,T,D) -> (B,K,T,3)\n",
    "        B, T, D = h.shape\n",
    "        out = self.proj(h).view(B, T, self.num_preds, 3).permute(0,2,1,3)\n",
    "        return out\n",
    "\n",
    "class RNAModel(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.backbone = RNABackbone(cfg)\n",
    "        self.head = CoordHead(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        h, _ = self.backbone(tokens)\n",
    "        coords = self.head(h)\n",
    "        return coords\n",
    "\n",
    "model = RNAModel(cfg).to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d36f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_21604\\3084461300.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
      "Epoch 1/60 [train]:   0%|          | 0/268 [00:00<?, ?it/s]C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_21604\\3084461300.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
      "Epoch 1/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:15<00:00, 17.61it/s, loss=48.6, stage=MASKED_L1(softmin), lr=6.02e-6]\n",
      "Epoch 1/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 36.24it/s, loss=60.7, stage=MASKED_L1(softmin), lr=6.02e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] stage=MASKED_L1(softmin) train=52.686225 val=54.442373\n",
      "âœ… best updated: 54.442373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:15<00:00, 17.47it/s, loss=45.4, stage=MASKED_L1(softmin), lr=1.2e-5] \n",
      "Epoch 2/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 32.31it/s, loss=60.7, stage=MASKED_L1(softmin), lr=1.2e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] stage=MASKED_L1(softmin) train=52.673494 val=54.440674\n",
      "âœ… best updated: 54.440674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:15<00:00, 17.85it/s, loss=78, stage=MASKED_L1(softmin), lr=1.8e-5]   \n",
      "Epoch 3/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 71.71it/s, loss=60.7, stage=MASKED_L1(softmin), lr=1.8e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] stage=MASKED_L1(softmin) train=52.769332 val=54.440844\n",
      "â¸ no improvement: 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.84it/s, loss=51.3, stage=MASKED_L1(softmin), lr=2.4e-5] \n",
      "Epoch 4/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 70.62it/s, loss=60.7, stage=MASKED_L1(softmin), lr=2.4e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] stage=MASKED_L1(softmin) train=52.688195 val=54.439472\n",
      "âœ… best updated: 54.439472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.98it/s, loss=64.8, stage=MASKED_L1(softmin), lr=3e-5]   \n",
      "Epoch 5/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 69.56it/s, loss=60.7, stage=MASKED_L1(softmin), lr=3e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] stage=MASKED_L1(softmin) train=52.731108 val=54.443864\n",
      "â¸ no improvement: 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/60 [train]:   0%|          | 0/268 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kabsch_align' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m LossComposer(cfg)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    102\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(cfg, model, loss_fn, device)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhold_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 81\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[0;32m     78\u001b[0m stale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m---> 81\u001b[0m     tr, stage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     va, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_epoch(val_loader, epoch, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] stage=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m val=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mTrainer.run_epoch\u001b[1;34m(self, loader, epoch, train)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_amp):\n\u001b[0;32m     51\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(tokens)\n\u001b[1;32m---> 52\u001b[0m     loss, stage_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tkdwl\\anaconda3\\envs\\rna_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tkdwl\\anaconda3\\envs\\rna_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 102\u001b[0m, in \u001b[0;36mLossComposer.forward\u001b[1;34m(self, preds, target, mask, epoch)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mwarmup_epochs:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m masked_l1_loss(preds, target, mask, temperature\u001b[38;5;241m=\u001b[39mtemp), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASKED_L1(softmin)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 102\u001b[0m main \u001b[38;5;241m=\u001b[39m \u001b[43mkabsch_rmsd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m dist \u001b[38;5;241m=\u001b[39m pairwise_distance_loss_sampled(preds, target, mask,\n\u001b[0;32m    104\u001b[0m                                      num_pairs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpair_num_pairs,\n\u001b[0;32m    105\u001b[0m                                      temperature\u001b[38;5;241m=\u001b[39mtemp)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m main \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdist_w \u001b[38;5;241m*\u001b[39m dist, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKABSCH(softmin)+PAIR(sampled)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m, in \u001b[0;36mkabsch_rmsd_loss\u001b[1;34m(preds, target, mask, temperature)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[0;32m     54\u001b[0m     pk \u001b[38;5;241m=\u001b[39m preds[:, k]\n\u001b[1;32m---> 55\u001b[0m     pk_aligned \u001b[38;5;241m=\u001b[39m \u001b[43mkabsch_align\u001b[49m(pk, target, mask)\n\u001b[0;32m     56\u001b[0m     diff_sq \u001b[38;5;241m=\u001b[39m (pk_aligned \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     57\u001b[0m     sum_sq \u001b[38;5;241m=\u001b[39m (diff_sq \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kabsch_align' is not defined"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 9) Training (v6.1: AMP + warmup+cosine + early stopping)\n",
    "# ==========================================\n",
    "class Trainer:\n",
    "    def __init__(self, cfg: CFG, model: nn.Module, loss_fn: LossComposer, device):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        self.use_amp = bool(cfg.use_amp and (str(device).startswith(\"cuda\")))\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
    "\n",
    "    def build_scheduler(self, train_steps_per_epoch: int):\n",
    "        total_steps = self.cfg.epochs * train_steps_per_epoch\n",
    "        warmup_steps = self.cfg.warmup_epochs * train_steps_per_epoch\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return (step + 1) / max(1, warmup_steps)\n",
    "            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, lr_lambda)\n",
    "\n",
    "    def run_epoch(self, loader, epoch: int, train: bool):\n",
    "        self.model.train(train)\n",
    "        total_loss, steps = 0.0, 0\n",
    "        stage_name = None\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.cfg.epochs} [{'train' if train else 'eval'}]\")\n",
    "        for tokens, target, mask in pbar:\n",
    "            tokens = tokens.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            mask = mask.to(self.device)\n",
    "\n",
    "            fb = filter_batch(tokens, target, mask, self.cfg.min_valid)\n",
    "            if fb is None:\n",
    "                continue\n",
    "            tokens, target, mask = fb\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "                    preds = self.model(tokens)\n",
    "                    loss, stage_name = self.loss_fn(preds, target, mask, epoch)\n",
    "\n",
    "                if not torch.isfinite(loss):\n",
    "                    continue\n",
    "\n",
    "                if train:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                    self.scaler.step(self.opt)\n",
    "                    self.scaler.update()\n",
    "\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "            steps += 1\n",
    "            lr = self.opt.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix({\"loss\": float(loss.item()), \"stage\": stage_name, \"lr\": lr})\n",
    "\n",
    "        return total_loss / max(1, steps), stage_name\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        self.build_scheduler(len(train_loader))\n",
    "        best = float(\"inf\")\n",
    "        stale = 0\n",
    "\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            tr, stage = self.run_epoch(train_loader, epoch, train=True)\n",
    "            va, _ = self.run_epoch(val_loader, epoch, train=False)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1:02d}] stage={stage} train={tr:.6f} val={va:.6f}\")\n",
    "\n",
    "            if va < best - 1e-4:\n",
    "                best = va\n",
    "                stale = 0\n",
    "                torch.save(self.model.state_dict(), self.cfg.ckpt_path)\n",
    "                print(f\"âœ… best updated: {best:.6f}\")\n",
    "            else:\n",
    "                stale += 1\n",
    "                print(f\"â¸ no improvement: {stale}/{self.cfg.patience}\")\n",
    "                if stale >= self.cfg.patience and epoch >= self.cfg.warmup_epochs:\n",
    "                    print(\"ðŸ›‘ early stopping.\")\n",
    "                    break\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.cfg.ckpt_path, map_location=self.device))\n",
    "        print(\"Best model loaded:\", self.cfg.ckpt_path, \"best_val=\", best)\n",
    "\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "trainer = Trainer(cfg, model, loss_fn, device)\n",
    "trainer.fit(train_loader, hold_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) Quick visualization on holdout batch (k=0)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_b = model(tokens_b)  # (B,K,T,3)\n",
    "pred0 = preds_b[0,0].detach().cpu().numpy()\n",
    "tgt0  = tgt_b[0].detach().cpu().numpy()\n",
    "m0    = m_b[0].detach().cpu().numpy().astype(bool)\n",
    "\n",
    "pred0 = pred0[m0]\n",
    "tgt0  = tgt0[m0]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(tgt0[:,0], tgt0[:,1], tgt0[:,2], s=6)\n",
    "ax.scatter(pred0[:,0], pred0[:,1], pred0[:,2], s=6)\n",
    "ax.set_title(\"Holdout sample (target vs pred k=0) - centered\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
