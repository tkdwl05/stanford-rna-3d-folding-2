{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6cdd8b",
   "metadata": {},
   "source": [
    "# improve_data_label_v9.ipynb\n",
    "\n",
    "- v9: TM-score validation metric (best-of-5), stage-aware checkpointing, dist_w ramp, AMP-safe Kabsch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06254dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0) Imports, Device, Config\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 30\n",
    "\n",
    "    # model\n",
    "    n_tokens: int = 5           # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    num_preds: int = 5\n",
    "\n",
    "    # train\n",
    "    batch: int = 16\n",
    "    epochs: int = 60\n",
    "    warmup_epochs: int = 5\n",
    "    lr: float = 3e-5\n",
    "    wd: float = 1e-2\n",
    "    grad_clip: float = 1.0\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # loss\n",
    "    dist_w: float = 0.10\n",
    "    softmin_temp: float = 0.30   # softmin temperature for K heads\n",
    "    pair_num_pairs: int = 2048  # sampled pair count for pairwise loss\n",
    "\n",
    "    # regularizers\n",
    "    var_w: float = 0.01         # encourage coordinate spread (anti-collapse)\n",
    "    local_w: float = 0.10       # adjacent bond length supervision\n",
    "\n",
    "    # candidate confidence (ranking)\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05        # aux loss weight for confidence head\n",
    "\n",
    "    # amp\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # early stop\n",
    "    patience: int = 7\n",
    "    ckpt_path: str = \"best_structured_v10.pt\"\n",
    "    ckpt_stage1_path: str = \"best_stage1_v10.pt\"\n",
    "    dist_w_ramp_epochs: int = 5  # ramp dist_w over N epochs after warmup\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_all(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25102718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120aa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = np.asarray(self.tokens_list[idx], dtype=np.int64)\n",
    "        coords = np.asarray(self.coords_list[idx], dtype=np.float32)\n",
    "        mask   = np.asarray(self.mask_list[idx], dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        valid = mask.astype(bool)\n",
    "        if self.center_only and valid.sum() > 0:\n",
    "            center = coords[valid].mean(axis=0, keepdims=True)\n",
    "            coords = coords - center\n",
    "\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64);  tokens_p[:L] = tokens\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32); coords_p[:L] = coords\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32); mask_p[:L] = mask\n",
    "\n",
    "        return torch.tensor(tokens_p), torch.tensor(coords_p), torch.tensor(mask_p)\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccda3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_ds, batch_size=cfg.batch, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7) Losses + Composer  [v10]\n",
    "#   - per-head losses (B,K) so we can:\n",
    "#       (1) aggregate with softmin for stability\n",
    "#       (2) train a confidence head to rank candidates (aux loss)\n",
    "#   - Kabsch runs in FP32 even when AMP is enabled\n",
    "#   - pairwise distance loss uses sampled residue pairs (fast)\n",
    "#   - anti-collapse + local bond regularizers\n",
    "# ==========================================\n",
    "\n",
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    return torch.softmax(-losses / max(float(temperature), 1e-8), dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = softmin_weights(losses, temperature)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    out = []\n",
    "    denom = m.sum(dim=(1,2)).clamp_min(1.0)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        l1 = diff.sum(dim=(1,2)) / denom\n",
    "        out.append(l1)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        out.append(rmsd)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds, target, mask, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return losses_bk\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)  # (B,1,1)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]  # (B,T,3)\n",
    "        mean = (pk * m).sum(dim=1, keepdim=True) / denom\n",
    "        var = ((pk - mean) ** 2 * m).sum(dim=(1,2)) / denom.squeeze(1).squeeze(1).clamp_min(1.0)  # (B,)\n",
    "        out.append(var)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m_adj = (mask[:, 1:] * mask[:, :-1]).bool()  # (B,T-1)\n",
    "    tgt = (target[:, 1:] - target[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    out = []\n",
    "    denom = m_adj.sum(dim=1).clamp_min(1)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        l = (diff * m_adj).sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned - target).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n",
    "\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "            # confidence aux (optional)\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)  # (B,K)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: Kabsch + pairwise + regularizers\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)                      # (B,K)\n",
    "        pair_bk = pairwise_distance_losses_sampled(preds, target, mask,\n",
    "                                                   num_pairs=int(self.cfg.pair_num_pairs))  # (B,K)\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)                        # (B,K)\n",
    "        var_bk  = coord_variance_losses(preds, mask)                            # (B,K)\n",
    "\n",
    "        # ramp pairwise weight after warmup\n",
    "        t = max(0, int(epoch - self.cfg.warmup_epochs))\n",
    "        ramp = min(1.0, t / max(1, int(self.cfg.dist_w_ramp_epochs)))\n",
    "        dist_w_eff = float(self.cfg.dist_w) * ramp\n",
    "\n",
    "        total_bk = rmsd_bk + dist_w_eff * pair_bk + float(self.cfg.local_w) * loc_bk - float(self.cfg.var_w) * var_bk\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        # confidence aux: learn to rank candidates by total_bk\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)  # (B,K)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"KABSCH+PAIR+REG(+CONF)\" if aux != 0.0 else \"KABSCH+PAIR+REG\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8) Model (Backbone + Coord Head + Confidence Head)  [v10]\n",
    "#   - outputs: coords (B,K,T,3) and conf_logits (B,K)\n",
    "#   - conf head is trained to predict which candidate is best (auxiliary)\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos).unsqueeze(0)\n",
    "\n",
    "class RNABackbone(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg.n_tokens, cfg.d_model, padding_idx=0)\n",
    "        self.pos = PositionalEncodingLearned(cfg.d_model, max_len=2048)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=cfg.layers)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        pad_mask = (tokens == 0)\n",
    "        x = self.emb(tokens)\n",
    "        x = self.pos(x)\n",
    "        x = self.enc(x, src_key_padding_mask=pad_mask)\n",
    "        return x, pad_mask\n",
    "\n",
    "class CoordHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.proj = nn.Linear(cfg.d_model, 3 * cfg.num_preds)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B,T,D) -> (B,K,T,3)\n",
    "        B, T, D = h.shape\n",
    "        out = self.proj(h).view(B, T, self.num_preds, 3).permute(0, 2, 1, 3).contiguous()\n",
    "        return out\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.d_model, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        # pool over residues (masked mean)\n",
    "        m = (~pad_mask).float().unsqueeze(-1)  # (B,T,1)\n",
    "        denom = m.sum(dim=1).clamp_min(1.0)\n",
    "        pooled = (h * m).sum(dim=1) / denom  # (B,D)\n",
    "        logits = self.mlp(pooled)  # (B,K)\n",
    "        return logits\n",
    "\n",
    "class RNAModel(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = RNABackbone(cfg)\n",
    "        self.coord_head = CoordHead(cfg)\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        h, pad_mask = self.backbone(tokens)\n",
    "        coords = self.coord_head(h)\n",
    "        conf_logits = self.conf_head(h, pad_mask) if self.cfg.use_confidence else None\n",
    "        return coords, conf_logits\n",
    "\n",
    "model = RNAModel(cfg).to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9) Training (v10)\n",
    "#   - Stage1: optimize masked L1 (softmin) + optional confidence aux\n",
    "#   - Stage2: optimize Kabsch+pair+reg + confidence aux\n",
    "#   - Checkpoint/EarlyStop: use val TM-score (maximize) to match Kaggle best-of-5\n",
    "#   - Also logs RMSD for diagnostics\n",
    "# ==========================================\n",
    "class Trainer:\n",
    "    def __init__(self, cfg: CFG, model: nn.Module, loss_fn: LossComposer, device):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        self.use_amp = bool(cfg.use_amp and str(device).startswith(\"cuda\"))\n",
    "        self.scaler = torch.amp.GradScaler('cuda', enabled=self.use_amp)\n",
    "\n",
    "    def build_scheduler(self, steps_per_epoch: int):\n",
    "        total_steps = self.cfg.epochs * steps_per_epoch\n",
    "        warmup_steps = self.cfg.warmup_epochs * steps_per_epoch\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return (step + 1) / max(1, warmup_steps)\n",
    "            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, lr_lambda)\n",
    "\n",
    "    def run_epoch(self, loader, epoch: int, train: bool):\n",
    "        self.model.train(train)\n",
    "\n",
    "        total_loss, steps = 0.0, 0\n",
    "        total_rmsd, rmsd_steps = 0.0, 0\n",
    "        total_tm, tm_steps = 0.0, 0\n",
    "        stage_name = None\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.cfg.epochs} [{'train' if train else 'eval'}]\")\n",
    "\n",
    "        for tokens, target, mask in pbar:\n",
    "            tokens = tokens.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            mask = mask.to(self.device)\n",
    "\n",
    "            fb = filter_batch(tokens, target, mask, self.cfg.min_valid)\n",
    "            if fb is None:\n",
    "                continue\n",
    "            tokens, target, mask = fb\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
    "                    coords, conf_logits = self.model(tokens)  # coords (B,K,T,3), conf (B,K) or None\n",
    "                    loss, stage_name = self.loss_fn(coords, target, mask, epoch, conf_logits=conf_logits)\n",
    "\n",
    "                if not train:\n",
    "                    rmsd = kabsch_rmsd_metric_min(coords, target, mask)\n",
    "                    tm = tm_score_metric_maxK(coords, target, mask)\n",
    "                    total_rmsd += float(rmsd.item()); rmsd_steps += 1\n",
    "                    total_tm += float(tm.item()); tm_steps += 1\n",
    "\n",
    "                if train:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                    self.scaler.step(self.opt)\n",
    "                    self.scaler.update()\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "\n",
    "            total_loss += float(loss.item()); steps += 1\n",
    "            lr = self.opt.param_groups[0][\"lr\"]\n",
    "            post = {\"loss\": total_loss / max(1, steps), \"stage\": stage_name, \"lr\": lr}\n",
    "            if (not train) and rmsd_steps > 0:\n",
    "                post[\"rmsd\"] = total_rmsd / rmsd_steps\n",
    "            if (not train) and tm_steps > 0:\n",
    "                post[\"tm\"] = total_tm / tm_steps\n",
    "            pbar.set_postfix(post)\n",
    "\n",
    "        avg_loss = total_loss / max(1, steps)\n",
    "        avg_rmsd = total_rmsd / max(1, rmsd_steps) if rmsd_steps > 0 else float(\"nan\")\n",
    "        avg_tm = total_tm / max(1, tm_steps) if tm_steps > 0 else float(\"nan\")\n",
    "        return avg_loss, stage_name, avg_rmsd, avg_tm\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        self.build_scheduler(len(train_loader))\n",
    "\n",
    "        best_tm = -1e9\n",
    "        stale = 0\n",
    "        stage2_started = False\n",
    "\n",
    "        best_stage1 = float(\"inf\")\n",
    "\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            tr_loss, stage_tr, _, _ = self.run_epoch(train_loader, epoch, train=True)\n",
    "            va_loss, stage_va, va_rmsd, va_tm = self.run_epoch(val_loader, epoch, train=False)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1:02d}] stage={stage_va} train_loss={tr_loss:.6f} val_loss={va_loss:.6f} val_rmsd={va_rmsd:.4f} val_tm={va_tm:.4f}\")\n",
    "\n",
    "            # Stage1 optional checkpoint (by val loss)\n",
    "            if epoch < self.cfg.warmup_epochs:\n",
    "                if va_loss < best_stage1 - 1e-4:\n",
    "                    best_stage1 = va_loss\n",
    "                    torch.save(self.model.state_dict(), self.cfg.ckpt_stage1_path)\n",
    "                    print(f\"ðŸ’¾ stage1 best loss updated: {best_stage1:.6f}\")\n",
    "                continue\n",
    "\n",
    "            # Stage2 starts: reset patience tracking once\n",
    "            if (not stage2_started):\n",
    "                stage2_started = True\n",
    "                best_tm = -1e9\n",
    "                stale = 0\n",
    "                print(\"ðŸ” Stage2 started: reset best_tm/stale\")\n",
    "\n",
    "            # Stage2: maximize TM-score (competition aligned)\n",
    "            if va_tm > best_tm + 1e-4:\n",
    "                best_tm = va_tm\n",
    "                stale = 0\n",
    "                torch.save(self.model.state_dict(), self.cfg.ckpt_path)\n",
    "                print(f\"âœ… best TM updated: {best_tm:.4f}\")\n",
    "            else:\n",
    "                stale += 1\n",
    "                print(f\"â¸ no TM improvement: {stale}/{self.cfg.patience}\")\n",
    "                if stale >= self.cfg.patience:\n",
    "                    print(\"ðŸ›‘ early stopping (Stage2, TM-based).\")\n",
    "                    break\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.cfg.ckpt_path, map_location=self.device))\n",
    "        print(\"Best model loaded:\", self.cfg.ckpt_path, \"best_tm=\", best_tm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9.5) Fit (stage-aware: Stage2 RMSD-based checkpoint)\n",
    "# ==========================================\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "trainer = Trainer(cfg, model, loss_fn, device)\n",
    "trainer.fit(train_loader, hold_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) Quick sanity check on holdout batch  [v10]\n",
    "#   - pick best head by TM-score (competition aligned) for sample 0\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    coords_b, conf_b = model(tokens_b)  # coords (B,K,T,3)\n",
    "\n",
    "# choose sample 0\n",
    "preds0 = coords_b[0]   # (K,T,3)\n",
    "tgt0 = tgt_b[0]        # (T,3)\n",
    "mask0 = m_b[0]         # (T,)\n",
    "\n",
    "# compute TM-score per head (after Kabsch), pick best\n",
    "tms = []\n",
    "aligned = []\n",
    "for k in range(preds0.size(0)):\n",
    "    pk = preds0[k:k+1]  # (1,T,3)\n",
    "    tk = tgt0.unsqueeze(0)\n",
    "    mk = mask0.unsqueeze(0)\n",
    "    pk_al = kabsch_align(pk, tk, mk)[0].detach().cpu()\n",
    "    aligned.append(pk_al)\n",
    "\n",
    "    # TM-score for this sample/head\n",
    "    tm = tm_score_single(pk, tk, mk)[0].item()\n",
    "    tms.append(tm)\n",
    "\n",
    "best_k = int(np.argmax(tms))\n",
    "pred_best = aligned[best_k].numpy()\n",
    "tgt_np = tgt0.detach().cpu().numpy()\n",
    "mask_np = mask0.detach().cpu().numpy()\n",
    "\n",
    "print(\"TM per k:\", [round(x,4) for x in tms], \"best_k:\", best_k)\n",
    "if conf_b is not None:\n",
    "    print(\"conf logits:\", conf_b[0].detach().cpu().numpy().round(3))\n",
    "\n",
    "valid = mask_np.astype(bool)\n",
    "x_t, y_t, z_t = tgt_np[valid].T\n",
    "x_p, y_p, z_p = pred_best[valid].T\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].scatter(x_t, y_t, s=10, label='target')\n",
    "axes[0].scatter(x_p, y_p, s=10, label='pred(aligned)')\n",
    "axes[0].set_title('x-y'); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(x_t, z_t, s=10)\n",
    "axes[1].scatter(x_p, z_p, s=10)\n",
    "axes[1].set_title('x-z')\n",
    "\n",
    "axes[2].scatter(y_t, z_t, s=10)\n",
    "axes[2].scatter(y_p, z_p, s=10)\n",
    "axes[2].set_title('y-z')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff28ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10) Quick visualization on holdout batch (k=0)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_b = model(tokens_b)  # (B,K,T,3)\n",
    "pred0 = preds_b[0,0].detach().cpu().numpy()\n",
    "tgt0  = tgt_b[0].detach().cpu().numpy()\n",
    "m0    = m_b[0].detach().cpu().numpy().astype(bool)\n",
    "\n",
    "pred0 = pred0[m0]\n",
    "tgt0  = tgt0[m0]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(tgt0[:,0], tgt0[:,1], tgt0[:,2], s=6)\n",
    "ax.scatter(pred0[:,0], pred0[:,1], pred0[:,2], s=6)\n",
    "ax.set_title(\"Holdout sample (target vs pred k=0) - centered\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
