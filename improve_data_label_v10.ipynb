{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6cdd8b",
   "metadata": {},
   "source": [
    "# improve_data_label_v9.ipynb\n",
    "\n",
    "- v9: TM-score validation metric (best-of-5), stage-aware checkpointing, dist_w ramp, AMP-safe Kabsch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06254dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0) Imports, Device, Config\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 30\n",
    "\n",
    "    # model\n",
    "    n_tokens: int = 5           # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    layers: int = 6\n",
    "    dropout: float = 0.1\n",
    "    num_preds: int = 5\n",
    "\n",
    "    # train\n",
    "    batch: int = 16\n",
    "    epochs: int = 60\n",
    "    warmup_epochs: int = 5\n",
    "    lr: float = 3e-5\n",
    "    wd: float = 1e-2\n",
    "    grad_clip: float = 1.0\n",
    "    num_workers: int = 0\n",
    "\n",
    "    # loss\n",
    "    dist_w: float = 0.10\n",
    "    softmin_temp: float = 0.30   # softmin temperature for K heads\n",
    "    pair_num_pairs: int = 2048  # sampled pair count for pairwise loss\n",
    "\n",
    "    # regularizers\n",
    "    var_w: float = 0.01         # encourage coordinate spread (anti-collapse)\n",
    "    local_w: float = 0.10       # adjacent bond length supervision\n",
    "\n",
    "    # candidate confidence (ranking)\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05        # aux loss weight for confidence head\n",
    "\n",
    "    # amp\n",
    "    use_amp: bool = True\n",
    "\n",
    "    # early stop\n",
    "    patience: int = 7\n",
    "    ckpt_path: str = \"best_structured_v10.pt\"\n",
    "    ckpt_stage1_path: str = \"best_stage1_v10.pt\"\n",
    "    dist_w_ramp_epochs: int = 5  # ramp dist_w over N epochs after warmup\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_all(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25102718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120aa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = np.asarray(self.tokens_list[idx], dtype=np.int64)\n",
    "        coords = np.asarray(self.coords_list[idx], dtype=np.float32)\n",
    "        mask   = np.asarray(self.mask_list[idx], dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        valid = mask.astype(bool)\n",
    "        if self.center_only and valid.sum() > 0:\n",
    "            center = coords[valid].mean(axis=0, keepdims=True)\n",
    "            coords = coords - center\n",
    "\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64);  tokens_p[:L] = tokens\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32); coords_p[:L] = coords\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32); mask_p[:L] = mask\n",
    "\n",
    "        return torch.tensor(tokens_p), torch.tensor(coords_p), torch.tensor(mask_p)\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccda3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_ds, batch_size=cfg.batch, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7) Losses + Composer  [v10]\n",
    "#   - per-head losses (B,K) so we can:\n",
    "#       (1) aggregate with softmin for stability\n",
    "#       (2) train a confidence head to rank candidates (aux loss)\n",
    "#   - Kabsch runs in FP32 even when AMP is enabled\n",
    "#   - pairwise distance loss uses sampled residue pairs (fast)\n",
    "#   - anti-collapse + local bond regularizers\n",
    "# ==========================================\n",
    "\n",
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    return torch.softmax(-losses / max(float(temperature), 1e-8), dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = softmin_weights(losses, temperature)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    out = []\n",
    "    denom = m.sum(dim=(1,2)).clamp_min(1.0)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        l1 = diff.sum(dim=(1,2)) / denom\n",
    "        out.append(l1)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        out.append(rmsd)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds, target, mask, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return losses_bk\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)  # (B,1,1)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]  # (B,T,3)\n",
    "        mean = (pk * m).sum(dim=1, keepdim=True) / denom\n",
    "        var = ((pk - mean) ** 2 * m).sum(dim=(1,2)) / denom.squeeze(1).squeeze(1).clamp_min(1.0)  # (B,)\n",
    "        out.append(var)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m_adj = (mask[:, 1:] * mask[:, :-1]).bool()  # (B,T-1)\n",
    "    tgt = (target[:, 1:] - target[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    out = []\n",
    "    denom = m_adj.sum(dim=1).clamp_min(1)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        l = (diff * m_adj).sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned - target).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n",
    "\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "            # confidence aux (optional)\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)  # (B,K)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: Kabsch + pairwise + regularizers\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)                      # (B,K)\n",
    "        pair_bk = pairwise_distance_losses_sampled(preds, target, mask,\n",
    "                                                   num_pairs=int(self.cfg.pair_num_pairs))  # (B,K)\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)                        # (B,K)\n",
    "        var_bk  = coord_variance_losses(preds, mask)                            # (B,K)\n",
    "\n",
    "        # ramp pairwise weight after warmup\n",
    "        t = max(0, int(epoch - self.cfg.warmup_epochs))\n",
    "        ramp = min(1.0, t / max(1, int(self.cfg.dist_w_ramp_epochs)))\n",
    "        dist_w_eff = float(self.cfg.dist_w) * ramp\n",
    "\n",
    "        total_bk = rmsd_bk + dist_w_eff * pair_bk + float(self.cfg.local_w) * loc_bk - float(self.cfg.var_w) * var_bk\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        # confidence aux: learn to rank candidates by total_bk\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)  # (B,K)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"KABSCH+PAIR+REG(+CONF)\" if aux != 0.0 else \"KABSCH+PAIR+REG\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8) Model (Backbone + Coord Head + Confidence Head)  [v10]\n",
    "#   - outputs: coords (B,K,T,3) and conf_logits (B,K)\n",
    "#   - conf head is trained to predict which candidate is best (auxiliary)\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos).unsqueeze(0)\n",
    "\n",
    "class RNABackbone(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg.n_tokens, cfg.d_model, padding_idx=0)\n",
    "        self.pos = PositionalEncodingLearned(cfg.d_model, max_len=2048)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=cfg.layers)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        pad_mask = (tokens == 0)\n",
    "        x = self.emb(tokens)\n",
    "        x = self.pos(x)\n",
    "        x = self.enc(x, src_key_padding_mask=pad_mask)\n",
    "        return x, pad_mask\n",
    "\n",
    "class CoordHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.proj = nn.Linear(cfg.d_model, 3 * cfg.num_preds)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (B,T,D) -> (B,K,T,3)\n",
    "        B, T, D = h.shape\n",
    "        out = self.proj(h).view(B, T, self.num_preds, 3).permute(0, 2, 1, 3).contiguous()\n",
    "        return out\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.num_preds = cfg.num_preds\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.d_model, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        # pool over residues (masked mean)\n",
    "        m = (~pad_mask).float().unsqueeze(-1)  # (B,T,1)\n",
    "        denom = m.sum(dim=1).clamp_min(1.0)\n",
    "        pooled = (h * m).sum(dim=1) / denom  # (B,D)\n",
    "        logits = self.mlp(pooled)  # (B,K)\n",
    "        return logits\n",
    "\n",
    "class RNAModel(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = RNABackbone(cfg)\n",
    "        self.coord_head = CoordHead(cfg)\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        h, pad_mask = self.backbone(tokens)\n",
    "        coords = self.coord_head(h)\n",
    "        conf_logits = self.conf_head(h, pad_mask) if self.cfg.use_confidence else None\n",
    "        return coords, conf_logits\n",
    "\n",
    "model = RNAModel(cfg).to(device)\n",
    "print(\"params:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9) Training (v10)\n",
    "#   - Stage1: optimize masked L1 (softmin) + optional confidence aux\n",
    "#   - Stage2: optimize Kabsch+pair+reg + confidence aux\n",
    "#   - Checkpoint/EarlyStop: use val TM-score (maximize) to match Kaggle best-of-5\n",
    "#   - Also logs RMSD for diagnostics\n",
    "# ==========================================\n",
    "class Trainer:\n",
    "    def __init__(self, cfg: CFG, model: nn.Module, loss_fn: LossComposer, device):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        self.use_amp = bool(cfg.use_amp and str(device).startswith(\"cuda\"))\n",
    "        self.scaler = torch.amp.GradScaler('cuda', enabled=self.use_amp)\n",
    "\n",
    "    def build_scheduler(self, steps_per_epoch: int):\n",
    "        total_steps = self.cfg.epochs * steps_per_epoch\n",
    "        warmup_steps = self.cfg.warmup_epochs * steps_per_epoch\n",
    "\n",
    "        def lr_lambda(step):\n",
    "            if step < warmup_steps:\n",
    "                return (step + 1) / max(1, warmup_steps)\n",
    "            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.opt, lr_lambda)\n",
    "\n",
    "    def run_epoch(self, loader, epoch: int, train: bool):\n",
    "        self.model.train(train)\n",
    "\n",
    "        total_loss, steps = 0.0, 0\n",
    "        total_rmsd, rmsd_steps = 0.0, 0\n",
    "        total_tm, tm_steps = 0.0, 0\n",
    "        stage_name = None\n",
    "\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{self.cfg.epochs} [{'train' if train else 'eval'}]\")\n",
    "\n",
    "        for tokens, target, mask in pbar:\n",
    "            tokens = tokens.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            mask = mask.to(self.device)\n",
    "\n",
    "            fb = filter_batch(tokens, target, mask, self.cfg.min_valid)\n",
    "            if fb is None:\n",
    "                continue\n",
    "            tokens, target, mask = fb\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                with torch.amp.autocast(device_type='cuda', enabled=self.use_amp):\n",
    "                    coords, conf_logits = self.model(tokens)  # coords (B,K,T,3), conf (B,K) or None\n",
    "                    loss, stage_name = self.loss_fn(coords, target, mask, epoch, conf_logits=conf_logits)\n",
    "\n",
    "                if not train:\n",
    "                    rmsd = kabsch_rmsd_metric_min(coords, target, mask)\n",
    "                    tm = tm_score_metric_maxK(coords, target, mask)\n",
    "                    total_rmsd += float(rmsd.item()); rmsd_steps += 1\n",
    "                    total_tm += float(tm.item()); tm_steps += 1\n",
    "\n",
    "                if train:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                    self.scaler.step(self.opt)\n",
    "                    self.scaler.update()\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "                    self.global_step += 1\n",
    "\n",
    "            total_loss += float(loss.item()); steps += 1\n",
    "            lr = self.opt.param_groups[0][\"lr\"]\n",
    "            post = {\"loss\": total_loss / max(1, steps), \"stage\": stage_name, \"lr\": lr}\n",
    "            if (not train) and rmsd_steps > 0:\n",
    "                post[\"rmsd\"] = total_rmsd / rmsd_steps\n",
    "            if (not train) and tm_steps > 0:\n",
    "                post[\"tm\"] = total_tm / tm_steps\n",
    "            pbar.set_postfix(post)\n",
    "\n",
    "        avg_loss = total_loss / max(1, steps)\n",
    "        avg_rmsd = total_rmsd / max(1, rmsd_steps) if rmsd_steps > 0 else float(\"nan\")\n",
    "        avg_tm = total_tm / max(1, tm_steps) if tm_steps > 0 else float(\"nan\")\n",
    "        return avg_loss, stage_name, avg_rmsd, avg_tm\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        self.build_scheduler(len(train_loader))\n",
    "\n",
    "        best_tm = -1e9\n",
    "        stale = 0\n",
    "        stage2_started = False\n",
    "\n",
    "        best_stage1 = float(\"inf\")\n",
    "\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            tr_loss, stage_tr, _, _ = self.run_epoch(train_loader, epoch, train=True)\n",
    "            va_loss, stage_va, va_rmsd, va_tm = self.run_epoch(val_loader, epoch, train=False)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1:02d}] stage={stage_va} train_loss={tr_loss:.6f} val_loss={va_loss:.6f} val_rmsd={va_rmsd:.4f} val_tm={va_tm:.4f}\")\n",
    "\n",
    "            # Stage1 optional checkpoint (by val loss)\n",
    "            if epoch < self.cfg.warmup_epochs:\n",
    "                if va_loss < best_stage1 - 1e-4:\n",
    "                    best_stage1 = va_loss\n",
    "                    torch.save(self.model.state_dict(), self.cfg.ckpt_stage1_path)\n",
    "                    print(f\"ðŸ’¾ stage1 best loss updated: {best_stage1:.6f}\")\n",
    "                continue\n",
    "\n",
    "            # Stage2 starts: reset patience tracking once\n",
    "            if (not stage2_started):\n",
    "                stage2_started = True\n",
    "                best_tm = -1e9\n",
    "                stale = 0\n",
    "                print(\"ðŸ” Stage2 started: reset best_tm/stale\")\n",
    "\n",
    "            # Stage2: maximize TM-score (competition aligned)\n",
    "            if va_tm > best_tm + 1e-4:\n",
    "                best_tm = va_tm\n",
    "                stale = 0\n",
    "                torch.save(self.model.state_dict(), self.cfg.ckpt_path)\n",
    "                print(f\"âœ… best TM updated: {best_tm:.4f}\")\n",
    "            else:\n",
    "                stale += 1\n",
    "                print(f\"â¸ no TM improvement: {stale}/{self.cfg.patience}\")\n",
    "                if stale >= self.cfg.patience:\n",
    "                    print(\"ðŸ›‘ early stopping (Stage2, TM-based).\")\n",
    "                    break\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.cfg.ckpt_path, map_location=self.device))\n",
    "        print(\"Best model loaded:\", self.cfg.ckpt_path, \"best_tm=\", best_tm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572c13ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.33it/s, loss=52.5, stage=MASKED_L1(+CONF), lr=2.4e-5] \n",
      "Epoch 4/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 22.50it/s, loss=54.3, stage=MASKED_L1(+CONF), lr=2.4e-5, rmsd=22.5, tm=0.0338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] stage=MASKED_L1(+CONF) train_loss=52.488187 val_loss=54.279329 val_rmsd=22.5351 val_tm=0.0338\n",
      "ðŸ’¾ stage1 best loss updated: 54.279329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:08<00:00, 31.93it/s, loss=52.5, stage=MASKED_L1(+CONF), lr=3e-5]   \n",
      "Epoch 5/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 23.22it/s, loss=54.3, stage=MASKED_L1(+CONF), lr=3e-5, rmsd=22.5, tm=0.0339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] stage=MASKED_L1(+CONF) train_loss=52.500405 val_loss=54.264909 val_rmsd=22.5177 val_tm=0.0339\n",
      "ðŸ’¾ stage1 best loss updated: 54.264909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.04it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=3e-5]\n",
      "Epoch 6/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.77it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=3e-5, rmsd=22.5, tm=0.0348] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4587 val_tm=0.0348\n",
      "ðŸ” Stage2 started: reset best_tm/stale\n",
      "âœ… best TM updated: 0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.06it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.99e-5] \n",
      "Epoch 7/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.47it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.99e-5, rmsd=22.5, tm=0.0355] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4610 val_tm=0.0355\n",
      "âœ… best TM updated: 0.0355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00,  9.99it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.98e-5]\n",
      "Epoch 8/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.18it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.98e-5, rmsd=22.5, tm=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4675 val_tm=0.0358\n",
      "âœ… best TM updated: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.28it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.96e-5]\n",
      "Epoch 9/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.85it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.96e-5, rmsd=22.5, tm=0.0356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4696 val_tm=0.0356\n",
      "â¸ no TM improvement: 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.17it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.94e-5]\n",
      "Epoch 10/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.10it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.94e-5, rmsd=22.5, tm=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4772 val_tm=0.0358\n",
      "â¸ no TM improvement: 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.24it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.91e-5]\n",
      "Epoch 11/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.67it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.91e-5, rmsd=22.5, tm=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4763 val_tm=0.0358\n",
      "â¸ no TM improvement: 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.07it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.88e-5]\n",
      "Epoch 12/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.09it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.88e-5, rmsd=22.5, tm=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.4742 val_tm=0.0358\n",
      "â¸ no TM improvement: 4/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.21it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.85e-5]\n",
      "Epoch 13/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.87it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.85e-5, rmsd=22, tm=0.0378]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=22.0494 val_tm=0.0378\n",
      "âœ… best TM updated: 0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.24it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.81e-5]\n",
      "Epoch 14/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.95it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.81e-5, rmsd=20, tm=0.0455]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=20.0066 val_tm=0.0455\n",
      "âœ… best TM updated: 0.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.06it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.76e-5]\n",
      "Epoch 15/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.09it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.76e-5, rmsd=19.4, tm=0.0485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=19.4205 val_tm=0.0485\n",
      "âœ… best TM updated: 0.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.28it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.71e-5]\n",
      "Epoch 16/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.12it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.71e-5, rmsd=18.9, tm=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=18.9183 val_tm=0.0502\n",
      "âœ… best TM updated: 0.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.22it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.66e-5]\n",
      "Epoch 17/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.92it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.66e-5, rmsd=18.7, tm=0.0508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=18.6700 val_tm=0.0508\n",
      "âœ… best TM updated: 0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.23it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.61e-5]\n",
      "Epoch 18/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.99it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.61e-5, rmsd=18.6, tm=0.0504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=18.6301 val_tm=0.0504\n",
      "â¸ no TM improvement: 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.29it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.55e-5]\n",
      "Epoch 19/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.05it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.55e-5, rmsd=18.6, tm=0.0507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=18.6338 val_tm=0.0507\n",
      "â¸ no TM improvement: 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.22it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.48e-5]\n",
      "Epoch 20/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.03it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.48e-5, rmsd=18.7, tm=0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=18.7471 val_tm=0.0497\n",
      "â¸ no TM improvement: 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:25<00:00, 10.33it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.42e-5]\n",
      "Epoch 21/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 16.20it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.42e-5, rmsd=19.6, tm=0.0471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=19.5998 val_tm=0.0471\n",
      "â¸ no TM improvement: 4/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.12it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.35e-5]\n",
      "Epoch 22/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.55it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.35e-5, rmsd=19.7, tm=0.0466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=19.6813 val_tm=0.0466\n",
      "â¸ no TM improvement: 5/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.04it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.27e-5]\n",
      "Epoch 23/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.70it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.27e-5, rmsd=19.7, tm=0.0466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=19.6819 val_tm=0.0466\n",
      "â¸ no TM improvement: 6/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/60 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:26<00:00, 10.14it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.2e-5] \n",
      "Epoch 24/60 [eval]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:01<00:00, 15.87it/s, loss=nan, stage=KABSCH+PAIR+REG(+CONF), lr=2.2e-5, rmsd=19.7, tm=0.0466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] stage=KABSCH+PAIR+REG(+CONF) train_loss=nan val_loss=nan val_rmsd=19.6816 val_tm=0.0466\n",
      "â¸ no TM improvement: 7/7\n",
      "ðŸ›‘ early stopping (Stage2, TM-based).\n",
      "Best model loaded: best_structured_v10.pt best_tm= 0.0507513340562582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 9.5) Fit (stage-aware: Stage2 RMSD-based checkpoint)\n",
    "# ==========================================\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "trainer = Trainer(cfg, model, loss_fn, device)\n",
    "trainer.fit(train_loader, hold_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a8de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkdwl\\anaconda3\\envs\\rna_env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM per k: [0.0315, 0.0237, 0.0325, 0.0193, 0.0314] best_k: 2\n",
      "conf logits: [-0.519  0.419 -0.467  0.146 -0.481]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 10) Quick sanity check on holdout batch  [v10]\n",
    "#   - pick best head by TM-score (competition aligned) for sample 0\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    coords_b, conf_b = model(tokens_b)  # coords (B,K,T,3)\n",
    "\n",
    "# choose sample 0\n",
    "preds0 = coords_b[0]   # (K,T,3)\n",
    "tgt0 = tgt_b[0]        # (T,3)\n",
    "mask0 = m_b[0]         # (T,)\n",
    "\n",
    "# compute TM-score per head (after Kabsch), pick best\n",
    "tms = []\n",
    "aligned = []\n",
    "for k in range(preds0.size(0)):\n",
    "    pk = preds0[k:k+1]  # (1,T,3)\n",
    "    tk = tgt0.unsqueeze(0)\n",
    "    mk = mask0.unsqueeze(0)\n",
    "    pk_al = kabsch_align(pk, tk, mk)[0].detach().cpu()\n",
    "    aligned.append(pk_al)\n",
    "\n",
    "    # TM-score for this sample/head\n",
    "    tm = tm_score_single(pk, tk, mk)[0].item()\n",
    "    tms.append(tm)\n",
    "\n",
    "best_k = int(np.argmax(tms))\n",
    "pred_best = aligned[best_k].numpy()\n",
    "tgt_np = tgt0.detach().cpu().numpy()\n",
    "mask_np = mask0.detach().cpu().numpy()\n",
    "\n",
    "print(\"TM per k:\", [round(x,4) for x in tms], \"best_k:\", best_k)\n",
    "if conf_b is not None:\n",
    "    print(\"conf logits:\", conf_b[0].detach().cpu().numpy().round(3))\n",
    "\n",
    "valid = mask_np.astype(bool)\n",
    "x_t, y_t, z_t = tgt_np[valid].T\n",
    "x_p, y_p, z_p = pred_best[valid].T\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].scatter(x_t, y_t, s=10, label='target')\n",
    "axes[0].scatter(x_p, y_p, s=10, label='pred(aligned)')\n",
    "axes[0].set_title('x-y'); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(x_t, z_t, s=10)\n",
    "axes[1].scatter(x_p, z_p, s=10)\n",
    "axes[1].set_title('x-z')\n",
    "\n",
    "axes[2].scatter(y_t, z_t, s=10)\n",
    "axes[2].scatter(y_p, z_p, s=10)\n",
    "axes[2].set_title('y-z')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff28ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     13\u001b[0m     preds_b \u001b[38;5;241m=\u001b[39m model(tokens_b)  \u001b[38;5;66;03m# (B,K,T,3)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m pred0 \u001b[38;5;241m=\u001b[39m \u001b[43mpreds_b\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     15\u001b[0m tgt0  \u001b[38;5;241m=\u001b[39m tgt_b[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     16\u001b[0m m0    \u001b[38;5;241m=\u001b[39m m_b[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 10) Quick visualization on holdout batch (k=0)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "tokens_b, tgt_b, m_b = next(iter(hold_loader))\n",
    "tokens_b = tokens_b.to(device)\n",
    "tgt_b = tgt_b.to(device)\n",
    "m_b = m_b.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_b = model(tokens_b)  # (B,K,T,3)\n",
    "pred0 = preds_b[0,0].detach().cpu().numpy()\n",
    "tgt0  = tgt_b[0].detach().cpu().numpy()\n",
    "m0    = m_b[0].detach().cpu().numpy().astype(bool)\n",
    "\n",
    "pred0 = pred0[m0]\n",
    "tgt0  = tgt0[m0]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(tgt0[:,0], tgt0[:,1], tgt0[:,2], s=6)\n",
    "ax.scatter(pred0[:,0], pred0[:,1], pred0[:,2], s=6)\n",
    "ax.set_title(\"Holdout sample (target vs pred k=0) - centered\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
