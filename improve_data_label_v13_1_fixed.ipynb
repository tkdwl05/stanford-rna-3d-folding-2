{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5ce781",
   "metadata": {},
   "source": [
    "# improve_data_label_v13\n",
    "\n",
    "v13: EGNN backbone + base-pair 확률(pair feature) 주입 + weak distance auxiliary (0.05)\n",
    "\n",
    "- 목표: topology(장거리 상호작용) 개선\n",
    "- 입력: train_sequences.csv / validation_sequences.csv, train_labels.csv / validation_labels.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 0) Imports, Device, Config  [v13]\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "\n",
    "    # data\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 10\n",
    "    batch: int = 8\n",
    "    num_workers: int = 2\n",
    "\n",
    "    # model\n",
    "    vocab: int = 5  # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 192\n",
    "    d_edge: int = 256\n",
    "    n_layers: int = 8\n",
    "    k_nn: int = 12  # sequence neighborhood edges\n",
    "    num_preds: int = 4  # K heads\n",
    "\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # base-pair feature\n",
    "    bp_tau: float = 40.0         # distance decay for |i-j|\n",
    "    bp_min_sep: int = 4          # do not pair too-close residues\n",
    "    pair_alpha: float = 2.0      # message/coord weight boost: (1 + pair_alpha * p_ij)\n",
    "\n",
    "    # optimization\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 0.02\n",
    "    epochs: int = 20\n",
    "    warmup_epochs: int = 2\n",
    "\n",
    "    # softmin aggregation\n",
    "    softmin_temp: float = 1.0\n",
    "\n",
    "    # losses\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05\n",
    "\n",
    "    dist_w: float = 0.05         # weak distance auxiliary (as requested)\n",
    "    pair_num_pairs: int = 2048    # sampled pairs for distance/repulsion\n",
    "    local_w: float = 0.2\n",
    "    var_w: float = 0.02\n",
    "    repulse_w: float = 0.02\n",
    "    diversity_w: float = 0.01\n",
    "    repulse_margin: float = 2.5\n",
    "    diversity_margin: float = 2.0\n",
    "\n",
    "cfg = CFG()\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73414564",
   "metadata": {},
   "source": [
    "## 1) Dataset / Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2e4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_5796\\799196925.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n",
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n",
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b18db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (5739, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n",
      "train batches: 646 hold batches: 72\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = np.asarray(self.tokens_list[idx], dtype=np.int64)\n",
    "        coords = np.asarray(self.coords_list[idx], dtype=np.float32)\n",
    "        mask   = np.asarray(self.mask_list[idx], dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        valid = mask.astype(bool)\n",
    "        if self.center_only and valid.sum() > 0:\n",
    "            center = coords[valid].mean(axis=0, keepdims=True)\n",
    "            coords = coords - center\n",
    "\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64);  tokens_p[:L] = tokens\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32); coords_p[:L] = coords\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32); mask_p[:L] = mask\n",
    "\n",
    "        return torch.tensor(tokens_p), torch.tensor(coords_p), torch.tensor(mask_p)\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "hold_loader  = DataLoader(hold_ds, batch_size=cfg.batch, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4df894",
   "metadata": {},
   "source": [
    "## 2) Base-pair feature 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02c55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2) Base-pair probability encoder (sequence-only heuristic)\n",
    "#   - 입력 tokens(B,T) -> p(B,T,T) in [0,1]\n",
    "#   - canonical: A-U, C-G, G-U wobble\n",
    "#   - 거리 prior: exp(-|i-j|/tau), and |i-j|>=bp_min_sep\n",
    "# ==========================================\n",
    "class BasePairEncoder(nn.Module):\n",
    "    def __init__(self, tau: float = 40.0, min_sep: int = 4):\n",
    "        super().__init__()\n",
    "        self.tau = float(tau)\n",
    "        self.min_sep = int(min_sep)\n",
    "\n",
    "        # 0 PAD, 1 A,2 C,3 G,4 U\n",
    "        # canonical probs\n",
    "        P = torch.zeros((5,5), dtype=torch.float32)\n",
    "        P[1,4] = 1.0; P[4,1] = 1.0  # A-U\n",
    "        P[2,3] = 1.0; P[3,2] = 1.0  # C-G\n",
    "        P[3,4] = 0.6; P[4,3] = 0.6  # G-U wobble (weaker)\n",
    "        self.register_buffer(\"pair_table\", P, persistent=False)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, pad_mask: torch.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        tokens: (B,T) int64\n",
    "        pad_mask: (B,T) True for valid nodes (optional)\n",
    "        returns p: (B,T,T) float32\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        t_i = tokens[:, :, None]  # (B,T,1)\n",
    "        t_j = tokens[:, None, :]  # (B,1,T)\n",
    "\n",
    "        base = self.pair_table[t_i, t_j]  # (B,T,T)\n",
    "\n",
    "        # 거리 prior\n",
    "        idx = torch.arange(T, device=tokens.device)\n",
    "        dist = (idx[None, :, None] - idx[None, None, :]).abs().float()  # (1,T,T)\n",
    "        sep_ok = (dist >= float(self.min_sep)).float()\n",
    "        prior = torch.exp(-dist / max(self.tau, 1e-6)) * sep_ok\n",
    "\n",
    "        p = base * prior  # (B,T,T)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            m = pad_mask.float()\n",
    "            p = p * (m[:, :, None] * m[:, None, :])\n",
    "\n",
    "        # zero diagonal\n",
    "        p = p * (1.0 - torch.eye(T, device=p.device, dtype=p.dtype)[None, :, :])\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4f945",
   "metadata": {},
   "source": [
    "## 3) Loss helpers (Kabsch + sampled distance + local/var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3eab1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    return torch.softmax(-losses / max(float(temperature), 1e-8), dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = softmin_weights(losses, temperature)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    out = []\n",
    "    denom = m.sum(dim=(1,2)).clamp_min(1.0)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        l1 = diff.sum(dim=(1,2)) / denom\n",
    "        out.append(l1)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        out.append(rmsd)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds, target, mask, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return losses_bk\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)  # (B,1,1)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]  # (B,T,3)\n",
    "        mean = (pk * m).sum(dim=1, keepdim=True) / denom\n",
    "        var = ((pk - mean) ** 2 * m).sum(dim=(1,2)) / denom.squeeze(1).squeeze(1).clamp_min(1.0)  # (B,)\n",
    "        out.append(var)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m_adj = (mask[:, 1:] * mask[:, :-1]).bool()  # (B,T-1)\n",
    "    tgt = (target[:, 1:] - target[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    out = []\n",
    "    denom = m_adj.sum(dim=1).clamp_min(1)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        l = (diff * m_adj).sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned - target).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2774e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# v11 추가: non-neighbor repulsion + head diversity\n",
    "# ==========================================\n",
    "def repulsion_losses_sampled(preds: torch.Tensor, mask: torch.Tensor,\n",
    "                             num_pairs: int = 2048, margin: float = 2.5) -> torch.Tensor:\n",
    "    \"\"\"겹침 방지용 hinge loss. 인접(i,i+1)은 제외하고 랜덤 pair에 대해\n",
    "    d < margin 이면 (margin-d)^2 를 부과.\n",
    "    returns (B,K)\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses = preds.new_zeros((B, K))\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 3:\n",
    "            continue\n",
    "\n",
    "        # sample pairs\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        # exclude neighbors\n",
    "        nn_mask = (torch.abs(i - j) > 1)\n",
    "        if nn_mask.sum() < 8:\n",
    "            continue\n",
    "        i = i[nn_mask]\n",
    "        j = j[nn_mask]\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            hinge = (margin - d).clamp_min(0.0)\n",
    "            losses[b, k] = (hinge * hinge).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "def head_diversity_losses(preds: torch.Tensor, mask: torch.Tensor, margin: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"헤드 간 유사하면 패널티. (B,K)로 반환해서 기존 softmin 프레임에 맞춘다.\n",
    "    각 헤드의 masked centered coords를 만들고, 헤드쌍 RMSD가 margin보다 작으면 hinge.\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "    # centered\n",
    "    centered = preds - (preds * m.unsqueeze(1)).sum(dim=2, keepdim=True) / denom.unsqueeze(1)\n",
    "\n",
    "    # pairwise head RMSD (no rotation; diversity 목적이라 단순 RMSD)\n",
    "    out = preds.new_zeros((B, K))\n",
    "    if K < 2:\n",
    "        return out\n",
    "\n",
    "    for a in range(K):\n",
    "        pen = 0.0\n",
    "        cnt = 0\n",
    "        for b in range(K):\n",
    "            if a == b: \n",
    "                continue\n",
    "            diff = (centered[:, a] - centered[:, b])**2  # (B,T,3)\n",
    "            rmsd = torch.sqrt((diff * m).sum(dim=(1,2)) / (mask.sum(dim=1).clamp_min(1.0)*3.0) + 1e-8)  # (B,)\n",
    "            hinge = (margin - rmsd).clamp_min(0.0)\n",
    "            pen = pen + hinge*hinge\n",
    "            cnt += 1\n",
    "        out[:, a] = pen / max(cnt, 1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58a6a7",
   "metadata": {},
   "source": [
    "## 4) Model (Pair-aware EGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd83612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4) Model (EGNN backbone + Pair-aware message passing + K coord heads + ConfidenceHead)  [v13]\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos)[None, :, :]\n",
    "\n",
    "def build_seq_edges(T: int, k: int, device):\n",
    "    r = max(1, k // 2)\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(T):\n",
    "        for j in range(max(0, i - r), min(T, i + r + 1)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            src.append(i); dst.append(j)\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long, device=device)  # (2,E)\n",
    "    return edge_index\n",
    "\n",
    "class EGNNPairAwareLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EGNN + pair feature p_ij (base-pair 확률)\n",
    "    message: phi(h_i, h_j, d_ij^2, p_ij)\n",
    "    + long-range boost: (1 + pair_alpha * p_ij) 를 coord/node 업데이트에 곱한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_node: int, d_edge: int, dropout: float, pair_alpha: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.pair_alpha = float(pair_alpha)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node*2 + 1 + 1, d_edge),  # +p_ij\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_edge, 1),\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node + d_edge, d_node),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_node, d_node),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(d_node)\n",
    "\n",
    "    def forward(self, h, x, edge_index, node_mask, pair_p):\n",
    "        \"\"\"\n",
    "        h: (B,T,D), x: (B,T,3)\n",
    "        edge_index: (2,E)\n",
    "        node_mask: (B,T) bool\n",
    "        pair_p: (B,T,T) float in [0,1]\n",
    "        \"\"\"\n",
    "        B, T, D = h.shape\n",
    "        src, dst = edge_index[0], edge_index[1]  # (E,)\n",
    "\n",
    "        hi = h[:, src, :]\n",
    "        hj = h[:, dst, :]\n",
    "        xi = x[:, src, :]\n",
    "        xj = x[:, dst, :]\n",
    "\n",
    "        rij = xi - xj\n",
    "        dij2 = (rij * rij).sum(dim=-1, keepdim=True)  # (B,E,1)\n",
    "        pij = pair_p[:, src, dst].unsqueeze(-1)        # (B,E,1)\n",
    "\n",
    "        m_ij = self.edge_mlp(torch.cat([hi, hj, dij2, pij], dim=-1))  # (B,E,d_edge)\n",
    "        boost = (1.0 + self.pair_alpha * pij).clamp(0.0, 10.0)\n",
    "\n",
    "        w = self.coord_mlp(m_ij) * boost  # (B,E,1)\n",
    "        dx = rij * w  # (B,E,3)\n",
    "\n",
    "        agg_dx = x.new_zeros((B, T, 3))\n",
    "        agg_m  = h.new_zeros((B, T, m_ij.size(-1)))\n",
    "\n",
    "        agg_dx.index_add_(1, src, dx.to(agg_dx.dtype))\n",
    "        agg_m.index_add_(1, src, (m_ij * boost).to(agg_m.dtype))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            agg_dx = agg_dx.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            agg_m  = agg_m.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "\n",
    "        x = x + agg_dx\n",
    "        h = self.ln(h + self.node_mlp(torch.cat([h, agg_m], dim=-1)))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            h = h.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            x = x.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "        return h, x\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(cfg.d_model//2, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        if pad_mask is None:\n",
    "            pooled = h.mean(dim=1)\n",
    "        else:\n",
    "            m = pad_mask.float().unsqueeze(-1)\n",
    "            denom = m.sum(dim=1).clamp_min(1.0)\n",
    "            pooled = (h * m).sum(dim=1) / denom\n",
    "        return self.mlp(pooled)  # (B,K)\n",
    "\n",
    "class EGNNv13(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(cfg.vocab, cfg.d_model, padding_idx=0)\n",
    "        self.posenc = PositionalEncodingLearned(cfg.d_model, max_len=cfg.max_len)\n",
    "        self.bp_enc = BasePairEncoder(tau=cfg.bp_tau, min_sep=cfg.bp_min_sep)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EGNNPairAwareLayer(cfg.d_model, cfg.d_edge, cfg.dropout, pair_alpha=cfg.pair_alpha)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        # K coord heads\n",
    "        self.coord_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(cfg.d_model, cfg.d_model),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(cfg.d_model, 3),\n",
    "            ) for _ in range(cfg.num_preds)\n",
    "        ])\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "    def forward(self, tokens, pad_mask):\n",
    "        \"\"\"\n",
    "        tokens: (B,T)\n",
    "        pad_mask: (B,T) bool\n",
    "        returns:\n",
    "          preds: (B,K,T,3)\n",
    "          conf_logits: (B,K)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        h = self.embed(tokens)\n",
    "        h = self.posenc(h)\n",
    "        if pad_mask is not None:\n",
    "            h = h.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        # init coords at 0\n",
    "        x = torch.zeros((B, T, 3), device=tokens.device, dtype=h.dtype)\n",
    "\n",
    "        edge_index = build_seq_edges(T, self.cfg.k_nn, tokens.device)\n",
    "\n",
    "        # pair probabilities (sequence-only heuristic)\n",
    "        pair_p = self.bp_enc(tokens, pad_mask)  # (B,T,T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h, x = layer(h, x, edge_index, pad_mask, pair_p)\n",
    "\n",
    "        preds = []\n",
    "        for head in self.coord_heads:\n",
    "            preds.append(head(h).unsqueeze(1))\n",
    "        preds = torch.cat(preds, dim=1)  # (B,K,T,3)\n",
    "\n",
    "        conf_logits = self.conf_head(h, pad_mask)  # (B,K)\n",
    "        return preds, conf_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9630",
   "metadata": {},
   "source": [
    "## 5) LossComposer (weak distance auxiliary 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d31a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3) LossComposer  [v12_c]\n",
    "#   - warmup: masked L1 (+optional confidence)\n",
    "#   - main: Kabsch RMSD + pairwise + local + variance + repulsion + diversity (+confidence)\n",
    "# ==========================================\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: structured losses (all are (B,K))\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)\n",
    "        pair_bk = pairwise_distance_losses_sampled(preds, target, mask, num_pairs=int(self.cfg.pair_num_pairs))\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)\n",
    "        var_bk  = coord_variance_losses(preds, mask)\n",
    "\n",
    "        rep_bk  = repulsion_losses_sampled(preds, mask, num_pairs=int(self.cfg.pair_num_pairs),\n",
    "                                           margin=float(self.cfg.repulse_margin))\n",
    "        div_bk  = head_diversity_losses(preds, mask, margin=float(self.cfg.diversity_margin))\n",
    "\n",
    "        # ramp pairwise/repulsion after warmup\n",
    "        dist_w_eff = float(self.cfg.dist_w)\n",
    "        rep_w_eff  = float(self.cfg.repulse_w)\n",
    "\n",
    "        total_bk = (\n",
    "            rmsd_bk\n",
    "            + dist_w_eff * pair_bk\n",
    "            + float(self.cfg.local_w) * loc_bk\n",
    "            - float(self.cfg.var_w) * var_bk\n",
    "            + rep_w_eff * rep_bk\n",
    "            + float(self.cfg.diversity_w) * div_bk\n",
    "        )\n",
    "\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"STRUCTURED(+CONF)\" if aux != 0.0 else \"STRUCTURED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3584cf",
   "metadata": {},
   "source": [
    "## 6) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [train]:   0%|          | 0/646 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6) Train loop  [v13]\n",
    "# ==========================================\n",
    "model = EGNNv13(cfg).to(device)\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "def run_epoch(loader, epoch: int, train: bool):\n",
    "    model.train(train)\n",
    "    total_loss, n = 0.0, 0\n",
    "    total_rmsd, n_r = 0.0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs} [{'train' if train else 'eval'}]\")\n",
    "    for tokens, target, mask in pbar:\n",
    "        tokens = tokens.to(device)\n",
    "        target = target.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        fb = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "        if fb is None:\n",
    "            continue\n",
    "        tokens, target, mask = fb\n",
    "        pad_mask = (tokens != 0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n",
    "                preds, conf_logits = model(tokens, pad_mask)\n",
    "                loss, stage = loss_fn(preds, target, mask, epoch, conf_logits=conf_logits)\n",
    "\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()); n += 1\n",
    "\n",
    "        if not train:\n",
    "            rmsd = kabsch_rmsd_metric_min(preds, target, mask)\n",
    "            total_rmsd += float(rmsd.item()); n_r += 1\n",
    "\n",
    "        pbar.set_postfix(loss=total_loss/max(n,1), stage=stage,\n",
    "                         rmsd=(total_rmsd/max(n_r,1) if n_r>0 else None))\n",
    "    return total_loss/max(n,1), (total_rmsd/max(n_r,1) if n_r>0 else None)\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    tr_loss, _ = run_epoch(train_loader, epoch, train=True)\n",
    "    ev_loss, ev_rmsd = run_epoch(hold_loader, epoch, train=False)\n",
    "    print(f\"[Epoch {epoch+1}] train_loss={tr_loss:.4f}  eval_loss={ev_loss:.4f}  eval_rmsd={ev_rmsd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e3366",
   "metadata": {},
   "source": [
    "## 7) Sanity check 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31348ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7) Quick sanity check plot (xy/xz/yz)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens, target, mask = next(iter(hold_loader))\n",
    "tokens = tokens.to(device); target = target.to(device); mask = mask.to(device)\n",
    "tokens, target, mask = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "pad_mask = (tokens != 0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds, conf_logits = model(tokens, pad_mask)  # (B,K,T,3)\n",
    "    # pick best head by TM-score for sample 0\n",
    "    tm_per_head = []\n",
    "    for k in range(cfg.num_preds):\n",
    "        tm = tm_score_single(preds[0:1, k], target[0:1], mask[0:1])\n",
    "        tm_per_head.append(float(tm.item()))\n",
    "    best_k = int(np.argmax(tm_per_head))\n",
    "    pred0 = preds[0, best_k].detach()\n",
    "    tgt0  = target[0].detach()\n",
    "    m0    = mask[0].detach().bool()\n",
    "\n",
    "    pred0a = kabsch_align(pred0.unsqueeze(0), tgt0.unsqueeze(0), mask[0:1]).squeeze(0)\n",
    "\n",
    "def scat(a, b, title, ax):\n",
    "    ax.scatter(a[:,0].cpu(), a[:,1].cpu(), s=10, label=\"target\")\n",
    "    ax.scatter(b[:,0].cpu(), b[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "A = tgt0[m0]\n",
    "B = pred0a[m0]\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "axes[0].scatter(A[:,0].cpu(), A[:,1].cpu(), s=10, label=\"target\")\n",
    "axes[0].scatter(B[:,0].cpu(), B[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[0].set_title(\"x-y\"); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(A[:,0].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[1].scatter(B[:,0].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[1].set_title(\"x-z\"); axes[1].legend()\n",
    "\n",
    "axes[2].scatter(A[:,1].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[2].scatter(B[:,1].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[2].set_title(\"y-z\"); axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"best head:\", best_k, \"tm per head:\", tm_per_head)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}