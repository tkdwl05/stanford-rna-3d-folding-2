{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffc979c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 0. Imports & Device\n",
        "# ==========================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 재현성(선택)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. Load Data (train + validation)\n",
        "# ==========================================\n",
        "train_seq = pd.read_csv('train_sequences.csv')\n",
        "val_seq   = pd.read_csv('validation_sequences.csv')\n",
        "\n",
        "# low_memory=False로 mixed dtype 경고 완화\n",
        "train_labels = pd.read_csv('train_labels.csv', low_memory=False)\n",
        "val_labels   = pd.read_csv('validation_labels.csv', low_memory=False)\n",
        "\n",
        "print(f\"Train seq shape: {train_seq.shape}\")\n",
        "print(f\"Val   seq shape: {val_seq.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Val   labels shape: {val_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
        "# ==========================================\n",
        "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
        "\n",
        "def tokenize(seq: str):\n",
        "    return [mapping.get(b, 0) for b in str(seq)]\n",
        "\n",
        "for df in (train_seq, val_seq):\n",
        "    df['target_id'] = df['target_id'].astype(str).str.strip()\n",
        "    df['tokenized'] = df['sequence'].apply(tokenize)\n",
        "\n",
        "print(\"Tokenized train/val sequences.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. Label preprocessing (train + validation)\n",
        "#    - train_labels: (x_1,y_1,z_1) 단일 구조\n",
        "#    - val_labels: (x_1..z_40) 다중 슬롯 + 결측이 -1e18 같은 센티넬로 들어있음\n",
        "#      => isfinite만으로는 결측을 못 걸러서 abs<threshold 조건을 추가\n",
        "# ==========================================\n",
        "XYZ = ['x_1','y_1','z_1']\n",
        "ABS_THRESH = 1e17   # -1e18 같은 센티넬 결측 제거용\n",
        "MIN_VALID_POINTS = 30  # 너무 결측이 많은 target 제거(안정성)\n",
        "\n",
        "def _clean_base(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
        "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
        "    df = df.dropna(subset=['resid']).copy()\n",
        "    df['resid'] = df['resid'].astype(int)\n",
        "    df = df.sort_values(['target_id','resid'])\n",
        "    return df\n",
        "\n",
        "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = _clean_base(train_labels)\n",
        "\n",
        "    for c in XYZ:\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
        "\n",
        "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < ABS_THRESH).all(axis=1)\n",
        "    df['coord_ok'] = ok.astype(np.float32)\n",
        "\n",
        "    # 결측은 0으로 채우고 mask로 제외\n",
        "    df.loc[~ok, XYZ] = 0.0\n",
        "    df[XYZ] = df[XYZ].astype(np.float32)\n",
        "\n",
        "    # target 단위 유효 포인트 수 기준 필터\n",
        "    valid_counts = df.groupby('target_id')['coord_ok'].sum()\n",
        "    good_ids = valid_counts[valid_counts >= MIN_VALID_POINTS].index\n",
        "    df = df[df['target_id'].isin(good_ids)].copy()\n",
        "\n",
        "    coords_df = (df.groupby('target_id')[XYZ]\n",
        "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
        "                 .reset_index(name='coordinates'))\n",
        "    mask_df = (df.groupby('target_id')['coord_ok']\n",
        "               .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
        "               .reset_index(name='coord_mask'))\n",
        "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
        "\n",
        "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
        "    df = _clean_base(val_labels)\n",
        "\n",
        "    # 각 row(residue)마다 k=1..K 중 '처음으로 유효한' 좌표를 선택해 (x_1,y_1,z_1)에 저장\n",
        "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
        "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
        "    filled = np.zeros((len(df),), dtype=bool)\n",
        "\n",
        "    for k in range(1, K+1):\n",
        "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
        "        if not all(c in df.columns for c in cols):\n",
        "            continue\n",
        "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
        "        arr = tmp.to_numpy(dtype=np.float64)\n",
        "\n",
        "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < ABS_THRESH).all(axis=1)\n",
        "        take = ok & (~filled)\n",
        "        if take.any():\n",
        "            chosen[take] = arr[take].astype(np.float32)\n",
        "            ok_mask[take] = 1.0\n",
        "            filled[take] = True\n",
        "\n",
        "    df['x_1'], df['y_1'], df['z_1'] = chosen[:,0], chosen[:,1], chosen[:,2]\n",
        "    df['coord_ok'] = ok_mask\n",
        "\n",
        "    # target 단위 유효 포인트 수 기준 필터\n",
        "    valid_counts = df.groupby('target_id')['coord_ok'].sum()\n",
        "    good_ids = valid_counts[valid_counts >= MIN_VALID_POINTS].index\n",
        "    df = df[df['target_id'].isin(good_ids)].copy()\n",
        "\n",
        "    coords_df = (df.groupby('target_id')[XYZ]\n",
        "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
        "                 .reset_index(name='coordinates'))\n",
        "    mask_df = (df.groupby('target_id')['coord_ok']\n",
        "               .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
        "               .reset_index(name='coord_mask'))\n",
        "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
        "\n",
        "train_coords = build_coords_from_train_labels(train_labels)\n",
        "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
        "\n",
        "# key 정리\n",
        "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
        "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
        "\n",
        "# seq/coords 합치기\n",
        "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
        "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
        "\n",
        "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
        "print(\"all_df shape:\", all_df.shape)\n",
        "\n",
        "# max_len 창에서 유효 포인트가 너무 적으면 제거 (Kabsch 안정화)\n",
        "MAX_LEN = 200\n",
        "all_df['valid_in_window'] = all_df['coord_mask'].apply(\n",
        "    lambda m: float(np.sum(np.asarray(m, dtype=np.float32)[:MAX_LEN]))\n",
        ")\n",
        "before = len(all_df)\n",
        "all_df = all_df[all_df['valid_in_window'] >= 30].copy()\n",
        "print(f\"Filtered all_df by valid_in_window>=30: {before} -> {len(all_df)}\")\n",
        "\n",
        "# sanity: NaN/Inf check\n",
        "def has_nan_inf(coords):\n",
        "    a = np.asarray(coords, dtype=np.float32)\n",
        "    return (not np.isfinite(a).all())\n",
        "print(\"NaN/Inf coords after cleaning:\", all_df['coordinates'].apply(has_nan_inf).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ba2ebf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 4. Kabsch RMSD Loss (mask supported, and correct SVD handling)\n",
        "# ==========================================\n",
        "def kabsch_rotation(P, Q, mask=None):\n",
        "    \"\"\"\n",
        "    P, Q: (B, N, 3)\n",
        "    mask: (B, N)  1(valid), 0(invalid/pad)\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        mask = torch.ones(P.shape[0], P.shape[1], device=P.device, dtype=P.dtype)\n",
        "\n",
        "    mask_exp = mask.unsqueeze(-1)  # (B,N,1)\n",
        "    mask_sum = mask_exp.sum(dim=1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "    P_mean = (P * mask_exp).sum(dim=1, keepdim=True) / mask_sum\n",
        "    Q_mean = (Q * mask_exp).sum(dim=1, keepdim=True) / mask_sum\n",
        "\n",
        "    P_c = (P - P_mean) * mask_exp\n",
        "    Q_c = (Q - Q_mean) * mask_exp\n",
        "\n",
        "    H = torch.matmul(P_c.transpose(1, 2), Q_c)  # (B,3,3)\n",
        "\n",
        "    # torch.linalg.svd returns U,S,Vh\n",
        "    U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
        "    V = Vh.transpose(1, 2)\n",
        "\n",
        "    det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
        "    sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
        "\n",
        "    E = torch.eye(3, device=P.device, dtype=P.dtype).unsqueeze(0).repeat(P.shape[0], 1, 1)\n",
        "    E[:, 2, 2] = sign\n",
        "\n",
        "    R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
        "    P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
        "\n",
        "    return P_aligned * mask_exp\n",
        "\n",
        "\n",
        "class KabschRMSDLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, preds, target, mask):\n",
        "        \"\"\"\n",
        "        preds : (B, K, N, 3)\n",
        "        target: (B, N, 3)\n",
        "        mask  : (B, N) 1(valid), 0(invalid/pad)\n",
        "        \"\"\"\n",
        "        B, K, N, _ = preds.shape\n",
        "        losses = []\n",
        "        for k in range(K):\n",
        "            pred_k = preds[:, k, :, :]\n",
        "            pred_aligned = kabsch_rotation(pred_k, target, mask)\n",
        "\n",
        "            diff_sq = (pred_aligned - target) ** 2\n",
        "            sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
        "            n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)           # (B,)\n",
        "            mse = sum_sq / n_valid\n",
        "            rmsd = torch.sqrt(mse + 1e-8)\n",
        "            losses.append(rmsd)\n",
        "\n",
        "        losses = torch.stack(losses, dim=1)  # (B,K)\n",
        "        min_loss, _ = torch.min(losses, dim=1)\n",
        "        return torch.mean(min_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff15b7dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 5. Dataset (uses coord_mask to ignore missing labels)\n",
        "# ==========================================\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, sequences, coordinates, coord_masks, max_len=200):\n",
        "        self.sequences = sequences\n",
        "        self.coordinates = coordinates\n",
        "        self.coord_masks = coord_masks\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        coords = np.asarray(self.coordinates[idx], dtype=np.float32)\n",
        "        c_mask = np.asarray(self.coord_masks[idx], dtype=np.float32)\n",
        "\n",
        "        # 안전하게 길이 맞추기\n",
        "        L = min(len(seq), coords.shape[0], c_mask.shape[0], self.max_len)\n",
        "\n",
        "        # seq padding\n",
        "        seq_padded = np.zeros(self.max_len, dtype=np.int64)\n",
        "        seq_padded[:L] = np.asarray(seq[:L], dtype=np.int64)\n",
        "\n",
        "        # coords padding\n",
        "        coords_padded = np.zeros((self.max_len, 3), dtype=np.float32)\n",
        "        coords_padded[:L] = coords[:L]\n",
        "\n",
        "        # 최종 mask: 라벨 유효(coord_ok)만 1 (패딩/결측은 0)\n",
        "        mask = np.zeros(self.max_len, dtype=np.float32)\n",
        "        mask[:L] = c_mask[:L]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(seq_padded, dtype=torch.long),\n",
        "            torch.tensor(coords_padded, dtype=torch.float32),\n",
        "            torch.tensor(mask, dtype=torch.float32),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 6. Train/Holdout split & DataLoaders\n",
        "#    - val을 학습 데이터로 포함했으므로, 별도 holdout을 만든다\n",
        "# ==========================================\n",
        "train_idx, val_idx = train_test_split(range(len(all_df)), test_size=0.1, random_state=42)\n",
        "\n",
        "train_df = all_df.iloc[train_idx].reset_index(drop=True)\n",
        "hold_df  = all_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"train_df:\", train_df.shape, \"hold_df:\", hold_df.shape)\n",
        "\n",
        "train_dataset = RNADataset(\n",
        "    train_df['tokenized'].values,\n",
        "    train_df['coordinates'].values,\n",
        "    train_df['coord_mask'].values,\n",
        "    max_len=200\n",
        ")\n",
        "val_dataset = RNADataset(\n",
        "    hold_df['tokenized'].values,\n",
        "    hold_df['coordinates'].values,\n",
        "    hold_df['coord_mask'].values,\n",
        "    max_len=200\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc0d681",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 7. Model (Transformer + padding mask for attention)\n",
        "# ==========================================\n",
        "class RNATransformer(nn.Module):\n",
        "    def __init__(self, n_tokens=5, d_model=128, nhead=8, num_layers=4, dropout=0.1, num_preds=5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(n_tokens, d_model, padding_idx=0)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, 1000, d_model))\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "\n",
        "        self.num_preds = num_preds\n",
        "        self.fc_coords = nn.Linear(d_model, 3 * num_preds)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: (B,T)\n",
        "        B, T = tokens.shape\n",
        "        padding_mask = (tokens == 0)  # True at PAD positions\n",
        "\n",
        "        x = self.embedding(tokens)\n",
        "        x = x + self.pos_encoder[:, :T, :]\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        out = self.fc_coords(x)  # (B,T,3*K)\n",
        "        out = out.view(B, T, self.num_preds, 3).permute(0, 2, 1, 3)  # (B,K,T,3)\n",
        "        return out\n",
        "\n",
        "model = RNATransformer(num_preds=5).to(device)\n",
        "print(\"Model Initialized (Best-of-5 Output Strategy).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 8. Train (robust batch filtering)\n",
        "# ==========================================\n",
        "criterion = KabschRMSDLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-2)  # 더 안정적으로\n",
        "epochs = 5\n",
        "\n",
        "print(f\"Starting Training for {epochs} epochs...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    n_steps = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    for seq, target, mask in pbar:\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # ===== 핵심: 유효 포인트 부족 샘플 제거 =====\n",
        "        # Kabsch는 최소 3점(=3 residues) 이상이 있어야 회전 정렬이 의미가 있고 수치적으로 안전함\n",
        "        valid_counts = mask.sum(dim=1)  # (B,)\n",
        "        keep = valid_counts >= 30       # 여기서 30은 안정 마진(3보다 크게 추천)\n",
        "\n",
        "        if keep.sum() < 2:\n",
        "            # 배치에 남는 샘플이 너무 적으면 그냥 스킵\n",
        "            continue\n",
        "\n",
        "        seq = seq[keep]\n",
        "        target = target[keep]\n",
        "        mask = mask[keep]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        preds = model(seq)\n",
        "        loss = criterion(preds, target, mask)\n",
        "\n",
        "        if not torch.isfinite(loss):\n",
        "            # 여기까지 왔는데도 NaN이면, 더 강한 방어 로깅\n",
        "            print(\"Warning: Loss is NaN/Inf even after filtering. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        n_steps += 1\n",
        "        pbar.set_postfix({'loss': float(loss.item()), 'kept': int(keep.sum())})\n",
        "\n",
        "    avg_loss = train_loss / max(1, n_steps)\n",
        "    print(f\"Epoch {epoch+1} Average Train Loss: {avg_loss:.6f} (steps={n_steps})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f15536",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 9. Validation\n",
        "# ==========================================\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for seq, target, mask in val_loader:\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        preds = model(seq)\n",
        "        loss = criterion(preds, target, mask)\n",
        "\n",
        "        if torch.isfinite(loss):\n",
        "            val_loss += loss.item()\n",
        "\n",
        "val_rmsd = val_loss / max(1, len(val_loader))\n",
        "print(f\"Validation RMSD (masked): {val_rmsd:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28659606",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 10. Visualization (uses mask correctly)\n",
        "# ==========================================\n",
        "seq_batch, target_batch, mask_batch = next(iter(val_loader))\n",
        "seq_batch = seq_batch.to(device)\n",
        "target_batch = target_batch.to(device)\n",
        "mask_batch = mask_batch.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_batch = model(seq_batch)  # (B,5,T,3)\n",
        "\n",
        "mask0 = mask_batch[0].detach().cpu().numpy().astype(bool)\n",
        "t0 = target_batch[0].detach().cpu().numpy()[mask0]     # (L_valid,3)\n",
        "p0 = pred_batch[0, 0].detach().cpu().numpy()[mask0]    # pred k=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7269c5da",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(t0[:, 0], label=\"Actual X\", alpha=0.7)\n",
        "plt.plot(p0[:, 0], label=\"Pred X (k=0)\", linestyle=\"--\", alpha=0.9)\n",
        "plt.title(f\"Actual vs Predicted X (valid points={mask0.sum()})\")\n",
        "plt.xlabel(\"Valid-point Index\")\n",
        "plt.ylabel(\"X-coordinate\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, name in enumerate([\"X\", \"Y\", \"Z\"]):\n",
        "    plt.plot(t0[:, i], label=f\"Actual {name}\", alpha=0.5)\n",
        "    plt.plot(p0[:, i], label=f\"Pred {name}\", linestyle=\"--\", alpha=0.8)\n",
        "plt.title(\"Actual vs Predicted XYZ (masked)\")\n",
        "plt.legend(ncol=3)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot(t0[:, 0], t0[:, 1], t0[:, 2], label=\"Actual\", alpha=0.7)\n",
        "ax.plot(p0[:, 0], p0[:, 1], p0[:, 2], label=\"Pred (k=0)\", alpha=0.7, linestyle=\"--\")\n",
        "ax.set_title(\"3D Trace (masked)\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rna_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
