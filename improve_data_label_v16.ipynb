{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5ce781",
   "metadata": {},
   "source": [
    "# improve_data_label_v15\n",
    "\n",
    "v15: v14(로더/체크포인트 안정화) + **좌표 초기화/출력 경로 수정**으로 예측 붕괴(선분/클러스터) 문제를 해결한 버전.\n",
    "\n",
    "- 핵심 수정: EGNN 내부 좌표 `x`를 0으로 시작하지 않고, **학습 가능한 init_x(h)** 로 초기화 → 좌표 업데이트가 실제로 동작\n",
    "- 출력: `preds = x + head_offset(h)` 구조로 v12 스타일 복원 (멀티헤드 유지)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16412a32",
   "metadata": {},
   "source": [
    "## v15 변경점 (v14 대비 예측 붕괴/좌표 업데이트 무력화 해결)\n",
    "\n",
    "1) **좌표 초기화(degenerate zero-init) 제거**\n",
    "- v13/v14는 `x=0`에서 시작해 `dx = (xi-xj)*w` 이라서 **항상 dx=0 → x가 영원히 0** 문제가 생길 수 있음\n",
    "- v15는 `x = init_x(h)` 로 시작해 **rij가 0이 아니게 만들고**, EGNN의 좌표 업데이트가 실제로 동작\n",
    "\n",
    "2) **출력 경로 수정**\n",
    "- `preds = x + offset_k(h)` 형태로, EGNN이 만든 기하 정보를 출력에 직접 반영\n",
    "\n",
    "3) 나머지(로더/체크포인트/학습루프)는 v14 그대로\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0) Imports, Device, Config  [v16]\n",
    "#   - NaN 방지: (1) loss는 FP32로 계산, (2) mask 곱셈 대신 masked_fill/where 사용,\n",
    "#              (3) softmin에서 inf/NaN sanitize\n",
    "#   - 안정화: EGNN edge에서 pad 노드 연결 제거 + coord update tanh/clamp-scale\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "\n",
    "    # data\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 10\n",
    "    batch: int = 8\n",
    "    num_workers: int = 0          # ✅ v14: 안전 기본값 (Windows/Jupyter에서 hang 방지)\n",
    "    loader_timeout: int = 60      # ✅ num_workers>0 일 때만 사용\n",
    "\n",
    "    # model\n",
    "    vocab: int = 5  # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 192\n",
    "    d_edge: int = 256\n",
    "    n_layers: int = 8\n",
    "    k_nn: int = 12  # sequence neighborhood edges\n",
    "    num_preds: int = 4  # K heads\n",
    "    dropout: float = 0.1\n",
    "    init_x_scale: float = 1.0   # learnable init_x(h) 스케일\n",
    "    offset_scale: float = 0.1   # head offset 스케일(초기 안정화)\n",
    "    coord_step_scale: float = 0.10  # ✅ v16: EGNN 좌표 업데이트 스텝(폭발 방지)\n",
    "\n",
    "    # base-pair feature\n",
    "    bp_tau: float = 40.0         # distance decay for |i-j|\n",
    "    bp_min_sep: int = 4          # do not pair too-close residues\n",
    "    pair_alpha: float = 2.0      # message/coord weight boost: (1 + pair_alpha * p_ij)\n",
    "\n",
    "    # optimization\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 0.02\n",
    "    epochs: int = 20\n",
    "    warmup_epochs: int = 2\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # mixed precision / stability\n",
    "    amp: bool = True             # ✅ 필요하면 False로 꺼서 NaN 여부 확인\n",
    "    loss_fp32: bool = True       # ✅ v16: loss 계산은 FP32 고정(AMP 환경에서도 안정화)\n",
    "    fail_on_nan: bool = True     # ✅ v16: NaN/Inf 감지 시 즉시 중단(원인 추적)\n",
    "\n",
    "    # softmin aggregation\n",
    "    softmin_temp: float = 1.0\n",
    "\n",
    "    # losses\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05\n",
    "\n",
    "    dist_w: float = 0.05         # weak distance auxiliary (as requested)\n",
    "    pair_num_pairs: int = 512    # sampled pairs for distance/repulsion (speed-tuned)\n",
    "    aux_every: int = 2           # compute expensive aux losses every N steps\n",
    "    local_w: float = 0.2\n",
    "    var_w: float = 0.02\n",
    "    repulse_w: float = 0.02\n",
    "    diversity_w: float = 0.01\n",
    "    repulse_margin: float = 2.5\n",
    "    diversity_margin: float = 2.0\n",
    "\n",
    "    # checkpoint / resume\n",
    "    ckpt_dir: str = \"checkpoints\"\n",
    "    ckpt_best_path: str = \"checkpoints/best_structured_v16.pt\"\n",
    "    ckpt_stage1_path: str = \"checkpoints/best_stage1_v16.pt\"\n",
    "    resume_path: str = \"\"          # 예) \"checkpoints/last_v16.pt\" 또는 v12/v13 state_dict 파일\n",
    "    patience: int = 10\n",
    "\n",
    "    # debug\n",
    "    profile_first_batch: bool = True  # 첫 배치에서 to_gpu/forward 타이밍 출력\n",
    "\n",
    "cfg = CFG()\n",
    "os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "# (선택) matmul 정밀도 힌트 (Ampere+에서 유효)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73414564",
   "metadata": {},
   "source": [
    "## 1) Dataset / Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2e4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_5376\\1999568070.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n",
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n",
      "train_coords bad mask: 0\n",
      "val_coords   bad mask: 0\n",
      "train_coords max|coord| (valid only, sample<=500): 778.1259765625\n",
      "val_coords   max|coord| (valid only, sample<=500): 420.6499938964844\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n",
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e4  # v16: float16 overflow/이상치 방지용 상한\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "\n",
    "\n",
    "def has_bad_mask(m):\n",
    "    a = np.asarray(m, dtype=np.float32)\n",
    "    if (not np.isfinite(a).all()):\n",
    "        return True\n",
    "    # allow only 0/1 (float)\n",
    "    return bool(((a != 0.0) & (a != 1.0)).any())\n",
    "\n",
    "def max_abs_valid(coords, mask):\n",
    "    c = np.asarray(coords, dtype=np.float32)\n",
    "    m = np.asarray(mask, dtype=np.float32).astype(bool)\n",
    "    if m.sum() == 0:\n",
    "        return 0.0\n",
    "    return float(np.abs(c[m]).max())\n",
    "\n",
    "print(\"train_coords bad mask:\", train_coords['coord_mask'].apply(has_bad_mask).sum())\n",
    "print(\"val_coords   bad mask:\", val_coords['coord_mask'].apply(has_bad_mask).sum())\n",
    "\n",
    "# 좌표 범위 체크(유효 마스크 기준)\n",
    "_tc = train_coords.sample(n=min(len(train_coords), 500), random_state=0) if len(train_coords)>500 else train_coords\n",
    "print(\"train_coords max|coord| (valid only, sample<=500):\",\n",
    "      _tc.apply(lambda r: max_abs_valid(r['coordinates'], r['coord_mask']), axis=1).max())\n",
    "_vc = val_coords.sample(n=min(len(val_coords), 500), random_state=0) if len(val_coords)>500 else val_coords\n",
    "print(\"val_coords   max|coord| (valid only, sample<=500):\",\n",
    "      _vc.apply(lambda r: max_abs_valid(r['coordinates'], r['coord_mask']), axis=1).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b18db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (5739, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n",
      "train batches: 646 hold batches: 72\n",
      "[loader precheck] first batch OK in 0.09s\n",
      "[loader precheck] batch shapes: [torch.Size([8, 256]), torch.Size([8, 256, 3]), torch.Size([8, 256])]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 직접 참조 (복사 최소화)\n",
    "        tokens = self.tokens_list[idx]\n",
    "        coords = self.coords_list[idx]\n",
    "        mask   = self.mask_list[idx]\n",
    "\n",
    "        # numpy 변환 (필요 시 1회만)\n",
    "        if not isinstance(tokens, np.ndarray):\n",
    "            tokens = np.array(tokens, dtype=np.int64)\n",
    "        if not isinstance(coords, np.ndarray):\n",
    "            coords = np.array(coords, dtype=np.float32)\n",
    "        if not isinstance(mask, np.ndarray):\n",
    "            mask = np.array(mask, dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        if self.center_only:\n",
    "            valid = mask.astype(bool)\n",
    "            if valid.any():\n",
    "                coords = coords - coords[valid].mean(axis=0, keepdims=True)\n",
    "\n",
    "        # padding (vectorized, 최소 연산)\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64)\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32)\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32)\n",
    "\n",
    "        tokens_p[:L] = tokens\n",
    "        coords_p[:L] = coords\n",
    "        mask_p[:L]   = mask\n",
    "\n",
    "        # torch.from_numpy (복사 없음 → 매우 빠름)\n",
    "        return (\n",
    "            torch.from_numpy(tokens_p),\n",
    "            torch.from_numpy(coords_p),\n",
    "            torch.from_numpy(mask_p),\n",
    "        )\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader  [v13.2 speed]\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "dl_num_workers = int(getattr(cfg, \"num_workers\", 0) or 0)\n",
    "\n",
    "# NOTE (Windows/Jupyter): num_workers>0가 멈춘 것처럼 보이는 경우가 많습니다.\n",
    "# v14 기본값은 0이며, 속도가 필요하면 2→4로 천천히 올려가며 확인하세요.\n",
    "if os.name == \"nt\":\n",
    "    dl_num_workers = 0\n",
    "\n",
    "# ✅ 중요: PyTorch는 num_workers==0 인데 timeout>0이면 AssertionError가 날 수 있습니다.\n",
    "timeout = int(getattr(cfg, \"loader_timeout\", 0) or 0) if (dl_num_workers > 0) else 0\n",
    "\n",
    "dl_kwargs = dict(\n",
    "    num_workers=dl_num_workers,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "if dl_num_workers > 0:\n",
    "    dl_kwargs.update(dict(persistent_workers=True, prefetch_factor=2))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch,\n",
    "    shuffle=True,\n",
    "    timeout=timeout,\n",
    "    **dl_kwargs\n",
    ")\n",
    "hold_loader = DataLoader(\n",
    "    hold_ds,\n",
    "    batch_size=cfg.batch,\n",
    "    shuffle=False,\n",
    "    timeout=timeout,\n",
    "    **dl_kwargs\n",
    ")\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n",
    "\n",
    "# ---- First-batch sanity check (detect loader hangs / preprocessing stalls) ----\n",
    "import time as _time\n",
    "_t0 = _time.time()\n",
    "try:\n",
    "    _it = iter(train_loader)\n",
    "    _b = next(_it)\n",
    "    print(f\"[loader precheck] first batch OK in {_time.time()-_t0:.2f}s\")\n",
    "    # print shapes\n",
    "    if isinstance(_b, (tuple, list)):\n",
    "        print(\"[loader precheck] batch shapes:\", [getattr(x, \"shape\", type(x)) for x in _b])\n",
    "except Exception as e:\n",
    "    print(\"[loader precheck] FAILED:\", repr(e))\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4df894",
   "metadata": {},
   "source": [
    "## 2) Base-pair feature 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02c55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2) Base-pair probability encoder (sequence-only heuristic)\n",
    "#   - 입력 tokens(B,T) -> p(B,T,T) in [0,1]\n",
    "#   - canonical: A-U, C-G, G-U wobble\n",
    "#   - 거리 prior: exp(-|i-j|/tau), and |i-j|>=bp_min_sep\n",
    "# ==========================================\n",
    "class BasePairEncoder(nn.Module):\n",
    "    def __init__(self, tau: float = 40.0, min_sep: int = 4):\n",
    "        super().__init__()\n",
    "        self.tau = float(tau)\n",
    "        self.min_sep = int(min_sep)\n",
    "\n",
    "        # 0 PAD, 1 A,2 C,3 G,4 U\n",
    "        # canonical probs\n",
    "        P = torch.zeros((5,5), dtype=torch.float32)\n",
    "        P[1,4] = 1.0; P[4,1] = 1.0  # A-U\n",
    "        P[2,3] = 1.0; P[3,2] = 1.0  # C-G\n",
    "        P[3,4] = 0.6; P[4,3] = 0.6  # G-U wobble (weaker)\n",
    "        self.register_buffer(\"pair_table\", P, persistent=False)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, pad_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        tokens: (B,T) int64\n",
    "        pad_mask: (B,T) True for valid nodes (optional)\n",
    "        returns p: (B,T,T) float32\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        t_i = tokens[:, :, None]  # (B,T,1)\n",
    "        t_j = tokens[:, None, :]  # (B,1,T)\n",
    "\n",
    "        base = self.pair_table[t_i, t_j]  # (B,T,T)\n",
    "\n",
    "        # 거리 prior\n",
    "        idx = torch.arange(T, device=tokens.device)\n",
    "        dist = (idx[None, :, None] - idx[None, None, :]).abs().float()  # (1,T,T)\n",
    "        sep_ok = (dist >= float(self.min_sep)).float()\n",
    "        prior = torch.exp(-dist / max(self.tau, 1e-6)) * sep_ok\n",
    "\n",
    "        p = base * prior  # (B,T,T)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            m = pad_mask.float()\n",
    "            p = p * (m[:, :, None] * m[:, None, :])\n",
    "\n",
    "        # zero diagonal\n",
    "        p = p * (1.0 - torch.eye(T, device=p.device, dtype=p.dtype)[None, :, :])\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4f945",
   "metadata": {},
   "source": [
    "## 3) Loss helpers (Kabsch + sampled distance + local/var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3eab1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3) Loss utilities  [v16 NaN-safe]\n",
    "#   - mask 곱셈(0)과 inf의 조합(inf*0=nan) 방지: masked_fill/where 방식\n",
    "#   - softmin에서 inf/NaN sanitize\n",
    "#   - Kabsch는 FP32에서 수행\n",
    "# ==========================================\n",
    "def _sanitize_losses(x: torch.Tensor, large: float = 1e6) -> torch.Tensor:\n",
    "    # nan/inf -> 큰 유한값으로 치환 (softmin/가중합에서 0*inf -> nan 방지)\n",
    "    return torch.nan_to_num(x, nan=large, posinf=large, neginf=large)\n",
    "\n",
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    temp = max(float(temperature), 1e-8)\n",
    "    L = _sanitize_losses(losses.float())\n",
    "    return torch.softmax(-L / temp, dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    L = _sanitize_losses(losses.float())\n",
    "    w = softmin_weights(L, temperature)\n",
    "    # w and L are finite now\n",
    "    return (w * L).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    # preds: (B,K,T,3)\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.bool()  # (B,T)\n",
    "    m3 = m.unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "    denom = (m.sum(dim=1).float() * 3.0).clamp_min(1.0)  # (B,)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k].float()\n",
    "        tg = target.float()\n",
    "        # invalid positions are force-set to 0 without multiplication (inf*0 회피)\n",
    "        pk = pk.masked_fill(~m3, 0.0)\n",
    "        tg = tg.masked_fill(~m3, 0.0)\n",
    "        diff = (pk - tg).abs()\n",
    "        l1 = diff.sum(dim=(1, 2)) / denom\n",
    "        out.append(l1)\n",
    "    return _sanitize_losses(torch.stack(out, dim=1))\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type=('cuda' if P.is_cuda else 'cpu'), enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.bool()\n",
    "    m3 = m.unsqueeze(-1)\n",
    "    denom = (m.sum(dim=1).float() * 3.0).clamp_min(1.0)  # (B,)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        # invalid positions -> 0으로 강제\n",
    "        pa = pk_aligned.float().masked_fill(~m3, 0.0)\n",
    "        tg = target.float().masked_fill(~m3, 0.0)\n",
    "        diff_sq = (pa - tg) ** 2\n",
    "        sum_sq = diff_sq.sum(dim=(1, 2))  # (B,)\n",
    "        rmsd = torch.sqrt(sum_sq / denom + 1e-8)\n",
    "        out.append(rmsd)\n",
    "    return _sanitize_losses(torch.stack(out, dim=1))\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    preds_f = preds.float()\n",
    "    target_f = target.float()\n",
    "\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=torch.float32)\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target_f[b, i] - target_f[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds_f[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return _sanitize_losses(losses_bk)\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.bool()\n",
    "    m3 = m.unsqueeze(-1)\n",
    "\n",
    "    denom = m.sum(dim=1).float().clamp_min(1.0)  # (B,)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k].float()\n",
    "        pk = pk.masked_fill(~m3, 0.0)\n",
    "        mean = pk.sum(dim=1, keepdim=True) / denom.view(B, 1, 1)\n",
    "        var = ((pk - mean) ** 2).sum(dim=(1, 2)) / (denom * 3.0)\n",
    "        out.append(var)\n",
    "    return _sanitize_losses(torch.stack(out, dim=1))\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.bool()\n",
    "    m_adj = (m[:, 1:] & m[:, :-1])  # (B,T-1)\n",
    "\n",
    "    preds_f = preds.float()\n",
    "    target_f = target.float()\n",
    "\n",
    "    tgt = (target_f[:, 1:] - target_f[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    denom = m_adj.sum(dim=1).float().clamp_min(1.0)  # (B,)\n",
    "\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds_f[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        diff = diff.masked_fill(~m_adj, 0.0)\n",
    "        l = diff.sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return _sanitize_losses(torch.stack(out, dim=1))\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned.float() - target.float()).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0, dtype=torch.float32))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2774e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# v11 추가: non-neighbor repulsion + head diversity  [v16 NaN-safe]\n",
    "# ==========================================\n",
    "def repulsion_losses_sampled(preds: torch.Tensor, mask: torch.Tensor,\n",
    "                             num_pairs: int = 2048, margin: float = 2.5) -> torch.Tensor:\n",
    "    \"\"\"겹침 방지용 hinge loss. 인접(i,i+1)은 제외하고 랜덤 pair에 대해\n",
    "    d < margin 이면 (margin-d)^2 를 부과.\n",
    "    returns (B,K)\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    preds_f = preds.float()\n",
    "    losses = torch.zeros((B, K), device=device_, dtype=torch.float32)\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 3:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        nn_mask = (torch.abs(i - j) > 1)\n",
    "        if nn_mask.sum() < 8:\n",
    "            continue\n",
    "        i = i[nn_mask]\n",
    "        j = j[nn_mask]\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds_f[b, k]\n",
    "            d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            hinge = (margin - d).clamp_min(0.0)\n",
    "            losses[b, k] = (hinge * hinge).mean()\n",
    "\n",
    "    return _sanitize_losses(losses)\n",
    "\n",
    "def head_diversity_losses(preds: torch.Tensor, mask: torch.Tensor, margin: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"헤드 간 유사하면 패널티. (B,K)로 반환해서 기존 softmin 프레임에 맞춘다.\n",
    "    각 헤드의 masked centered coords를 만들고, 헤드쌍 RMSD가 margin보다 작으면 hinge.\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    preds_f = preds.float()\n",
    "    m = mask.bool()\n",
    "    m3 = m.unsqueeze(-1)\n",
    "    denom = m.sum(dim=1).float().clamp_min(1.0)  # (B,)\n",
    "\n",
    "    # centered (invalid는 0)\n",
    "    centered = preds_f.masked_fill(~m3.unsqueeze(1), 0.0)\n",
    "    mean = centered.sum(dim=2, keepdim=True) / denom.view(B, 1, 1, 1)\n",
    "    centered = centered - mean\n",
    "    centered = centered.masked_fill(~m3.unsqueeze(1), 0.0)\n",
    "\n",
    "    out = torch.zeros((B, K), device=preds.device, dtype=torch.float32)\n",
    "    if K < 2:\n",
    "        return out\n",
    "\n",
    "    for a in range(K):\n",
    "        pen = 0.0\n",
    "        cnt = 0\n",
    "        for b in range(K):\n",
    "            if a == b:\n",
    "                continue\n",
    "            diff = (centered[:, a] - centered[:, b]) ** 2  # (B,T,3)\n",
    "            # invalid는 0으로 masked_fill (inf*0 방지)\n",
    "            diff = diff.masked_fill(~m3, 0.0)\n",
    "            rmsd = torch.sqrt(diff.sum(dim=(1, 2)) / (denom * 3.0) + 1e-8)  # (B,)\n",
    "            hinge = (margin - rmsd).clamp_min(0.0)\n",
    "            pen = pen + hinge * hinge\n",
    "            cnt += 1\n",
    "        out[:, a] = pen / max(cnt, 1)\n",
    "\n",
    "    return _sanitize_losses(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58a6a7",
   "metadata": {},
   "source": [
    "## 4) Model (Pair-aware EGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd83612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4) Model (EGNN backbone + Pair-aware message passing + K coord heads + ConfidenceHead)  [v16]\n",
    "#   - pad edge masking: (pad->valid) ghost message 제거\n",
    "#   - coord update 안정화: tanh + coord_step_scale\n",
    "#   - v15: init_x(h) + preds = x + offset_k(h)\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos)[None, :, :]\n",
    "\n",
    "def build_seq_edges(T: int, k: int, device):\n",
    "    r = max(1, k // 2)\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(T):\n",
    "        for j in range(max(0, i - r), min(T, i + r + 1)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            src.append(i); dst.append(j)\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long, device=device)  # (2,E)\n",
    "    return edge_index\n",
    "\n",
    "class EGNNPairAwareLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EGNN + pair feature p_ij (base-pair 확률)\n",
    "    message: phi(h_i, h_j, d_ij^2, p_ij)\n",
    "    + long-range boost: (1 + pair_alpha * p_ij) 를 coord/node 업데이트에 곱한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_node: int, d_edge: int, dropout: float, pair_alpha: float = 2.0, coord_scale: float = 0.10):\n",
    "        super().__init__()\n",
    "        self.pair_alpha = float(pair_alpha)\n",
    "        self.coord_scale = float(coord_scale)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node*2 + 1 + 1, d_edge),  # +p_ij\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_edge, 1),\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node + d_edge, d_node),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_node, d_node),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(d_node)\n",
    "\n",
    "    def forward(self, h, x, edge_index, node_mask, pair_p):\n",
    "        \"\"\"\n",
    "        h: (B,T,D), x: (B,T,3)\n",
    "        edge_index: (2,E)\n",
    "        node_mask: (B,T) bool\n",
    "        pair_p: (B,T,T) float in [0,1]\n",
    "        \"\"\"\n",
    "        B, T, D = h.shape\n",
    "        src, dst = edge_index[0], edge_index[1]  # (E,)\n",
    "\n",
    "        hi = h[:, src, :]\n",
    "        hj = h[:, dst, :]\n",
    "        xi = x[:, src, :]\n",
    "        xj = x[:, dst, :]\n",
    "\n",
    "        rij = xi - xj\n",
    "        dij2 = (rij * rij).sum(dim=-1, keepdim=True)  # (B,E,1)\n",
    "        pij = pair_p[:, src, dst].unsqueeze(-1)        # (B,E,1)\n",
    "\n",
    "        # ✅ edge mask: pad 노드가 섞인 edge는 완전히 제거(ghost message 방지)\n",
    "        if node_mask is not None:\n",
    "            evalid = (node_mask[:, src] & node_mask[:, dst]).float().unsqueeze(-1)  # (B,E,1)\n",
    "        else:\n",
    "            evalid = 1.0\n",
    "\n",
    "        m_ij = self.edge_mlp(torch.cat([hi, hj, dij2, pij], dim=-1))  # (B,E,d_edge)\n",
    "        m_ij = m_ij * evalid\n",
    "\n",
    "        boost = (1.0 + self.pair_alpha * pij).clamp(0.0, 10.0) * evalid\n",
    "\n",
    "        # ✅ coord update 안정화: tanh로 제한 + step scale\n",
    "        w = torch.tanh(self.coord_mlp(m_ij)) * (self.coord_scale) * boost  # (B,E,1)\n",
    "        dx = rij * w  # (B,E,3)\n",
    "\n",
    "        agg_dx = x.new_zeros((B, T, 3))\n",
    "        agg_m  = h.new_zeros((B, T, m_ij.size(-1)))\n",
    "\n",
    "        # aggregate to src node\n",
    "        agg_dx.index_add_(1, src, dx.to(agg_dx.dtype))\n",
    "        agg_m.index_add_(1, src, (m_ij * boost).to(agg_m.dtype))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            agg_dx = agg_dx.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            agg_m  = agg_m.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "\n",
    "        x = x + agg_dx\n",
    "        h = self.ln(h + self.node_mlp(torch.cat([h, agg_m], dim=-1)))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            h = h.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            x = x.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "        return h, x\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(cfg.d_model//2, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        if pad_mask is None:\n",
    "            pooled = h.mean(dim=1)\n",
    "        else:\n",
    "            m = pad_mask.float().unsqueeze(-1)\n",
    "            denom = m.sum(dim=1).clamp_min(1.0)\n",
    "            pooled = (h * m).sum(dim=1) / denom\n",
    "        return self.mlp(pooled)  # (B,K)\n",
    "\n",
    "class EGNNv16(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(cfg.vocab, cfg.d_model, padding_idx=0)\n",
    "        self.posenc = PositionalEncodingLearned(cfg.d_model, max_len=cfg.max_len)\n",
    "        self.bp_enc = BasePairEncoder(tau=cfg.bp_tau, min_sep=cfg.bp_min_sep)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EGNNPairAwareLayer(cfg.d_model, cfg.d_edge, cfg.dropout,\n",
    "                               pair_alpha=cfg.pair_alpha,\n",
    "                               coord_scale=float(getattr(cfg, \"coord_step_scale\", 0.10)))\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        # ✅ v15: learnable coord init (breaks zero-symmetry)\n",
    "        self.init_x = nn.Linear(cfg.d_model, 3)\n",
    "        self.init_scale = float(getattr(cfg, \"init_x_scale\", 1.0))\n",
    "\n",
    "        # ✅ v15: K-head offsets (v12 style)\n",
    "        self.offset = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(cfg.d_model, cfg.num_preds * 3),\n",
    "        )\n",
    "        self.offset_scale = float(getattr(cfg, \"offset_scale\", 0.1))\n",
    "\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "        # init\n",
    "        nn.init.normal_(self.init_x.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.init_x.bias)\n",
    "\n",
    "    def forward(self, tokens, pad_mask=None):\n",
    "        \"\"\"\n",
    "        tokens: (B,T)\n",
    "        pad_mask: (B,T) bool (True for valid nodes)\n",
    "        returns:\n",
    "          preds: (B,K,T,3)\n",
    "          conf_logits: (B,K)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        if pad_mask is None:\n",
    "            pad_mask = (tokens != 0)\n",
    "\n",
    "        h = self.embed(tokens)\n",
    "        h = self.posenc(h)\n",
    "        h = h.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        # ---- initialize coords from h (non-zero) ----\n",
    "        x = self.init_x(h) * self.init_scale  # (B,T,3)\n",
    "        x = x.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        # center x to remove translation (stabilizes warmup L1; target is centered in dataset)\n",
    "        m = pad_mask.float().unsqueeze(-1)\n",
    "        denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "        x = x - (x * m).sum(dim=1, keepdim=True) / denom\n",
    "        x = x.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        edge_index = build_seq_edges(T, self.cfg.k_nn, tokens.device)\n",
    "\n",
    "        # pair probabilities (sequence-only heuristic)\n",
    "        pair_p = self.bp_enc(tokens, pad_mask)  # (B,T,T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h, x = layer(h, x, edge_index, pad_mask, pair_p)\n",
    "\n",
    "        # K-head offsets\n",
    "        off = self.offset(h).view(B, T, self.cfg.num_preds, 3) * self.offset_scale  # (B,T,K,3)\n",
    "        off = off.permute(0, 2, 1, 3).contiguous()  # (B,K,T,3)\n",
    "\n",
    "        preds = x.unsqueeze(1) + off  # (B,K,T,3)\n",
    "        preds = preds.masked_fill(~pad_mask[:, None, :, None], 0.0)\n",
    "\n",
    "        conf_logits = self.conf_head(h, pad_mask)  # (B,K)\n",
    "        return preds, conf_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9630",
   "metadata": {},
   "source": [
    "## 5) LossComposer (weak distance auxiliary 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d31a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5) LossComposer  [v16]\n",
    "#   - warmup: masked L1 (+optional confidence)\n",
    "#   - main: Kabsch RMSD + (optional sampled pairwise/repulsion) + local + variance + diversity (+confidence)\n",
    "#   - NaN-safe: loss 계산 FP32 고정 + sanitize\n",
    "# ==========================================\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, step: int = 0, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "        device_type = ('cuda' if preds.is_cuda else 'cpu')\n",
    "\n",
    "        # ✅ loss는 FP32에서 계산 (AMP로 인한 overflow/inf 방지)\n",
    "        if bool(getattr(self.cfg, \"loss_fp32\", True)):\n",
    "            with torch.amp.autocast(device_type=device_type, enabled=False):\n",
    "                preds_f = preds.float()\n",
    "                target_f = target.float()\n",
    "                mask_f   = mask.float()\n",
    "                conf_f   = conf_logits.float() if conf_logits is not None else None\n",
    "                return self._forward_impl(preds_f, target_f, mask_f, epoch, step, conf_f, temp)\n",
    "        else:\n",
    "            return self._forward_impl(preds, target, mask, epoch, step, conf_logits, temp)\n",
    "\n",
    "    def _forward_impl(self, preds, target, mask, epoch, step: int, conf_logits, temp: float):\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: structured losses (all are (B,K))\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)\n",
    "        var_bk  = coord_variance_losses(preds, mask)\n",
    "\n",
    "        # expensive aux losses\n",
    "        dist_bk = preds.new_zeros(rmsd_bk.shape).float()\n",
    "        rep_bk  = preds.new_zeros(rmsd_bk.shape).float()\n",
    "        div_bk  = preds.new_zeros(rmsd_bk.shape).float()\n",
    "\n",
    "        if (step % int(self.cfg.aux_every)) == 0:\n",
    "            if float(self.cfg.dist_w) > 0:\n",
    "                dist_bk = pairwise_distance_losses_sampled(preds, target, mask, num_pairs=int(self.cfg.pair_num_pairs))\n",
    "            if float(self.cfg.repulse_w) > 0:\n",
    "                rep_bk  = repulsion_losses_sampled(preds, mask, num_pairs=int(self.cfg.pair_num_pairs), margin=float(self.cfg.repulse_margin))\n",
    "            if float(self.cfg.diversity_w) > 0:\n",
    "                div_bk  = head_diversity_losses(preds, mask, margin=float(self.cfg.diversity_margin))\n",
    "\n",
    "        # combine (smaller is better)\n",
    "        total_bk = (\n",
    "            rmsd_bk\n",
    "            + float(self.cfg.local_w)     * loc_bk\n",
    "            - float(self.cfg.var_w)       * var_bk\n",
    "            + float(self.cfg.dist_w)      * dist_bk\n",
    "            + float(self.cfg.repulse_w)   * rep_bk\n",
    "            + float(self.cfg.diversity_w) * div_bk\n",
    "        )\n",
    "\n",
    "        total_bk = _sanitize_losses(total_bk)\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        # confidence: match distribution to softmin weights\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"STRUCTURED(+CONF)\" if aux != 0.0 else \"STRUCTURED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3584cf",
   "metadata": {},
   "source": [
    "## 6) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1577828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5.5) Checkpoint utils  [v14]\n",
    "#   - state_dict 기반 저장/로드 (pickling 클래스 의존 X)\n",
    "#   - flexible load: shape가 맞는 파라미터만 부분 로드\n",
    "# ==========================================\n",
    "from typing import Tuple, List, Optional, Any, Dict\n",
    "\n",
    "def save_checkpoint(path: str,\n",
    "                    model: nn.Module,\n",
    "                    opt: Optional[torch.optim.Optimizer],\n",
    "                    scaler: Optional[torch.amp.GradScaler],\n",
    "                    epoch: int,\n",
    "                    best_metric: Optional[float] = None,\n",
    "                    cfg_obj: Optional[CFG] = None,\n",
    "                    extra: Optional[Dict[str, Any]] = None):\n",
    "    ckpt: Dict[str, Any] = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model\": model.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "    }\n",
    "    if opt is not None:\n",
    "        ckpt[\"opt\"] = opt.state_dict()\n",
    "    if scaler is not None:\n",
    "        try:\n",
    "            ckpt[\"scaler\"] = scaler.state_dict()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if cfg_obj is not None:\n",
    "        ckpt[\"cfg\"] = dict(cfg_obj.__dict__)\n",
    "    if extra is not None:\n",
    "        ckpt[\"extra\"] = extra\n",
    "\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "def _unwrap_checkpoint(obj: Any) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Return (state_dict, meta_dict).\"\"\"\n",
    "    if isinstance(obj, dict) and (\"model\" in obj) and isinstance(obj[\"model\"], dict):\n",
    "        return obj[\"model\"], obj\n",
    "    # v12/v13 일부는 model.state_dict()만 저장했을 수 있음\n",
    "    if isinstance(obj, dict):\n",
    "        return obj, {}\n",
    "    raise TypeError(f\"Unsupported checkpoint type: {type(obj)}\")\n",
    "\n",
    "def load_state_dict_flexible(model: nn.Module, state_dict: Dict[str, torch.Tensor], verbose: bool = True):\n",
    "    model_sd = model.state_dict()\n",
    "    loadable: Dict[str, torch.Tensor] = {}\n",
    "    skipped: List[str] = []\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if (k in model_sd) and (tuple(model_sd[k].shape) == tuple(v.shape)):\n",
    "            loadable[k] = v\n",
    "        else:\n",
    "            skipped.append(k)\n",
    "\n",
    "    msg = model.load_state_dict(loadable, strict=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[flex load] loaded={len(loadable)}  skipped={len(skipped)}\")\n",
    "        # 너무 길어질 수 있으니 예시만\n",
    "        if skipped:\n",
    "            print(\"  - skipped examples:\", skipped[:10])\n",
    "        if hasattr(msg, \"missing_keys\") and msg.missing_keys:\n",
    "            print(\"  - missing_keys examples:\", msg.missing_keys[:10])\n",
    "        if hasattr(msg, \"unexpected_keys\") and msg.unexpected_keys:\n",
    "            print(\"  - unexpected_keys examples:\", msg.unexpected_keys[:10])\n",
    "\n",
    "    return msg, loadable, skipped\n",
    "\n",
    "def load_checkpoint(path: str,\n",
    "                    model: nn.Module,\n",
    "                    opt: Optional[torch.optim.Optimizer] = None,\n",
    "                    scaler: Optional[torch.amp.GradScaler] = None,\n",
    "                    map_location: Any = \"cpu\",\n",
    "                    flexible: bool = True,\n",
    "                    verbose: bool = True):\n",
    "    obj = torch.load(path, map_location=map_location)\n",
    "    state_dict, meta = _unwrap_checkpoint(obj)\n",
    "\n",
    "    if flexible:\n",
    "        load_state_dict_flexible(model, state_dict, verbose=verbose)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    if (opt is not None) and isinstance(meta, dict) and (\"opt\" in meta):\n",
    "        try:\n",
    "            opt.load_state_dict(meta[\"opt\"])\n",
    "        except Exception as e:\n",
    "            print(\"[ckpt] opt load skipped:\", repr(e))\n",
    "\n",
    "    if (scaler is not None) and isinstance(meta, dict) and (\"scaler\" in meta):\n",
    "        try:\n",
    "            scaler.load_state_dict(meta[\"scaler\"])\n",
    "        except Exception as e:\n",
    "            print(\"[ckpt] scaler load skipped:\", repr(e))\n",
    "\n",
    "    start_epoch = int(meta.get(\"epoch\", -1)) + 1 if isinstance(meta, dict) else 0\n",
    "    best_metric = meta.get(\"best_metric\", None) if isinstance(meta, dict) else None\n",
    "    return start_epoch, best_metric, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5a741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [train]:   0%|          | 0/646 [00:00<?, ?it/s, loss=19, rmsd=None, stage=MASKED_L1(+CONF), tm=None]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[profile] step0 to_gpu: 0.000s  forward+loss: 0.203s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [train]: 100%|██████████| 646/646 [00:43<00:00, 14.79it/s, loss=15.7, rmsd=None, stage=MASKED_L1(+CONF), tm=None]\n",
      "Epoch 1/20 [eval]:   0%|          | 0/72 [00:00<?, ?it/s, loss=16.1, rmsd=19.5, stage=MASKED_L1(+CONF), tm=0.0269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[profile] step0 to_gpu: 0.001s  forward+loss: 0.021s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [eval]: 100%|██████████| 72/72 [00:03<00:00, 23.40it/s, loss=15.6, rmsd=19.2, stage=MASKED_L1(+CONF), tm=0.0287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=15.6574  eval_loss=15.6110  eval_rmsd=19.235278248786926  eval_tm=0.028664885179346636  stage=MASKED_L1(+CONF)\n",
      "💾 stage1 best loss updated: 15.611000 -> checkpoints/best_stage1_v16.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [train]: 100%|██████████| 646/646 [00:43<00:00, 14.91it/s, loss=15.7, rmsd=None, stage=MASKED_L1(+CONF), tm=None]\n",
      "Epoch 2/20 [eval]: 100%|██████████| 72/72 [00:03<00:00, 23.86it/s, loss=15.6, rmsd=19.2, stage=MASKED_L1(+CONF), tm=0.0287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=15.6519  eval_loss=15.6102  eval_rmsd=19.2347647746404  eval_tm=0.028653578483499587  stage=MASKED_L1(+CONF)\n",
      "💾 stage1 best loss updated: 15.610175 -> checkpoints/best_stage1_v16.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [train]:  20%|█▉        | 126/646 [00:13<00:55,  9.45it/s, loss=92.7, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NaN/Inf detected] epoch=2 step=126 train=True  loss=nan  |pred|max=nan  |tgt|max=2.859e+02  mask_sum=963.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Non-finite loss encountered.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m last_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39mckpt_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_v16.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, cfg\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m--> 124\u001b[0m     tr_loss, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     ev_loss, ev_rmsd, ev_tm, stage \u001b[38;5;241m=\u001b[39m run_epoch(hold_loader, epoch, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  eval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  eval_rmsd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev_rmsd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  eval_tm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mev_tm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  stage=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 92\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, epoch, train)\u001b[0m\n\u001b[0;32m     89\u001b[0m     t3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[profile] step0 to_gpu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms  forward+loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt3\u001b[38;5;241m-\u001b[39mt2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m \u001b[43m_fail_if_nonfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m step=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstep\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m train=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m     95\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36m_fail_if_nonfinite\u001b[1;34m(loss, preds, target, mask, where)\u001b[0m\n\u001b[0;32m     44\u001b[0m     msum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(mask\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[NaN/Inf detected] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  |pred|max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpmax\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  |tgt|max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmax\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  mask_sum=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsum\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-finite loss encountered.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Non-finite loss encountered."
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6) Train loop + checkpoint/resume  [v16]\n",
    "#   - AMP 토글(cfg.amp)\n",
    "#   - loss FP32(cfg.loss_fp32)\n",
    "#   - NaN/Inf 감지 시 즉시 중단(cfg.fail_on_nan)\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "model = EGNNv16(cfg).to(device)\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "use_amp = (device.type == \"cuda\") and bool(getattr(cfg, \"amp\", True))\n",
    "scaler = torch.amp.GradScaler(device.type, enabled=use_amp)\n",
    "\n",
    "# ----- (optional) resume -----\n",
    "start_epoch = 0\n",
    "best_stage1 = float(\"inf\")\n",
    "best_tm = -1e9\n",
    "stale = 0\n",
    "stage2_started = False\n",
    "\n",
    "if isinstance(getattr(cfg, \"resume_path\", \"\"), str) and cfg.resume_path and os.path.exists(cfg.resume_path):\n",
    "    print(\"[resume] loading:\", cfg.resume_path)\n",
    "    start_epoch, best_metric, meta = load_checkpoint(\n",
    "        cfg.resume_path, model, opt=opt, scaler=scaler,\n",
    "        map_location=device, flexible=True, verbose=True\n",
    "    )\n",
    "    if best_metric is not None:\n",
    "        best_tm = float(best_metric)\n",
    "    if isinstance(meta, dict) and isinstance(meta.get(\"extra\", None), dict):\n",
    "        if \"best_stage1\" in meta[\"extra\"]:\n",
    "            best_stage1 = float(meta[\"extra\"][\"best_stage1\"])\n",
    "    print(f\"[resume] start_epoch={start_epoch}  best_stage1={best_stage1:.6f}  best_tm={best_tm:.4f}\")\n",
    "\n",
    "def _fail_if_nonfinite(loss, preds, target, mask, where: str = \"\"):\n",
    "    if not bool(getattr(cfg, \"fail_on_nan\", True)):\n",
    "        return\n",
    "    if (loss is None) or (not torch.isfinite(loss).all()):\n",
    "        # 최소 정보만 출력 (원인 추적용)\n",
    "        with torch.no_grad():\n",
    "            pmax = float(preds.abs().max().item()) if preds is not None else float(\"nan\")\n",
    "            tmax = float(target.abs().max().item()) if target is not None else float(\"nan\")\n",
    "            msum = float(mask.sum().item()) if mask is not None else float(\"nan\")\n",
    "        print(f\"\\n[NaN/Inf detected] {where}  loss={loss}  |pred|max={pmax:.3e}  |tgt|max={tmax:.3e}  mask_sum={msum:.1f}\")\n",
    "        raise RuntimeError(\"Non-finite loss encountered.\")\n",
    "\n",
    "def run_epoch(loader, epoch: int, train: bool):\n",
    "    model.train(train)\n",
    "    total_loss, n = 0.0, 0\n",
    "    total_rmsd, n_r = 0.0, 0\n",
    "    total_tm, n_t = 0.0, 0\n",
    "    stage_name = None\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs} [{'train' if train else 'eval'}]\", mininterval=0.5)\n",
    "    for step, (tokens, target, mask) in enumerate(pbar):\n",
    "        do_prof = bool(getattr(cfg, \"profile_first_batch\", False)) and (epoch == start_epoch) and (step == 0)\n",
    "\n",
    "        if do_prof:\n",
    "            t0 = time.time()\n",
    "\n",
    "        tokens = tokens.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        mask   = mask.to(device, non_blocking=True)\n",
    "\n",
    "        if do_prof and device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "\n",
    "        fb = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "        if fb is None:\n",
    "            continue\n",
    "        tokens, target, mask = fb\n",
    "        pad_mask = (tokens != 0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            if do_prof:\n",
    "                t2 = time.time()\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "                preds, conf_logits = model(tokens, pad_mask)\n",
    "                loss, stage_name = loss_fn(preds, target, mask, epoch, step=step, conf_logits=conf_logits)\n",
    "\n",
    "            if do_prof and device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "                t3 = time.time()\n",
    "                print(f\"[profile] step0 to_gpu: {t1-t0:.3f}s  forward+loss: {t3-t2:.3f}s\")\n",
    "\n",
    "            _fail_if_nonfinite(loss, preds, target, mask, where=f\"epoch={epoch} step={step} train={train}\")\n",
    "\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.grad_clip))\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()); n += 1\n",
    "\n",
    "        if not train:\n",
    "            rmsd = kabsch_rmsd_metric_min(preds, target, mask)\n",
    "            tm   = tm_score_metric_maxK(preds, target, mask)\n",
    "            total_rmsd += float(rmsd.item()); n_r += 1\n",
    "            total_tm   += float(tm.item());   n_t += 1\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            loss=total_loss/max(n,1),\n",
    "            stage=stage_name,\n",
    "            rmsd=(total_rmsd/max(n_r,1) if n_r>0 else None),\n",
    "            tm=(total_tm/max(n_t,1) if n_t>0 else None),\n",
    "        )\n",
    "\n",
    "    avg_loss = total_loss/max(n,1)\n",
    "    avg_rmsd = (total_rmsd/max(n_r,1) if n_r>0 else None)\n",
    "    avg_tm   = (total_tm/max(n_t,1) if n_t>0 else None)\n",
    "    return avg_loss, avg_rmsd, avg_tm, stage_name\n",
    "\n",
    "last_path = os.path.join(cfg.ckpt_dir, \"last_v16.pt\")\n",
    "\n",
    "for epoch in range(start_epoch, cfg.epochs):\n",
    "    tr_loss, _, _, _ = run_epoch(train_loader, epoch, train=True)\n",
    "    ev_loss, ev_rmsd, ev_tm, stage = run_epoch(hold_loader, epoch, train=False)\n",
    "    print(f\"[Epoch {epoch+1}] train_loss={tr_loss:.4f}  eval_loss={ev_loss:.4f}  eval_rmsd={ev_rmsd}  eval_tm={ev_tm}  stage={stage}\")\n",
    "\n",
    "    # --- always save last (resume용) ---\n",
    "    save_checkpoint(\n",
    "        last_path, model, opt, scaler, epoch,\n",
    "        best_metric=best_tm,\n",
    "        cfg_obj=cfg,\n",
    "        extra={\"best_stage1\": best_stage1},\n",
    "    )\n",
    "\n",
    "    # --- Stage1: best val loss ---\n",
    "    if epoch < cfg.warmup_epochs:\n",
    "        if ev_loss < best_stage1 - 1e-4:\n",
    "            best_stage1 = ev_loss\n",
    "            save_checkpoint(cfg.ckpt_stage1_path, model, opt, scaler, epoch,\n",
    "                            best_metric=best_stage1, cfg_obj=cfg)\n",
    "            print(f\"💾 stage1 best loss updated: {best_stage1:.6f} -> {cfg.ckpt_stage1_path}\")\n",
    "        continue\n",
    "\n",
    "    # --- Stage2 starts: reset patience tracking once ---\n",
    "    if not stage2_started:\n",
    "        stage2_started = True\n",
    "        best_tm = -1e9\n",
    "        stale = 0\n",
    "        print(\"🔁 Stage2 started: reset best_tm/stale\")\n",
    "\n",
    "    # --- Stage2: maximize TM-score ---\n",
    "    cur_tm = float(ev_tm) if ev_tm is not None else -1e9\n",
    "    if cur_tm > best_tm + 1e-4:\n",
    "        best_tm = cur_tm\n",
    "        stale = 0\n",
    "        save_checkpoint(cfg.ckpt_best_path, model, opt, scaler, epoch,\n",
    "                        best_metric=best_tm, cfg_obj=cfg)\n",
    "        print(f\"✅ best TM updated: {best_tm:.4f} -> {cfg.ckpt_best_path}\")\n",
    "    else:\n",
    "        stale += 1\n",
    "        print(f\"⏸ no TM improvement: {stale}/{cfg.patience}\")\n",
    "        if stale >= int(cfg.patience):\n",
    "            print(\"🛑 early stopping (Stage2, TM-based).\")\n",
    "            break\n",
    "\n",
    "# ---- load best for inference ----\n",
    "if os.path.exists(cfg.ckpt_best_path):\n",
    "    load_checkpoint(cfg.ckpt_best_path, model, opt=None, scaler=None, map_location=device, flexible=False, verbose=True)\n",
    "    print(\"Best model loaded:\", cfg.ckpt_best_path, \"best_tm=\", best_tm)\n",
    "elif os.path.exists(cfg.ckpt_stage1_path):\n",
    "    load_checkpoint(cfg.ckpt_stage1_path, model, opt=None, scaler=None, map_location=device, flexible=False, verbose=True)\n",
    "    print(\"Stage1 best model loaded:\", cfg.ckpt_stage1_path, \"best_loss=\", best_stage1)\n",
    "else:\n",
    "    print(\"No checkpoint found (best/stage1). Using current model in memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e3366",
   "metadata": {},
   "source": [
    "## 7) Sanity check 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31348ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7) Quick sanity check plot (xy/xz/yz)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens, target, mask = next(iter(hold_loader))\n",
    "tokens = tokens.to(device); target = target.to(device); mask = mask.to(device)\n",
    "tokens, target, mask = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "pad_mask = (tokens != 0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds, conf_logits = model(tokens, pad_mask)  # (B,K,T,3)\n",
    "    # pick best head by TM-score for sample 0\n",
    "    tm_per_head = []\n",
    "    for k in range(cfg.num_preds):\n",
    "        tm = tm_score_single(preds[0:1, k], target[0:1], mask[0:1])\n",
    "        tm_per_head.append(float(tm.item()))\n",
    "    best_k = int(np.argmax(tm_per_head))\n",
    "    pred0 = preds[0, best_k].detach()\n",
    "    tgt0  = target[0].detach()\n",
    "    m0    = mask[0].detach().bool()\n",
    "\n",
    "    pred0a = kabsch_align(pred0.unsqueeze(0), tgt0.unsqueeze(0), mask[0:1]).squeeze(0)\n",
    "\n",
    "def scat(a, b, title, ax):\n",
    "    ax.scatter(a[:,0].cpu(), a[:,1].cpu(), s=10, label=\"target\")\n",
    "    ax.scatter(b[:,0].cpu(), b[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "A = tgt0[m0]\n",
    "B = pred0a[m0]\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "axes[0].scatter(A[:,0].cpu(), A[:,1].cpu(), s=10, label=\"target\")\n",
    "axes[0].scatter(B[:,0].cpu(), B[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[0].set_title(\"x-y\"); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(A[:,0].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[1].scatter(B[:,0].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[1].set_title(\"x-z\"); axes[1].legend()\n",
    "\n",
    "axes[2].scatter(A[:,1].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[2].scatter(B[:,1].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[2].set_title(\"y-z\"); axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"best head:\", best_k, \"tm per head:\", tm_per_head)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
