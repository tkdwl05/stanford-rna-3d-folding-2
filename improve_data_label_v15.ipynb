{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5ce781",
   "metadata": {},
   "source": [
    "# improve_data_label_v15\n",
    "\n",
    "v15: v14(로더/체크포인트 안정화) + **좌표 초기화/출력 경로 수정**으로 예측 붕괴(선분/클러스터) 문제를 해결한 버전.\n",
    "\n",
    "- 핵심 수정: EGNN 내부 좌표 `x`를 0으로 시작하지 않고, **학습 가능한 init_x(h)** 로 초기화 → 좌표 업데이트가 실제로 동작\n",
    "- 출력: `preds = x + head_offset(h)` 구조로 v12 스타일 복원 (멀티헤드 유지)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16412a32",
   "metadata": {},
   "source": [
    "## v15 변경점 (v14 대비 예측 붕괴/좌표 업데이트 무력화 해결)\n",
    "\n",
    "1) **좌표 초기화(degenerate zero-init) 제거**\n",
    "- v13/v14는 `x=0`에서 시작해 `dx = (xi-xj)*w` 이라서 **항상 dx=0 → x가 영원히 0** 문제가 생길 수 있음\n",
    "- v15는 `x = init_x(h)` 로 시작해 **rij가 0이 아니게 만들고**, EGNN의 좌표 업데이트가 실제로 동작\n",
    "\n",
    "2) **출력 경로 수정**\n",
    "- `preds = x + offset_k(h)` 형태로, EGNN이 만든 기하 정보를 출력에 직접 반영\n",
    "\n",
    "3) 나머지(로더/체크포인트/학습루프)는 v14 그대로\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 0) Imports, Device, Config  [v14]\n",
    "# ==========================================\n",
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "\n",
    "    # data\n",
    "    max_len: int = 256\n",
    "    min_valid: int = 10\n",
    "    batch: int = 8\n",
    "    num_workers: int = 0          # ✅ v14: 안전 기본값 (Windows/Jupyter에서 hang 방지)\n",
    "    loader_timeout: int = 60      # ✅ num_workers>0 일 때만 사용\n",
    "\n",
    "    # model\n",
    "    vocab: int = 5  # PAD=0, A=1,C=2,G=3,U=4\n",
    "    d_model: int = 192\n",
    "    d_edge: int = 256\n",
    "    n_layers: int = 8\n",
    "    k_nn: int = 12  # sequence neighborhood edges\n",
    "    num_preds: int = 4  # K heads\n",
    "    dropout: float = 0.1\n",
    "    init_x_scale: float = 1.0   # v15: init_x(h) 스케일\n",
    "    offset_scale: float = 0.1   # v15: head offset 스케일(초기 안정화)\n",
    "\n",
    "    # base-pair feature\n",
    "    bp_tau: float = 40.0         # distance decay for |i-j|\n",
    "    bp_min_sep: int = 4          # do not pair too-close residues\n",
    "    pair_alpha: float = 2.0      # message/coord weight boost: (1 + pair_alpha * p_ij)\n",
    "\n",
    "    # optimization\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 0.02\n",
    "    epochs: int = 20\n",
    "    warmup_epochs: int = 2\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # softmin aggregation\n",
    "    softmin_temp: float = 1.0\n",
    "\n",
    "    # losses\n",
    "    use_confidence: bool = True\n",
    "    conf_w: float = 0.05\n",
    "\n",
    "    dist_w: float = 0.05         # weak distance auxiliary (as requested)\n",
    "    pair_num_pairs: int = 512    # sampled pairs for distance/repulsion (speed-tuned)\n",
    "    aux_every: int = 2           # compute expensive aux losses every N steps\n",
    "    local_w: float = 0.2\n",
    "    var_w: float = 0.02\n",
    "    repulse_w: float = 0.02\n",
    "    diversity_w: float = 0.01\n",
    "    repulse_margin: float = 2.5\n",
    "    diversity_margin: float = 2.0\n",
    "\n",
    "    # checkpoint / resume\n",
    "    ckpt_dir: str = \"checkpoints\"\n",
    "    ckpt_best_path: str = \"checkpoints/best_structured_v14.pt\"\n",
    "    ckpt_stage1_path: str = \"checkpoints/best_stage1_v14.pt\"\n",
    "    resume_path: str = \"\"          # 예) \"checkpoints/last.pt\" 또는 v12/v13 state_dict 파일\n",
    "    patience: int = 10\n",
    "\n",
    "    # debug\n",
    "    profile_first_batch: bool = True  # 첫 배치에서 to_gpu/forward 타이밍 출력\n",
    "\n",
    "cfg = CFG()\n",
    "os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "# (선택) matmul 정밀도 힌트 (Ampere+에서 유효)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73414564",
   "metadata": {},
   "source": [
    "## 1) Dataset / Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2e4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwl\\AppData\\Local\\Temp\\ipykernel_24220\\799196925.py:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(\"train_labels.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq: (5716, 8) val_seq: (28, 8)\n",
      "train_labels: (7794971, 8) val_labels: (9762, 126)\n",
      "  target_id                                           sequence\n",
      "0      4TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "1      6TNA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "2      1TRA  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "3      1TN2  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "4      1TN1  GCGGAUUUAGCUCAGUUGGGAGAGCGCCAGACUGAAGAUCUGGAGG...\n",
      "train_coords: (5716, 3)\n",
      "val_coords  : (28, 3)\n",
      "train_coords NaN/Inf: 0\n",
      "val_coords   NaN/Inf: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1) Load CSVs\n",
    "# ==========================================\n",
    "train_seq = pd.read_csv(\"train_sequences.csv\")\n",
    "val_seq   = pd.read_csv(\"validation_sequences.csv\")\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "val_labels   = pd.read_csv(\"validation_labels.csv\")\n",
    "\n",
    "print(\"train_seq:\", train_seq.shape, \"val_seq:\", val_seq.shape)\n",
    "print(\"train_labels:\", train_labels.shape, \"val_labels:\", val_labels.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 2) Tokenize sequences (A,C,G,U -> 1..4, PAD=0)\n",
    "# ==========================================\n",
    "mapping = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n",
    "\n",
    "def tokenize_sequence(seq: str):\n",
    "    # unknown -> 0 (PAD)\n",
    "    return [mapping.get(ch, 0) for ch in seq]\n",
    "\n",
    "train_seq['tokens'] = train_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "val_seq['tokens']   = val_seq['sequence'].astype(str).apply(tokenize_sequence)\n",
    "\n",
    "print(train_seq[['target_id','sequence']].head())\n",
    "# ==========================================\n",
    "# 3) Build coordinates + coord_mask from labels\n",
    "# ==========================================\n",
    "XYZ = ['x_1','y_1','z_1']\n",
    "THRESH = 1e17\n",
    "\n",
    "def _make_target_id_and_resid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['target_id'] = df['ID'].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    df['resid'] = pd.to_numeric(df['resid'], errors='coerce')\n",
    "    df = df.dropna(subset=['resid']).copy()\n",
    "    df['resid'] = df['resid'].astype(int)\n",
    "    df = df.sort_values(['target_id','resid'])\n",
    "    return df\n",
    "\n",
    "def build_coords_from_train_labels(train_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(train_labels)\n",
    "    for c in XYZ:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    arr = df[XYZ].to_numpy(dtype=np.float64)\n",
    "    ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "    df['coord_ok'] = ok.astype(np.float32)\n",
    "    df.loc[~ok, XYZ] = 0.0\n",
    "    df[XYZ] = df[XYZ].astype(np.float32)\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "def build_coords_from_val_labels(val_labels: pd.DataFrame, K: int = 40) -> pd.DataFrame:\n",
    "    df = _make_target_id_and_resid(val_labels)\n",
    "\n",
    "    chosen = np.zeros((len(df), 3), dtype=np.float32)\n",
    "    ok_mask = np.zeros((len(df),), dtype=np.float32)\n",
    "    filled = np.zeros((len(df),), dtype=bool)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        cols = [f'x_{k}', f'y_{k}', f'z_{k}']\n",
    "        if not all(c in df.columns for c in cols):\n",
    "            continue\n",
    "        tmp = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        arr = tmp.to_numpy(dtype=np.float64)\n",
    "        ok = np.isfinite(arr).all(axis=1) & (np.abs(arr) < THRESH).all(axis=1)\n",
    "        take = ok & (~filled)\n",
    "        if take.any():\n",
    "            chosen[take] = arr[take].astype(np.float32)\n",
    "            ok_mask[take] = 1.0\n",
    "            filled[take] = True\n",
    "\n",
    "    df['x_1'] = chosen[:,0]\n",
    "    df['y_1'] = chosen[:,1]\n",
    "    df['z_1'] = chosen[:,2]\n",
    "    df['coord_ok'] = ok_mask\n",
    "\n",
    "    coords_df = (df.groupby('target_id')[XYZ]\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coordinates'))\n",
    "    mask_df   = (df.groupby('target_id')['coord_ok']\n",
    "                 .apply(lambda x: x.to_numpy(np.float32).tolist())\n",
    "                 .reset_index(name='coord_mask'))\n",
    "    return coords_df.merge(mask_df, on='target_id', how='inner')\n",
    "\n",
    "train_coords = build_coords_from_train_labels(train_labels)\n",
    "val_coords   = build_coords_from_val_labels(val_labels, K=40)\n",
    "\n",
    "train_coords['target_id'] = train_coords['target_id'].astype(str).str.strip()\n",
    "val_coords['target_id']   = val_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "print(\"train_coords:\", train_coords.shape)\n",
    "print(\"val_coords  :\", val_coords.shape)\n",
    "\n",
    "def has_nan_inf(coords):\n",
    "    a = np.asarray(coords, dtype=np.float32)\n",
    "    return (not np.isfinite(a).all())\n",
    "\n",
    "print(\"train_coords NaN/Inf:\", train_coords['coordinates'].apply(has_nan_inf).sum())\n",
    "print(\"val_coords   NaN/Inf:\", val_coords['coordinates'].apply(has_nan_inf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b18db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_df: (5739, 12)\n",
      "  target_id  n_valid\n",
      "0      4TNA     76.0\n",
      "1      6TNA     76.0\n",
      "2      1TRA     76.0\n",
      "3      1TN2     76.0\n",
      "4      1TN1     76.0\n",
      "train batches: 646 hold batches: 72\n",
      "[loader precheck] first batch OK in 0.10s\n",
      "[loader precheck] batch shapes: [torch.Size([8, 256]), torch.Size([8, 256, 3]), torch.Size([8, 256])]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4) Build unified dataframe: sequences + coords\n",
    "# ==========================================\n",
    "all_seq = pd.concat([train_seq, val_seq], ignore_index=True)\n",
    "all_coords = pd.concat([train_coords, val_coords], ignore_index=True)\n",
    "\n",
    "all_seq['target_id'] = all_seq['target_id'].astype(str).str.strip()\n",
    "all_coords['target_id'] = all_coords['target_id'].astype(str).str.strip()\n",
    "\n",
    "all_df = all_seq.merge(all_coords, on='target_id', how='inner')\n",
    "\n",
    "# keep only rows that have at least a few valid coordinates\n",
    "def count_valid(m): \n",
    "    m = np.asarray(m, dtype=np.float32)\n",
    "    return float(m.sum())\n",
    "\n",
    "all_df['n_valid'] = all_df['coord_mask'].apply(count_valid)\n",
    "all_df = all_df[all_df['n_valid'] >= cfg.min_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"all_df:\", all_df.shape)\n",
    "print(all_df[['target_id','n_valid']].head())\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5) Dataset (centering only; NO target-based RMS scaling)\n",
    "# ==========================================\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, tokens_list, coords_list, mask_list, max_len=256, center_only=True):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.coords_list = coords_list\n",
    "        self.mask_list = mask_list\n",
    "        self.max_len = max_len\n",
    "        self.center_only = center_only\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 직접 참조 (복사 최소화)\n",
    "        tokens = self.tokens_list[idx]\n",
    "        coords = self.coords_list[idx]\n",
    "        mask   = self.mask_list[idx]\n",
    "\n",
    "        # numpy 변환 (필요 시 1회만)\n",
    "        if not isinstance(tokens, np.ndarray):\n",
    "            tokens = np.array(tokens, dtype=np.int64)\n",
    "        if not isinstance(coords, np.ndarray):\n",
    "            coords = np.array(coords, dtype=np.float32)\n",
    "        if not isinstance(mask, np.ndarray):\n",
    "            mask = np.array(mask, dtype=np.float32)\n",
    "\n",
    "        L = min(len(tokens), len(coords), len(mask), self.max_len)\n",
    "\n",
    "        tokens = tokens[:L]\n",
    "        coords = coords[:L]\n",
    "        mask   = mask[:L]\n",
    "\n",
    "        if self.center_only:\n",
    "            valid = mask.astype(bool)\n",
    "            if valid.any():\n",
    "                coords = coords - coords[valid].mean(axis=0, keepdims=True)\n",
    "\n",
    "        # padding (vectorized, 최소 연산)\n",
    "        tokens_p = np.zeros(self.max_len, dtype=np.int64)\n",
    "        coords_p = np.zeros((self.max_len, 3), dtype=np.float32)\n",
    "        mask_p   = np.zeros(self.max_len, dtype=np.float32)\n",
    "\n",
    "        tokens_p[:L] = tokens\n",
    "        coords_p[:L] = coords\n",
    "        mask_p[:L]   = mask\n",
    "\n",
    "        # torch.from_numpy (복사 없음 → 매우 빠름)\n",
    "        return (\n",
    "            torch.from_numpy(tokens_p),\n",
    "            torch.from_numpy(coords_p),\n",
    "            torch.from_numpy(mask_p),\n",
    "        )\n",
    "\n",
    "def filter_batch(tokens, target, mask, min_valid):\n",
    "    # drop samples with too few valid coords (training stability)\n",
    "    keep = (mask.sum(dim=1) >= min_valid)\n",
    "    if keep.sum() < 2:\n",
    "        return None\n",
    "    return tokens[keep], target[keep], mask[keep]\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6) Train/Holdout split + DataLoader  [v13.2 speed]\n",
    "# ==========================================\n",
    "idx_train, idx_hold = train_test_split(range(len(all_df)), test_size=0.1, random_state=cfg.seed)\n",
    "\n",
    "train_df = all_df.iloc[idx_train].reset_index(drop=True)\n",
    "hold_df  = all_df.iloc[idx_hold].reset_index(drop=True)\n",
    "\n",
    "train_ds = RNADataset(train_df['tokens'].tolist(),\n",
    "                      train_df['coordinates'].tolist(),\n",
    "                      train_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "hold_ds  = RNADataset(hold_df['tokens'].tolist(),\n",
    "                      hold_df['coordinates'].tolist(),\n",
    "                      hold_df['coord_mask'].tolist(),\n",
    "                      max_len=cfg.max_len,\n",
    "                      center_only=True)\n",
    "\n",
    "dl_num_workers = int(getattr(cfg, \"num_workers\", 0) or 0)\n",
    "\n",
    "# NOTE (Windows/Jupyter): num_workers>0가 멈춘 것처럼 보이는 경우가 많습니다.\n",
    "# v14 기본값은 0이며, 속도가 필요하면 2→4로 천천히 올려가며 확인하세요.\n",
    "if os.name == \"nt\":\n",
    "    dl_num_workers = 0\n",
    "\n",
    "# ✅ 중요: PyTorch는 num_workers==0 인데 timeout>0이면 AssertionError가 날 수 있습니다.\n",
    "timeout = int(getattr(cfg, \"loader_timeout\", 0) or 0) if (dl_num_workers > 0) else 0\n",
    "\n",
    "dl_kwargs = dict(\n",
    "    num_workers=dl_num_workers,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "if dl_num_workers > 0:\n",
    "    dl_kwargs.update(dict(persistent_workers=True, prefetch_factor=2))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch,\n",
    "    shuffle=True,\n",
    "    timeout=timeout,\n",
    "    **dl_kwargs\n",
    ")\n",
    "hold_loader = DataLoader(\n",
    "    hold_ds,\n",
    "    batch_size=cfg.batch,\n",
    "    shuffle=False,\n",
    "    timeout=timeout,\n",
    "    **dl_kwargs\n",
    ")\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"hold batches:\", len(hold_loader))\n",
    "\n",
    "# ---- First-batch sanity check (detect loader hangs / preprocessing stalls) ----\n",
    "import time as _time\n",
    "_t0 = _time.time()\n",
    "try:\n",
    "    _it = iter(train_loader)\n",
    "    _b = next(_it)\n",
    "    print(f\"[loader precheck] first batch OK in {_time.time()-_t0:.2f}s\")\n",
    "    # print shapes\n",
    "    if isinstance(_b, (tuple, list)):\n",
    "        print(\"[loader precheck] batch shapes:\", [getattr(x, \"shape\", type(x)) for x in _b])\n",
    "except Exception as e:\n",
    "    print(\"[loader precheck] FAILED:\", repr(e))\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4df894",
   "metadata": {},
   "source": [
    "## 2) Base-pair feature 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02c55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2) Base-pair probability encoder (sequence-only heuristic)\n",
    "#   - 입력 tokens(B,T) -> p(B,T,T) in [0,1]\n",
    "#   - canonical: A-U, C-G, G-U wobble\n",
    "#   - 거리 prior: exp(-|i-j|/tau), and |i-j|>=bp_min_sep\n",
    "# ==========================================\n",
    "class BasePairEncoder(nn.Module):\n",
    "    def __init__(self, tau: float = 40.0, min_sep: int = 4):\n",
    "        super().__init__()\n",
    "        self.tau = float(tau)\n",
    "        self.min_sep = int(min_sep)\n",
    "\n",
    "        # 0 PAD, 1 A,2 C,3 G,4 U\n",
    "        # canonical probs\n",
    "        P = torch.zeros((5,5), dtype=torch.float32)\n",
    "        P[1,4] = 1.0; P[4,1] = 1.0  # A-U\n",
    "        P[2,3] = 1.0; P[3,2] = 1.0  # C-G\n",
    "        P[3,4] = 0.6; P[4,3] = 0.6  # G-U wobble (weaker)\n",
    "        self.register_buffer(\"pair_table\", P, persistent=False)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, pad_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        tokens: (B,T) int64\n",
    "        pad_mask: (B,T) True for valid nodes (optional)\n",
    "        returns p: (B,T,T) float32\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        t_i = tokens[:, :, None]  # (B,T,1)\n",
    "        t_j = tokens[:, None, :]  # (B,1,T)\n",
    "\n",
    "        base = self.pair_table[t_i, t_j]  # (B,T,T)\n",
    "\n",
    "        # 거리 prior\n",
    "        idx = torch.arange(T, device=tokens.device)\n",
    "        dist = (idx[None, :, None] - idx[None, None, :]).abs().float()  # (1,T,T)\n",
    "        sep_ok = (dist >= float(self.min_sep)).float()\n",
    "        prior = torch.exp(-dist / max(self.tau, 1e-6)) * sep_ok\n",
    "\n",
    "        p = base * prior  # (B,T,T)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            m = pad_mask.float()\n",
    "            p = p * (m[:, :, None] * m[:, None, :])\n",
    "\n",
    "        # zero diagonal\n",
    "        p = p * (1.0 - torch.eye(T, device=p.device, dtype=p.dtype)[None, :, :])\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4f945",
   "metadata": {},
   "source": [
    "## 3) Loss helpers (Kabsch + sampled distance + local/var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3eab1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmin_weights(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> weights (B,K)\"\"\"\n",
    "    return torch.softmax(-losses / max(float(temperature), 1e-8), dim=1)\n",
    "\n",
    "def softmin_aggregate(losses: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"losses: (B,K) -> scalar\"\"\"\n",
    "    w = softmin_weights(losses, temperature)\n",
    "    return (w * losses).sum(dim=1).mean()\n",
    "\n",
    "def masked_l1_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Return per-head masked L1: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    out = []\n",
    "    denom = m.sum(dim=(1,2)).clamp_min(1.0)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        diff = (pk - target).abs() * m\n",
    "        l1 = diff.sum(dim=(1,2)) / denom\n",
    "        out.append(l1)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_align(P: torch.Tensor, Q: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"Align P to Q using Kabsch. SVD always runs in FP32.\"\"\"\n",
    "    with torch.amp.autocast(device_type=('cuda' if P.is_cuda else 'cpu'), enabled=False):\n",
    "        P32 = P.float()\n",
    "        Q32 = Q.float()\n",
    "        m32 = mask.float().unsqueeze(-1)  # (B,T,1)\n",
    "\n",
    "        msum = m32.sum(dim=1, keepdim=True).clamp_min(eps)\n",
    "        P_mean = (P32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "        Q_mean = (Q32 * m32).sum(dim=1, keepdim=True) / msum\n",
    "\n",
    "        P_c = (P32 - P_mean) * m32\n",
    "        Q_c = (Q32 - Q_mean) * m32\n",
    "\n",
    "        H = torch.matmul(P_c.transpose(1, 2), Q_c).contiguous()  # (B,3,3)\n",
    "        U, S, Vh = torch.linalg.svd(H, full_matrices=False)\n",
    "        V = Vh.transpose(1, 2)\n",
    "\n",
    "        det = torch.det(torch.matmul(V, U.transpose(1, 2)))\n",
    "        sign = torch.where(det < 0, -torch.ones_like(det), torch.ones_like(det))\n",
    "\n",
    "        E = torch.eye(3, device=H.device, dtype=H.dtype).unsqueeze(0).repeat(H.shape[0], 1, 1)\n",
    "        E[:, 2, 2] = sign\n",
    "        R = torch.matmul(torch.matmul(V, E), U.transpose(1, 2))  # (B,3,3)\n",
    "\n",
    "        P_aligned = torch.matmul(P_c, R.transpose(1, 2)) + Q_mean\n",
    "        P_aligned = P_aligned * m32\n",
    "\n",
    "    return P_aligned.to(dtype=P.dtype)\n",
    "\n",
    "def kabsch_rmsd_losses(preds, target, mask) -> torch.Tensor:\n",
    "    \"\"\"Per-head RMSD after Kabsch alignment: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pk_aligned = kabsch_align(pk, target, mask)\n",
    "        diff_sq = (pk_aligned - target) ** 2\n",
    "        sum_sq = (diff_sq * mask.unsqueeze(-1)).sum(dim=(1, 2))  # (B,)\n",
    "        n_valid = (mask.sum(dim=1) * 3).clamp_min(1.0)\n",
    "        rmsd = torch.sqrt(sum_sq / n_valid + 1e-8)  # (B,)\n",
    "        out.append(rmsd)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def pairwise_distance_losses_sampled(preds, target, mask, num_pairs: int = 2048) -> torch.Tensor:\n",
    "    \"\"\"Per-head sampled pairwise distance MSE: (B,K)\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses_bk = torch.zeros((B, K), device=device_, dtype=preds.dtype)\n",
    "\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 2:\n",
    "            continue\n",
    "\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        tgt_d = (target[b, i] - target[b, j]).norm(dim=-1)  # (num_pairs,)\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            pred_d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            losses_bk[b, k] = ((pred_d - tgt_d) ** 2).mean()\n",
    "\n",
    "    return losses_bk\n",
    "\n",
    "def coord_variance_losses(preds: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head masked coordinate variance (B,K). Larger is better (anti-collapse).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)  # (B,1,1)\n",
    "    out = []\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]  # (B,T,3)\n",
    "        mean = (pk * m).sum(dim=1, keepdim=True) / denom\n",
    "        var = ((pk - mean) ** 2 * m).sum(dim=(1,2)) / denom.squeeze(1).squeeze(1).clamp_min(1.0)  # (B,)\n",
    "        out.append(var)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def local_bond_losses(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Per-head adjacent (i,i+1) bond length MSE: (B,K).\"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m_adj = (mask[:, 1:] * mask[:, :-1]).bool()  # (B,T-1)\n",
    "    tgt = (target[:, 1:] - target[:, :-1]).norm(dim=-1)  # (B,T-1)\n",
    "    out = []\n",
    "    denom = m_adj.sum(dim=1).clamp_min(1)  # (B,)\n",
    "    for k in range(K):\n",
    "        pk = preds[:, k]\n",
    "        pd = (pk[:, 1:] - pk[:, :-1]).norm(dim=-1)\n",
    "        diff = (pd - tgt) ** 2\n",
    "        l = (diff * m_adj).sum(dim=1) / denom\n",
    "        out.append(l)\n",
    "    return torch.stack(out, dim=1)\n",
    "\n",
    "def kabsch_rmsd_metric_min(preds, target, mask) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)  # (B,K)\n",
    "        return rmsd_bk.min(dim=1).values.mean()\n",
    "\n",
    "# ------------------------------------------\n",
    "# TM-score metric (competition-aligned: best-of-K, index-matched residues)\n",
    "# d0 follows the RNA Folding Kaggle metric (Ribonanza TM-score) piecewise form.\n",
    "# ------------------------------------------\n",
    "def _tm_d0(L: int) -> float:\n",
    "    if L < 12: return 0.3\n",
    "    if L < 15: return 0.4\n",
    "    if L < 18: return 0.5\n",
    "    if L < 21: return 0.6\n",
    "    if L < 30: return 0.7\n",
    "    return max(0.6 * math.sqrt(L - 0.5) - 2.5, 0.5)\n",
    "\n",
    "def tm_score_single(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    pred_aligned = kabsch_align(pred, target, mask)\n",
    "    d = (pred_aligned - target).norm(dim=-1)  # (B,T)\n",
    "    m = mask.bool()\n",
    "    tm_list = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        idx = m[b]\n",
    "        L = int(idx.sum().item())\n",
    "        if L <= 0:\n",
    "            tm_list.append(pred.new_tensor(0.0))\n",
    "            continue\n",
    "        d0 = _tm_d0(L)\n",
    "        db = d[b, idx]\n",
    "        tm = (1.0 / (1.0 + (db / d0) ** 2)).mean()\n",
    "        tm_list.append(tm)\n",
    "    return torch.stack(tm_list, dim=0)\n",
    "\n",
    "def tm_score_metric_maxK(preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        B, K, T, _ = preds.shape\n",
    "        tms = []\n",
    "        for k in range(K):\n",
    "            tm_k = tm_score_single(preds[:, k], target, mask)  # (B,)\n",
    "            tms.append(tm_k)\n",
    "        tms = torch.stack(tms, dim=1)  # (B,K)\n",
    "        return tms.max(dim=1).values.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2774e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# v11 추가: non-neighbor repulsion + head diversity\n",
    "# ==========================================\n",
    "def repulsion_losses_sampled(preds: torch.Tensor, mask: torch.Tensor,\n",
    "                             num_pairs: int = 2048, margin: float = 2.5) -> torch.Tensor:\n",
    "    \"\"\"겹침 방지용 hinge loss. 인접(i,i+1)은 제외하고 랜덤 pair에 대해\n",
    "    d < margin 이면 (margin-d)^2 를 부과.\n",
    "    returns (B,K)\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    device_ = preds.device\n",
    "    losses = preds.new_zeros((B, K))\n",
    "    valid_indices = [torch.where(mask[b].bool())[0] for b in range(B)]\n",
    "\n",
    "    for b in range(B):\n",
    "        idx = valid_indices[b]\n",
    "        n = idx.numel()\n",
    "        if n < 3:\n",
    "            continue\n",
    "\n",
    "        # sample pairs\n",
    "        i = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "        j = idx[torch.randint(0, n, (num_pairs,), device=device_)]\n",
    "\n",
    "        # exclude neighbors\n",
    "        nn_mask = (torch.abs(i - j) > 1)\n",
    "        if nn_mask.sum() < 8:\n",
    "            continue\n",
    "        i = i[nn_mask]\n",
    "        j = j[nn_mask]\n",
    "\n",
    "        for k in range(K):\n",
    "            pk = preds[b, k]\n",
    "            d = (pk[i] - pk[j]).norm(dim=-1)\n",
    "            hinge = (margin - d).clamp_min(0.0)\n",
    "            losses[b, k] = (hinge * hinge).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "def head_diversity_losses(preds: torch.Tensor, mask: torch.Tensor, margin: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"헤드 간 유사하면 패널티. (B,K)로 반환해서 기존 softmin 프레임에 맞춘다.\n",
    "    각 헤드의 masked centered coords를 만들고, 헤드쌍 RMSD가 margin보다 작으면 hinge.\n",
    "    \"\"\"\n",
    "    B, K, T, _ = preds.shape\n",
    "    m = mask.unsqueeze(-1)  # (B,T,1)\n",
    "    denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "    # centered\n",
    "    centered = preds - (preds * m.unsqueeze(1)).sum(dim=2, keepdim=True) / denom.unsqueeze(1)\n",
    "\n",
    "    # pairwise head RMSD (no rotation; diversity 목적이라 단순 RMSD)\n",
    "    out = preds.new_zeros((B, K))\n",
    "    if K < 2:\n",
    "        return out\n",
    "\n",
    "    for a in range(K):\n",
    "        pen = 0.0\n",
    "        cnt = 0\n",
    "        for b in range(K):\n",
    "            if a == b: \n",
    "                continue\n",
    "            diff = (centered[:, a] - centered[:, b])**2  # (B,T,3)\n",
    "            rmsd = torch.sqrt((diff * m).sum(dim=(1,2)) / (mask.sum(dim=1).clamp_min(1.0)*3.0) + 1e-8)  # (B,)\n",
    "            hinge = (margin - rmsd).clamp_min(0.0)\n",
    "            pen = pen + hinge*hinge\n",
    "            cnt += 1\n",
    "        out[:, a] = pen / max(cnt, 1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58a6a7",
   "metadata": {},
   "source": [
    "## 4) Model (Pair-aware EGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd83612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4) Model (EGNN backbone + Pair-aware message passing + K coord heads + ConfidenceHead)  [v15]\n",
    "#   - FIX: coordinate path was dead when x was initialized to zeros (rij=0 -> dx=0 forever)\n",
    "#   - v15 uses learnable init_x(h) and outputs coords = x + offset_k(h) (v12-style)\n",
    "# ==========================================\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        return x + self.pe(pos)[None, :, :]\n",
    "\n",
    "def build_seq_edges(T: int, k: int, device):\n",
    "    r = max(1, k // 2)\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(T):\n",
    "        for j in range(max(0, i - r), min(T, i + r + 1)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            src.append(i); dst.append(j)\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long, device=device)  # (2,E)\n",
    "    return edge_index\n",
    "\n",
    "class EGNNPairAwareLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EGNN + pair feature p_ij (base-pair 확률)\n",
    "    message: phi(h_i, h_j, d_ij^2, p_ij)\n",
    "    + long-range boost: (1 + pair_alpha * p_ij) 를 coord/node 업데이트에 곱한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_node: int, d_edge: int, dropout: float, pair_alpha: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.pair_alpha = float(pair_alpha)\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node*2 + 1 + 1, d_edge),  # +p_ij\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.coord_mlp = nn.Sequential(\n",
    "            nn.Linear(d_edge, d_edge),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_edge, 1),\n",
    "        )\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(d_node + d_edge, d_node),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_node, d_node),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(d_node)\n",
    "\n",
    "    def forward(self, h, x, edge_index, node_mask, pair_p):\n",
    "        \"\"\"\n",
    "        h: (B,T,D), x: (B,T,3)\n",
    "        edge_index: (2,E)\n",
    "        node_mask: (B,T) bool\n",
    "        pair_p: (B,T,T) float in [0,1]\n",
    "        \"\"\"\n",
    "        B, T, D = h.shape\n",
    "        src, dst = edge_index[0], edge_index[1]  # (E,)\n",
    "\n",
    "        hi = h[:, src, :]\n",
    "        hj = h[:, dst, :]\n",
    "        xi = x[:, src, :]\n",
    "        xj = x[:, dst, :]\n",
    "\n",
    "        rij = xi - xj\n",
    "        dij2 = (rij * rij).sum(dim=-1, keepdim=True)  # (B,E,1)\n",
    "        pij = pair_p[:, src, dst].unsqueeze(-1)        # (B,E,1)\n",
    "\n",
    "        m_ij = self.edge_mlp(torch.cat([hi, hj, dij2, pij], dim=-1))  # (B,E,d_edge)\n",
    "        boost = (1.0 + self.pair_alpha * pij).clamp(0.0, 10.0)\n",
    "\n",
    "        w = self.coord_mlp(m_ij) * boost  # (B,E,1)\n",
    "        dx = rij * w  # (B,E,3)\n",
    "\n",
    "        agg_dx = x.new_zeros((B, T, 3))\n",
    "        agg_m  = h.new_zeros((B, T, m_ij.size(-1)))\n",
    "\n",
    "        # aggregate to src node\n",
    "        agg_dx.index_add_(1, src, dx.to(agg_dx.dtype))\n",
    "        agg_m.index_add_(1, src, (m_ij * boost).to(agg_m.dtype))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            agg_dx = agg_dx.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            agg_m  = agg_m.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "\n",
    "        x = x + agg_dx\n",
    "        h = self.ln(h + self.node_mlp(torch.cat([h, agg_m], dim=-1)))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            h = h.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "            x = x.masked_fill(~node_mask[:, :, None], 0.0)\n",
    "        return h, x\n",
    "\n",
    "class ConfidenceHead(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(cfg.d_model//2, cfg.num_preds)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pad_mask):\n",
    "        if pad_mask is None:\n",
    "            pooled = h.mean(dim=1)\n",
    "        else:\n",
    "            m = pad_mask.float().unsqueeze(-1)\n",
    "            denom = m.sum(dim=1).clamp_min(1.0)\n",
    "            pooled = (h * m).sum(dim=1) / denom\n",
    "        return self.mlp(pooled)  # (B,K)\n",
    "\n",
    "class EGNNv15(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = nn.Embedding(cfg.vocab, cfg.d_model, padding_idx=0)\n",
    "        self.posenc = PositionalEncodingLearned(cfg.d_model, max_len=cfg.max_len)\n",
    "        self.bp_enc = BasePairEncoder(tau=cfg.bp_tau, min_sep=cfg.bp_min_sep)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EGNNPairAwareLayer(cfg.d_model, cfg.d_edge, cfg.dropout, pair_alpha=cfg.pair_alpha)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        # ✅ v15: learnable coord init (breaks zero-symmetry)\n",
    "        self.init_x = nn.Linear(cfg.d_model, 3)\n",
    "        self.init_scale = float(getattr(cfg, \"init_x_scale\", 1.0))\n",
    "\n",
    "        # ✅ v15: K-head offsets (v12 style)\n",
    "        self.offset = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model, cfg.d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(cfg.d_model, cfg.num_preds * 3),\n",
    "        )\n",
    "        self.offset_scale = float(getattr(cfg, \"offset_scale\", 0.1))\n",
    "\n",
    "        self.conf_head = ConfidenceHead(cfg)\n",
    "\n",
    "        # init\n",
    "        nn.init.normal_(self.init_x.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.init_x.bias)\n",
    "\n",
    "    def forward(self, tokens, pad_mask=None):\n",
    "        \"\"\"\n",
    "        tokens: (B,T)\n",
    "        pad_mask: (B,T) bool (True for valid nodes)\n",
    "        returns:\n",
    "          preds: (B,K,T,3)\n",
    "          conf_logits: (B,K)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        if pad_mask is None:\n",
    "            pad_mask = (tokens != 0)\n",
    "\n",
    "        h = self.embed(tokens)\n",
    "        h = self.posenc(h)\n",
    "        h = h.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        # ---- v15 FIX: initialize coords from h (non-zero) ----\n",
    "        x = self.init_x(h) * self.init_scale  # (B,T,3)\n",
    "        x = x.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        # center x to remove translation (stabilizes warmup L1; target is centered in dataset)\n",
    "        m = pad_mask.float().unsqueeze(-1)\n",
    "        denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "        x = x - (x * m).sum(dim=1, keepdim=True) / denom\n",
    "        x = x.masked_fill(~pad_mask[:, :, None], 0.0)\n",
    "\n",
    "        edge_index = build_seq_edges(T, self.cfg.k_nn, tokens.device)\n",
    "\n",
    "        # pair probabilities (sequence-only heuristic)\n",
    "        pair_p = self.bp_enc(tokens, pad_mask)  # (B,T,T)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h, x = layer(h, x, edge_index, pad_mask, pair_p)\n",
    "\n",
    "        # K-head offsets\n",
    "        off = self.offset(h).view(B, T, self.cfg.num_preds, 3) * self.offset_scale  # (B,T,K,3)\n",
    "        off = off.permute(0, 2, 1, 3).contiguous()  # (B,K,T,3)\n",
    "\n",
    "        preds = x.unsqueeze(1) + off  # (B,K,T,3)\n",
    "        preds = preds.masked_fill(~pad_mask[:, None, :, None], 0.0)\n",
    "\n",
    "        conf_logits = self.conf_head(h, pad_mask)  # (B,K)\n",
    "        return preds, conf_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f9630",
   "metadata": {},
   "source": [
    "## 5) LossComposer (weak distance auxiliary 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d31a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3) LossComposer  [v13.2]\n",
    "#   - warmup: masked L1 (+optional confidence)\n",
    "#   - main: Kabsch RMSD + (optional sampled pairwise/repulsion) + local + variance + diversity (+confidence)\n",
    "#   - speed: compute expensive aux losses every cfg.aux_every steps\n",
    "# ==========================================\n",
    "class LossComposer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, preds, target, mask, epoch, step: int = 0, conf_logits=None):\n",
    "        temp = float(self.cfg.softmin_temp)\n",
    "\n",
    "        # Stage 1: masked L1\n",
    "        if epoch < self.cfg.warmup_epochs:\n",
    "            l1_bk = masked_l1_losses(preds, target, mask)  # (B,K)\n",
    "            loss = softmin_aggregate(l1_bk, temp)\n",
    "\n",
    "            aux = 0.0\n",
    "            if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "                with torch.no_grad():\n",
    "                    w_t = softmin_weights(l1_bk, temp)\n",
    "                logp = torch.log_softmax(conf_logits, dim=1)\n",
    "                aux = self.kl(logp, w_t)\n",
    "                loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "            return loss, \"MASKED_L1(+CONF)\" if aux != 0.0 else \"MASKED_L1\"\n",
    "\n",
    "        # Stage 2: structured losses (all are (B,K))\n",
    "        rmsd_bk = kabsch_rmsd_losses(preds, target, mask)\n",
    "        loc_bk  = local_bond_losses(preds, target, mask)\n",
    "        var_bk  = coord_variance_losses(preds, mask)\n",
    "\n",
    "        # expensive aux losses: compute every N steps\n",
    "        aux_every = int(getattr(self.cfg, \"aux_every\", 1))\n",
    "        do_aux = (aux_every <= 1) or ((step % aux_every) == 0)\n",
    "\n",
    "        if do_aux:\n",
    "            num_pairs = int(self.cfg.pair_num_pairs)\n",
    "            pair_bk = pairwise_distance_losses_sampled(preds, target, mask, num_pairs=num_pairs)\n",
    "            rep_bk  = repulsion_losses_sampled(preds, mask, num_pairs=num_pairs,\n",
    "                                               margin=float(self.cfg.repulse_margin))\n",
    "        else:\n",
    "            B, K = preds.shape[0], preds.shape[1]\n",
    "            pair_bk = preds.new_zeros((B, K))\n",
    "            rep_bk  = preds.new_zeros((B, K))\n",
    "\n",
    "        # diversity is relatively cheap; keep every step\n",
    "        div_bk  = head_diversity_losses(preds, mask, margin=float(self.cfg.diversity_margin))\n",
    "\n",
    "        # weak auxiliary weights (as requested)\n",
    "        dist_w_eff = float(self.cfg.dist_w)\n",
    "        rep_w_eff  = float(self.cfg.repulse_w)\n",
    "\n",
    "        total_bk = (\n",
    "            rmsd_bk\n",
    "            + dist_w_eff * pair_bk\n",
    "            + float(self.cfg.local_w) * loc_bk\n",
    "            - float(self.cfg.var_w) * var_bk\n",
    "            + rep_w_eff * rep_bk\n",
    "            + float(self.cfg.diversity_w) * div_bk\n",
    "        )\n",
    "\n",
    "        loss = softmin_aggregate(total_bk, temp)\n",
    "\n",
    "        aux = 0.0\n",
    "        if (conf_logits is not None) and self.cfg.use_confidence:\n",
    "            with torch.no_grad():\n",
    "                w_t = softmin_weights(total_bk, temp)\n",
    "            logp = torch.log_softmax(conf_logits, dim=1)\n",
    "            aux = self.kl(logp, w_t)\n",
    "            loss = loss + float(self.cfg.conf_w) * aux\n",
    "\n",
    "        return loss, \"STRUCTURED(+CONF)\" if aux != 0.0 else \"STRUCTURED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3584cf",
   "metadata": {},
   "source": [
    "## 6) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1577828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5.5) Checkpoint utils  [v14]\n",
    "#   - state_dict 기반 저장/로드 (pickling 클래스 의존 X)\n",
    "#   - flexible load: shape가 맞는 파라미터만 부분 로드\n",
    "# ==========================================\n",
    "from typing import Tuple, List, Optional, Any, Dict\n",
    "\n",
    "def save_checkpoint(path: str,\n",
    "                    model: nn.Module,\n",
    "                    opt: Optional[torch.optim.Optimizer],\n",
    "                    scaler: Optional[torch.amp.GradScaler],\n",
    "                    epoch: int,\n",
    "                    best_metric: Optional[float] = None,\n",
    "                    cfg_obj: Optional[CFG] = None,\n",
    "                    extra: Optional[Dict[str, Any]] = None):\n",
    "    ckpt: Dict[str, Any] = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model\": model.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "    }\n",
    "    if opt is not None:\n",
    "        ckpt[\"opt\"] = opt.state_dict()\n",
    "    if scaler is not None:\n",
    "        try:\n",
    "            ckpt[\"scaler\"] = scaler.state_dict()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if cfg_obj is not None:\n",
    "        ckpt[\"cfg\"] = dict(cfg_obj.__dict__)\n",
    "    if extra is not None:\n",
    "        ckpt[\"extra\"] = extra\n",
    "\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "def _unwrap_checkpoint(obj: Any) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Return (state_dict, meta_dict).\"\"\"\n",
    "    if isinstance(obj, dict) and (\"model\" in obj) and isinstance(obj[\"model\"], dict):\n",
    "        return obj[\"model\"], obj\n",
    "    # v12/v13 일부는 model.state_dict()만 저장했을 수 있음\n",
    "    if isinstance(obj, dict):\n",
    "        return obj, {}\n",
    "    raise TypeError(f\"Unsupported checkpoint type: {type(obj)}\")\n",
    "\n",
    "def load_state_dict_flexible(model: nn.Module, state_dict: Dict[str, torch.Tensor], verbose: bool = True):\n",
    "    model_sd = model.state_dict()\n",
    "    loadable: Dict[str, torch.Tensor] = {}\n",
    "    skipped: List[str] = []\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if (k in model_sd) and (tuple(model_sd[k].shape) == tuple(v.shape)):\n",
    "            loadable[k] = v\n",
    "        else:\n",
    "            skipped.append(k)\n",
    "\n",
    "    msg = model.load_state_dict(loadable, strict=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[flex load] loaded={len(loadable)}  skipped={len(skipped)}\")\n",
    "        # 너무 길어질 수 있으니 예시만\n",
    "        if skipped:\n",
    "            print(\"  - skipped examples:\", skipped[:10])\n",
    "        if hasattr(msg, \"missing_keys\") and msg.missing_keys:\n",
    "            print(\"  - missing_keys examples:\", msg.missing_keys[:10])\n",
    "        if hasattr(msg, \"unexpected_keys\") and msg.unexpected_keys:\n",
    "            print(\"  - unexpected_keys examples:\", msg.unexpected_keys[:10])\n",
    "\n",
    "    return msg, loadable, skipped\n",
    "\n",
    "def load_checkpoint(path: str,\n",
    "                    model: nn.Module,\n",
    "                    opt: Optional[torch.optim.Optimizer] = None,\n",
    "                    scaler: Optional[torch.amp.GradScaler] = None,\n",
    "                    map_location: Any = \"cpu\",\n",
    "                    flexible: bool = True,\n",
    "                    verbose: bool = True):\n",
    "    obj = torch.load(path, map_location=map_location)\n",
    "    state_dict, meta = _unwrap_checkpoint(obj)\n",
    "\n",
    "    if flexible:\n",
    "        load_state_dict_flexible(model, state_dict, verbose=verbose)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    if (opt is not None) and isinstance(meta, dict) and (\"opt\" in meta):\n",
    "        try:\n",
    "            opt.load_state_dict(meta[\"opt\"])\n",
    "        except Exception as e:\n",
    "            print(\"[ckpt] opt load skipped:\", repr(e))\n",
    "\n",
    "    if (scaler is not None) and isinstance(meta, dict) and (\"scaler\" in meta):\n",
    "        try:\n",
    "            scaler.load_state_dict(meta[\"scaler\"])\n",
    "        except Exception as e:\n",
    "            print(\"[ckpt] scaler load skipped:\", repr(e))\n",
    "\n",
    "    start_epoch = int(meta.get(\"epoch\", -1)) + 1 if isinstance(meta, dict) else 0\n",
    "    best_metric = meta.get(\"best_metric\", None) if isinstance(meta, dict) else None\n",
    "    return start_epoch, best_metric, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5a741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [train]:   0%|          | 0/646 [00:00<?, ?it/s, loss=nan, rmsd=None, stage=MASKED_L1(+CONF), tm=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[profile] step0 to_gpu: 0.000s  forward+loss: 0.188s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [train]: 100%|██████████| 646/646 [00:39<00:00, 16.30it/s, loss=nan, rmsd=None, stage=MASKED_L1(+CONF), tm=None]\n",
      "Epoch 1/20 [eval]:   0%|          | 0/72 [00:00<?, ?it/s, loss=nan, rmsd=nan, stage=MASKED_L1(+CONF), tm=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[profile] step0 to_gpu: 0.000s  forward+loss: 0.019s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [eval]: 100%|██████████| 72/72 [00:06<00:00, 11.73it/s, loss=nan, rmsd=nan, stage=MASKED_L1(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=MASKED_L1(+CONF)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [train]: 100%|██████████| 646/646 [00:39<00:00, 16.52it/s, loss=nan, rmsd=None, stage=MASKED_L1(+CONF), tm=None]\n",
      "Epoch 2/20 [eval]: 100%|██████████| 72/72 [00:05<00:00, 12.12it/s, loss=nan, rmsd=nan, stage=MASKED_L1(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=MASKED_L1(+CONF)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [train]: 100%|██████████| 646/646 [01:18<00:00,  8.20it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 3/20 [eval]: 100%|██████████| 72/72 [00:08<00:00,  8.48it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "🔁 Stage2 started: reset best_tm/stale\n",
      "⏸ no TM improvement: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [train]: 100%|██████████| 646/646 [01:25<00:00,  7.54it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 4/20 [eval]: 100%|██████████| 72/72 [00:09<00:00,  7.83it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [train]: 100%|██████████| 646/646 [01:32<00:00,  7.00it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 5/20 [eval]: 100%|██████████| 72/72 [00:10<00:00,  7.19it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [train]: 100%|██████████| 646/646 [01:34<00:00,  6.80it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 6/20 [eval]: 100%|██████████| 72/72 [00:09<00:00,  7.40it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [train]: 100%|██████████| 646/646 [01:34<00:00,  6.80it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 7/20 [eval]: 100%|██████████| 72/72 [00:09<00:00,  7.44it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [train]: 100%|██████████| 646/646 [01:34<00:00,  6.84it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 8/20 [eval]: 100%|██████████| 72/72 [00:09<00:00,  7.49it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [train]: 100%|██████████| 646/646 [01:34<00:00,  6.87it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 9/20 [eval]: 100%|██████████| 72/72 [00:09<00:00,  7.46it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [train]: 100%|██████████| 646/646 [01:30<00:00,  7.12it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 10/20 [eval]: 100%|██████████| 72/72 [00:08<00:00,  8.53it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [train]: 100%|██████████| 646/646 [01:18<00:00,  8.28it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 11/20 [eval]: 100%|██████████| 72/72 [00:08<00:00,  8.54it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [train]: 100%|██████████| 646/646 [01:17<00:00,  8.32it/s, loss=nan, rmsd=None, stage=STRUCTURED(+CONF), tm=None]\n",
      "Epoch 12/20 [eval]: 100%|██████████| 72/72 [00:08<00:00,  8.60it/s, loss=nan, rmsd=nan, stage=STRUCTURED(+CONF), tm=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] train_loss=nan  eval_loss=nan  eval_rmsd=nan  eval_tm=nan  stage=STRUCTURED(+CONF)\n",
      "⏸ no TM improvement: 10/10\n",
      "🛑 early stopping (Stage2, TM-based).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EGNNv15:\n\tMissing key(s) in state_dict: \"init_x.weight\", \"init_x.bias\", \"offset.0.weight\", \"offset.0.bias\", \"offset.2.weight\", \"offset.2.bias\". \n\tUnexpected key(s) in state_dict: \"coord_heads.0.0.weight\", \"coord_heads.0.0.bias\", \"coord_heads.0.2.weight\", \"coord_heads.0.2.bias\", \"coord_heads.1.0.weight\", \"coord_heads.1.0.bias\", \"coord_heads.1.2.weight\", \"coord_heads.1.2.bias\", \"coord_heads.2.0.weight\", \"coord_heads.2.0.bias\", \"coord_heads.2.2.weight\", \"coord_heads.2.2.bias\", \"coord_heads.3.0.weight\", \"coord_heads.3.0.bias\", \"coord_heads.3.2.weight\", \"coord_heads.3.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 158\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# ---- load best for inference ----\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cfg\u001b[38;5;241m.\u001b[39mckpt_best_path):\n\u001b[1;32m--> 158\u001b[0m     \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_best_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflexible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model loaded:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mckpt_best_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_tm=\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_tm)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cfg\u001b[38;5;241m.\u001b[39mckpt_stage1_path):\n",
      "Cell \u001b[1;32mIn[9], line 83\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[1;34m(path, model, opt, scaler, map_location, flexible, verbose)\u001b[0m\n\u001b[0;32m     81\u001b[0m     load_state_dict_flexible(model, state_dict, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (opt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(meta, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m meta):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tkdwl\\anaconda3\\envs\\rna_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2589\u001b[0m             ),\n\u001b[0;32m   2590\u001b[0m         )\n\u001b[0;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2595\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2596\u001b[0m         )\n\u001b[0;32m   2597\u001b[0m     )\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EGNNv15:\n\tMissing key(s) in state_dict: \"init_x.weight\", \"init_x.bias\", \"offset.0.weight\", \"offset.0.bias\", \"offset.2.weight\", \"offset.2.bias\". \n\tUnexpected key(s) in state_dict: \"coord_heads.0.0.weight\", \"coord_heads.0.0.bias\", \"coord_heads.0.2.weight\", \"coord_heads.0.2.bias\", \"coord_heads.1.0.weight\", \"coord_heads.1.0.bias\", \"coord_heads.1.2.weight\", \"coord_heads.1.2.bias\", \"coord_heads.2.0.weight\", \"coord_heads.2.0.bias\", \"coord_heads.2.2.weight\", \"coord_heads.2.2.bias\", \"coord_heads.3.0.weight\", \"coord_heads.3.0.bias\", \"coord_heads.3.2.weight\", \"coord_heads.3.2.bias\". "
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6) Train loop + checkpoint/resume  [v14]\n",
    "#   - num_workers/timeout 안전 처리(위 셀)\n",
    "#   - stage1(best val loss), stage2(best TM-score) 저장\n",
    "#   - resume_path 로 재시작 가능\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "model = EGNNv15(cfg).to(device)\n",
    "loss_fn = LossComposer(cfg).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(device.type, enabled=use_amp)\n",
    "\n",
    "# ----- (optional) resume -----\n",
    "start_epoch = 0\n",
    "best_stage1 = float(\"inf\")\n",
    "best_tm = -1e9\n",
    "stale = 0\n",
    "stage2_started = False\n",
    "\n",
    "if isinstance(getattr(cfg, \"resume_path\", \"\"), str) and cfg.resume_path and os.path.exists(cfg.resume_path):\n",
    "    print(\"[resume] loading:\", cfg.resume_path)\n",
    "    start_epoch, best_metric, meta = load_checkpoint(\n",
    "        cfg.resume_path, model, opt=opt, scaler=scaler,\n",
    "        map_location=device, flexible=True, verbose=True\n",
    "    )\n",
    "    # best_metric는 체크포인트에 저장된 값(있으면)\n",
    "    if best_metric is not None:\n",
    "        best_tm = float(best_metric)\n",
    "    # extra에 stage1 loss를 저장했다면 복원\n",
    "    if isinstance(meta, dict) and isinstance(meta.get(\"extra\", None), dict):\n",
    "        if \"best_stage1\" in meta[\"extra\"]:\n",
    "            best_stage1 = float(meta[\"extra\"][\"best_stage1\"])\n",
    "    print(f\"[resume] start_epoch={start_epoch}  best_stage1={best_stage1:.6f}  best_tm={best_tm:.4f}\")\n",
    "\n",
    "def run_epoch(loader, epoch: int, train: bool):\n",
    "    model.train(train)\n",
    "    total_loss, n = 0.0, 0\n",
    "    total_rmsd, n_r = 0.0, 0\n",
    "    total_tm, n_t = 0.0, 0\n",
    "    stage_name = None\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs} [{'train' if train else 'eval'}]\", mininterval=0.5)\n",
    "    for step, (tokens, target, mask) in enumerate(pbar):\n",
    "        do_prof = bool(getattr(cfg, \"profile_first_batch\", False)) and (epoch == start_epoch) and (step == 0)\n",
    "\n",
    "        if do_prof:\n",
    "            t0 = time.time()\n",
    "\n",
    "        tokens = tokens.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        mask   = mask.to(device, non_blocking=True)\n",
    "\n",
    "        if do_prof and device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "\n",
    "        fb = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "        if fb is None:\n",
    "            continue\n",
    "        tokens, target, mask = fb\n",
    "        pad_mask = (tokens != 0)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            if do_prof:\n",
    "                t2 = time.time()\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=use_amp):\n",
    "                preds, conf_logits = model(tokens, pad_mask)\n",
    "                loss, stage_name = loss_fn(preds, target, mask, epoch, step=step, conf_logits=conf_logits)\n",
    "\n",
    "            if do_prof and device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "                t3 = time.time()\n",
    "                print(f\"[profile] step0 to_gpu: {t1-t0:.3f}s  forward+loss: {t3-t2:.3f}s\")\n",
    "\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                # grad clip (안정성)\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(cfg.grad_clip))\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()); n += 1\n",
    "\n",
    "        if not train:\n",
    "            rmsd = kabsch_rmsd_metric_min(preds, target, mask)\n",
    "            tm   = tm_score_metric_maxK(preds, target, mask)\n",
    "            total_rmsd += float(rmsd.item()); n_r += 1\n",
    "            total_tm   += float(tm.item());   n_t += 1\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            loss=total_loss/max(n,1),\n",
    "            stage=stage_name,\n",
    "            rmsd=(total_rmsd/max(n_r,1) if n_r>0 else None),\n",
    "            tm=(total_tm/max(n_t,1) if n_t>0 else None),\n",
    "        )\n",
    "\n",
    "    avg_loss = total_loss/max(n,1)\n",
    "    avg_rmsd = (total_rmsd/max(n_r,1) if n_r>0 else None)\n",
    "    avg_tm   = (total_tm/max(n_t,1) if n_t>0 else None)\n",
    "    return avg_loss, avg_rmsd, avg_tm, stage_name\n",
    "\n",
    "last_path = os.path.join(cfg.ckpt_dir, \"last_v14.pt\")\n",
    "\n",
    "for epoch in range(start_epoch, cfg.epochs):\n",
    "    tr_loss, _, _, _ = run_epoch(train_loader, epoch, train=True)\n",
    "    ev_loss, ev_rmsd, ev_tm, stage = run_epoch(hold_loader, epoch, train=False)\n",
    "    print(f\"[Epoch {epoch+1}] train_loss={tr_loss:.4f}  eval_loss={ev_loss:.4f}  eval_rmsd={ev_rmsd}  eval_tm={ev_tm}  stage={stage}\")\n",
    "\n",
    "    # --- always save last (resume용) ---\n",
    "    save_checkpoint(\n",
    "        last_path, model, opt, scaler, epoch,\n",
    "        best_metric=best_tm,\n",
    "        cfg_obj=cfg,\n",
    "        extra={\"best_stage1\": best_stage1},\n",
    "    )\n",
    "\n",
    "    # --- Stage1: best val loss ---\n",
    "    if epoch < cfg.warmup_epochs:\n",
    "        if ev_loss < best_stage1 - 1e-4:\n",
    "            best_stage1 = ev_loss\n",
    "            save_checkpoint(cfg.ckpt_stage1_path, model, opt, scaler, epoch,\n",
    "                            best_metric=best_stage1, cfg_obj=cfg)\n",
    "            print(f\"💾 stage1 best loss updated: {best_stage1:.6f} -> {cfg.ckpt_stage1_path}\")\n",
    "        continue\n",
    "\n",
    "    # --- Stage2 starts: reset patience tracking once ---\n",
    "    if not stage2_started:\n",
    "        stage2_started = True\n",
    "        best_tm = -1e9\n",
    "        stale = 0\n",
    "        print(\"🔁 Stage2 started: reset best_tm/stale\")\n",
    "\n",
    "    # --- Stage2: maximize TM-score (competition aligned) ---\n",
    "    cur_tm = float(ev_tm) if ev_tm is not None else -1e9\n",
    "    if cur_tm > best_tm + 1e-4:\n",
    "        best_tm = cur_tm\n",
    "        stale = 0\n",
    "        save_checkpoint(cfg.ckpt_best_path, model, opt, scaler, epoch,\n",
    "                        best_metric=best_tm, cfg_obj=cfg)\n",
    "        print(f\"✅ best TM updated: {best_tm:.4f} -> {cfg.ckpt_best_path}\")\n",
    "    else:\n",
    "        stale += 1\n",
    "        print(f\"⏸ no TM improvement: {stale}/{cfg.patience}\")\n",
    "        if stale >= int(cfg.patience):\n",
    "            print(\"🛑 early stopping (Stage2, TM-based).\")\n",
    "            break\n",
    "\n",
    "# ---- load best for inference ----\n",
    "if os.path.exists(cfg.ckpt_best_path):\n",
    "    load_checkpoint(cfg.ckpt_best_path, model, opt=None, scaler=None, map_location=device, flexible=False, verbose=True)\n",
    "    print(\"Best model loaded:\", cfg.ckpt_best_path, \"best_tm=\", best_tm)\n",
    "elif os.path.exists(cfg.ckpt_stage1_path):\n",
    "    load_checkpoint(cfg.ckpt_stage1_path, model, opt=None, scaler=None, map_location=device, flexible=False, verbose=True)\n",
    "    print(\"Stage1 best model loaded:\", cfg.ckpt_stage1_path, \"best_loss=\", best_stage1)\n",
    "else:\n",
    "    print(\"No checkpoint found (best/stage1). Using current model in memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e3366",
   "metadata": {},
   "source": [
    "## 7) Sanity check 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31348ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7) Quick sanity check plot (xy/xz/yz)\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens, target, mask = next(iter(hold_loader))\n",
    "tokens = tokens.to(device); target = target.to(device); mask = mask.to(device)\n",
    "tokens, target, mask = filter_batch(tokens, target, mask, cfg.min_valid)\n",
    "pad_mask = (tokens != 0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds, conf_logits = model(tokens, pad_mask)  # (B,K,T,3)\n",
    "    # pick best head by TM-score for sample 0\n",
    "    tm_per_head = []\n",
    "    for k in range(cfg.num_preds):\n",
    "        tm = tm_score_metric(preds[0:1, k], target[0:1], mask[0:1])\n",
    "        tm_per_head.append(float(tm.item()))\n",
    "    best_k = int(np.argmax(tm_per_head))\n",
    "    pred0 = preds[0, best_k].detach()\n",
    "    tgt0  = target[0].detach()\n",
    "    m0    = mask[0].detach().bool()\n",
    "\n",
    "    pred0a = kabsch_align(pred0.unsqueeze(0), tgt0.unsqueeze(0), mask[0:1]).squeeze(0)\n",
    "\n",
    "def scat(a, b, title, ax):\n",
    "    ax.scatter(a[:,0].cpu(), a[:,1].cpu(), s=10, label=\"target\")\n",
    "    ax.scatter(b[:,0].cpu(), b[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "A = tgt0[m0]\n",
    "B = pred0a[m0]\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "axes[0].scatter(A[:,0].cpu(), A[:,1].cpu(), s=10, label=\"target\")\n",
    "axes[0].scatter(B[:,0].cpu(), B[:,1].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[0].set_title(\"x-y\"); axes[0].legend()\n",
    "\n",
    "axes[1].scatter(A[:,0].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[1].scatter(B[:,0].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[1].set_title(\"x-z\"); axes[1].legend()\n",
    "\n",
    "axes[2].scatter(A[:,1].cpu(), A[:,2].cpu(), s=10, label=\"target\")\n",
    "axes[2].scatter(B[:,1].cpu(), B[:,2].cpu(), s=10, label=\"pred(aligned)\")\n",
    "axes[2].set_title(\"y-z\"); axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"best head:\", best_k, \"tm per head:\", tm_per_head)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
